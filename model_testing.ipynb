{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle('data/datasets/dataset_with_features.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the numerical features created\n",
    "features = dataset[['max_sentiment_english',\n",
    "                    'max_sentiment_german',\n",
    "                    'std_max_english_sentiment',\n",
    "                    'std_max_german_sentiment',\n",
    "                    'german_sentence_length',\n",
    "                    'english_sentence_length',\n",
    "                    'sentence_length_difference',\n",
    "                    'verbs_diff',\n",
    "                    'adjectives_diff',\n",
    "                    'adverbs_diff',\n",
    "                    'nouns_diff',\n",
    "                    'non_translated_words',\n",
    "                    'correlation',\n",
    "                    'sentence_correlation',\n",
    "                    'non_match_correlation',\n",
    "                    'non_translated_ratio']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting between the Train/Validation dataset and the test\n",
    "train_val = features[:8000]\n",
    "test = features[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the features to a list (used for random forest important features)\n",
    "dataset_features_list = list(train_val.columns)\n",
    "dataset_features_arr = np.array(train_val)\n",
    "test_features_arr = np.array(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting the scores for the train and validation datasets\n",
    "path_train_scores = os.path.join(os.getcwd(), 'data', 'en-de', 'train.ende.scores')\n",
    "train_scores = pd.read_csv(path_train_scores,header=None)\n",
    "train_scores = train_scores.rename(columns={0:\"scores\"})\n",
    "path_val_scores = os.path.join(os.getcwd(), 'data', 'en-de', 'dev.ende.scores')\n",
    "val_scores = pd.read_csv(path_val_scores,header=None)\n",
    "val_scores = val_scores.rename(columns={0:\"scores\"})\n",
    "scores = pd.concat([train_scores, val_scores])\n",
    "\n",
    "dataset_labels_arr = np.array(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CROSS VALIDATION AND RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lines to standardize the features if desired\n",
    "\n",
    "# normalised = (dataset_features_arr - dataset_features_arr.min(axis=0))/ \\\n",
    "#              (dataset_features_arr.max(axis=0) - dataset_features_arr.min(axis=0))\n",
    "# normalised.var(axis=0)\n",
    "# sel = VarianceThreshold(threshold=0.01)\n",
    "# dataset_features_arr = sel.fit_transform(normalised)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a randomized grid search and cross validation to find the best PLSRegressor model \n",
    "\n",
    "n_components = [int(x) for x in np.arange(1,12)]\n",
    "scale = [True, False]\n",
    "max_iter = [int(x) for x in np.arange(300,1000)]\n",
    "\n",
    "random_grid = {'n_components': n_components,\n",
    "               'scale': scale, \n",
    "               'max_iter':max_iter}\n",
    "\n",
    "pls = PLSRegression()\n",
    "pls_random = RandomizedSearchCV(estimator = pls, param_distributions = random_grid, \n",
    "                                n_iter = 100, cv = 8, random_state=42, scoring='neg_mean_squared_error')\n",
    "pls_random.fit(dataset_features_arr, dataset_labels_arr)\n",
    "print(pls_random.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the next cell, a cross validation over many randomly created splits are run on each of the best model to provide estimates of the score distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = ShuffleSplit(n_splits=500, test_size=.25, random_state=0)\n",
    "cv = rs.split(dataset_features_arr)\n",
    "\n",
    "correlation_pls = []\n",
    "mae_pls = []\n",
    "X = dataset_features_arr\n",
    "y = dataset_labels_arr.reshape(-1)\n",
    "\n",
    "for train, test in cv:\n",
    "    pls = PLSRegression(**pls_random.best_params_)\n",
    "    pls.fit(X[train], y[train])\n",
    "    predictions = pls.predict(X[test]).reshape(-1)\n",
    "    pearson = pearsonr(y[test], predictions)[0]\n",
    "    error = abs(predictions - y[test])\n",
    "    correlation_pls.append(pearson)\n",
    "    mae_pls.append(error)\n",
    "    \n",
    "print(np.array(correlation_pls).mean())\n",
    "print(np.array(mae_pls).mean())\n",
    "print(np.array(correlation_pls).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions directory if don't exist\n",
    "predictions_dir = os.path.join(os.getcwd(), 'data', 'imgs')\n",
    "if not os.path.exists(predictions_dir):\n",
    "    os.mkdir(predictions_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(correlation_pls)\n",
    "plt.xlabel('Pearson Correlation')\n",
    "plt.ylabel('Counts')\n",
    "plt.savefig('data/imgs/pls.svg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a randomized grid search and cross validation to find the best Ridge Linear Regression model \n",
    "\n",
    "alpha = [x for x in np.linspace(0,1,10)]\n",
    "normalize = [True, False]\n",
    "max_iter = [int(x) for x in np.arange(300,1500)]\n",
    "\n",
    "random_grid = {'alpha': alpha,\n",
    "               'normalize': normalize, \n",
    "               'max_iter':max_iter}\n",
    "\n",
    "lr = Ridge()\n",
    "lr_random = RandomizedSearchCV(estimator = lr, param_distributions = random_grid, \n",
    "                                n_iter = 100, cv = 8, random_state=42, scoring='neg_mean_squared_error')\n",
    "lr_random.fit(dataset_features_arr, dataset_labels_arr)\n",
    "print(lr_random.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the next cell, a cross validation over many randomly created splits are run on each of the best model to provide estimates of the score distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a cross validation over 500 different splits to compute an estimate of the score distribution\n",
    "\n",
    "rs = ShuffleSplit(n_splits=500, test_size=.25, random_state=0)\n",
    "cv = rs.split(dataset_features_arr)\n",
    "\n",
    "correlation_ridge = []\n",
    "mae_ridge = []\n",
    "X = dataset_features_arr\n",
    "y = dataset_labels_arr.reshape(-1)\n",
    "\n",
    "# for all different split\n",
    "for train, test in cv:\n",
    "    rl = Ridge(**lr_random.best_params_)\n",
    "    rl.fit(X[train], y[train])\n",
    "    predictions = rl.predict(X[test]).reshape(-1)\n",
    "    pearson = pearsonr(y[test], predictions)[0]\n",
    "    error = abs(predictions - y[test])\n",
    "    correlation_ridge.append(pearson)\n",
    "    mae_ridge.append(error)\n",
    "\n",
    "# Print the score of interest\n",
    "print(np.array(correlation_ridge).mean())\n",
    "print(np.array(mae_ridge).mean())\n",
    "print(np.array(correlation_ridge).std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and save the distribution\n",
    "sns.distplot(correlation_ridge)\n",
    "plt.xlabel('Pearson Correlation')\n",
    "plt.ylabel('Counts')\n",
    "plt.savefig('data/imgs/ridge.svg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a randomized grid search and cross validatino to find the best SVM model \n",
    "\n",
    "kernel = ('linear', 'poly', 'rbf')\n",
    "C = [int(x) for x in np.linspace(1, 150, 10)]\n",
    "\n",
    "random_grid = {'kernel': kernel,\n",
    "               'C': C}\n",
    "svm = SVR()\n",
    "svm_random = RandomizedSearchCV(estimator = svm, param_distributions = random_grid, \n",
    "                                n_iter = 10, cv = 2, scoring='neg_mean_squared_error')\n",
    "svm_random.fit(dataset_features_arr, dataset_labels_arr.ravel())\n",
    "print(svm_random.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the next cell, a cross validation over many randomly created splits are run on each of the best model to provide estimates of the score distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = ShuffleSplit(n_splits=10, test_size=.25, random_state=0)\n",
    "cv = rs.split(dataset_features_arr)\n",
    "\n",
    "correlation_svm = []\n",
    "mae_svm = []\n",
    "X = dataset_features_arr\n",
    "y = dataset_labels_arr.reshape(-1)\n",
    "\n",
    "for train, test in cv\n",
    "    svm = SVR(**svm_random.best_params_)\n",
    "    svm.fit(X[train], y[train])\n",
    "    predictions = svm.predict(X[test]).reshape(-1)\n",
    "    pearson = pearsonr(y[test], predictions)[0]\n",
    "    error = abs(predictions - y[test])\n",
    "    correlation_svm.append(pearson)\n",
    "    mae_svm.append(error)\n",
    "    \n",
    "print(np.array(correlation_svm).mean())\n",
    "print(np.array(mae_svm).mean())\n",
    "print(np.array(correlation_svm).std())\n",
    "\n",
    "sns.distplot(correlation_svm)\n",
    "plt.xlabel('Pearson Correlation')\n",
    "plt.ylabel('Counts')\n",
    "plt.savefig('data/imgs/svr.svg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing a randomized grid search and cross validation to find the best random forest model \n",
    "# Setting the different parameters\n",
    "\n",
    "n_estimators = [500, 1000]\n",
    "max_depth = [1,2]\n",
    "bootstrap = [True]# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'bootstrap':bootstrap}\n",
    "\n",
    "rf = RandomForestRegressor()\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n",
    "                               n_iter = 2, cv = 2, random_state=42, scoring='neg_mean_squared_error')\n",
    "rf_random.fit(dataset_features_arr, dataset_labels_arr.ravel())\n",
    "print(rf_random.best_params_)\n",
    "print(rf_random.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlighting the most important features of the model\n",
    "rf = RandomForestRegressor(**{'n_estimators': 1000, 'max_depth': 2, 'bootstrap': True})\n",
    "rf.fit(dataset_features_arr, dataset_labels_arr.ravel())\n",
    "\n",
    "importances = list(rf.feature_importances_)\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(dataset_features_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the next cell, a cross validation over many randomly created splits are run on each of the best model to provide estimates of the score distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = ShuffleSplit(n_splits=10, test_size=.25, random_state=0)\n",
    "cv = rs.split(dataset_features_arr)\n",
    "\n",
    "correlation_rf = []\n",
    "mae_rf = []\n",
    "X = dataset_features_arr\n",
    "y = dataset_labels_arr.reshape(-1)\n",
    "\n",
    "for train, test in cv\n",
    "    rf = RandomForestRegressor(**rf_random.best_params_)\n",
    "    rf.fit(X[train], y[train])\n",
    "    predictions = rf.predict(X[test]).reshape(-1)\n",
    "    pearson = pearsonr(y[test], predictions)[0]\n",
    "    error = abs(predictions - y[test])\n",
    "    correlation_rf.append(pearson)\n",
    "    mae_rf.append(error)\n",
    "    \n",
    "print(np.array(correlation_rf).mean())\n",
    "print(np.array(mae_rf).mean())\n",
    "print(np.array(correlation_rf).std())\n",
    "\n",
    "sns.distplot(correlation_rf)\n",
    "plt.xlabel('Pearson Correlation')\n",
    "plt.ylabel('Counts')\n",
    "plt.savefig('data/imgs/rf.svg', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nerual Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model tested were neural networks. The following provides the implementation of the network using Pytorch, as well as a cross validation on the tested architecture. In this section, the full LASER embedding 1024 dimension vectors were included in the feartures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the laser embeddings daatsets\n",
    "english_laser = np.load('data/laser_embeddings/laser_1024_english.npy')\n",
    "german_laser = np.load('data/laser_embeddings/laser_1024_german.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the train and validation features\n",
    "english_laser_train = english_laser[:8000,:]\n",
    "german_laser_train = german_laser[:8000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get arrays from pandas columns\n",
    "english_laser_train = np.array(english_laser_train)\n",
    "german_laser_train = np.array(german_laser_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total features\n",
    "laser_features = np.concatenate((english_laser_train, german_laser_train), axis=1)\n",
    "train_val_features = np.concatenate((laser_features, dataset_features_arr), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing the model\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, x):\n",
    "        super(Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(x.shape[1], 1000) \n",
    "        nn.init.xavier_uniform_(self.fc1.weight, gain=1.0)\n",
    "        self.fc2 = nn.Linear(1000, 500)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight, gain=1.0)\n",
    "        self.fc3 = nn.Linear(500, 1)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight, gain=1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net(train_val_features)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cross validation over random splits of the data, and printing the best pearson correlation \n",
    "# coefficents obtained during on the test data\n",
    "\n",
    "rs = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n",
    "cv = rs.split(dataset_features_arr)\n",
    "\n",
    "for train_idx, val_idx in cv:\n",
    "    \n",
    "    # Rededining the model to restars learning\n",
    "    net = Net(train_val_features)\n",
    "    \n",
    "    train_features = train_val_features[train_idx]\n",
    "    train_labels = dataset_labels_arr[train_idx].reshape(-1)\n",
    "    \n",
    "    val_features = train_val_features[val_idx]\n",
    "    val_labels = dataset_labels_arr[val_idx].reshape(-1)\n",
    "    \n",
    "    # Making to tensors as required by Pytorch to propagate the gradients\n",
    "    train_features_ten, train_labels_ten = Tensor(train_features), Tensor(train_labels.reshape(-1,1))\n",
    "\n",
    "    # Standardizing the inputs\n",
    "    means = train_features_ten.mean(dim=0, keepdim=True)\n",
    "    stds = train_features_ten.std(dim=0, keepdim=True)\n",
    "    normalized_train = (train_features_ten - means) / stds\n",
    "\n",
    "    val_features_ten = (Tensor(val_features)- means) / stds\n",
    "    test_labels = Tensor((val_labels).reshape(-1, 1))\n",
    "\n",
    "    # Creating the dataloaders to create the batches and iterate over epochs\n",
    "    train_data = TensorDataset(normalized_train, train_labels_ten)\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=32)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "\n",
    "    test_pearson=[]\n",
    "    train_pearson=[]\n",
    "    \n",
    "    for epoch in range(15):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # No grads to prevent from propagating the gradient from the test data\n",
    "        with torch.no_grad():\n",
    "            # Getting the test correlation\n",
    "            predictions = net(val_features_ten)\n",
    "            test_loss = criterion(predictions, test_labels).item()\n",
    "            pearson_test = pearsonr(val_labels, predictions.numpy().reshape(-1,))[0]\n",
    "            test_pearson.append(pearson_test)\n",
    "\n",
    "            #train correlation\n",
    "            predictions = net(normalized_train)\n",
    "            pearson_train = pearsonr(train_labels, predictions.numpy().reshape(-1,))[0]\n",
    "            train_pearson.append(pearson_train)\n",
    "\n",
    "    print('Test: ', test_pearson.index(max(test_pearson)), max(test_pearson))\n",
    "    print('Train: ', train_pearson.index(max(train_pearson)), max(train_pearson))\n",
    "    plt.plot(test_pearson)\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions for test set with the best set of parameters for the Ridge and PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with ridge regression \n",
    "\n",
    "lr_test = Ridge(**{'normalize': True, 'max_iter': 595, 'alpha': 0.2222222222222222})\n",
    "lr_test.fit(dataset_features_arr, dataset_labels_arr)\n",
    "predictions = lr_test.predict(test_features_arr).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions with PLS regression \n",
    "\n",
    "pls_test = PLSRegression(**{'scale': True, 'n_components': 2, 'max_iter': 339})\n",
    "pls_test.fit(dataset_features_arr, dataset_labels_arr)\n",
    "predictions = pls_test.predict(test_features_arr).reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions directory if don't exist\n",
    "predictions_dir = os.path.join(os.getcwd(), 'data', 'predictions')\n",
    "if not os.path.exists(predictions_dir):\n",
    "    os.mkdir(predictions_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the scores to be tested on Codalab\n",
    "\n",
    "def writeScores(method_name,scores):\n",
    "    fn = \"data/predictions/predictions.txt\"\n",
    "    print(\"\")\n",
    "    with open(fn, 'w') as output_file:\n",
    "        for idx,x in enumerate(scores):\n",
    "            #out =  metrics[idx]+\":\"+str(\"{0:.2f}\".format(x))+\"\\n\"\n",
    "            #print(out)\n",
    "            output_file.write(f\"{x}\\n\")\n",
    "\n",
    "writeScores(\"model_name\",predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
