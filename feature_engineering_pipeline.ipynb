{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes approximately 30 minutes on a regular machine (Macbook Pro 2018), it is possible to load the result of this notebook in the folder data/datasets with the command line pd.read_pickle('data/datasets/dataset_with_features.pickle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pickle\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from itertools import repeat\n",
    "\n",
    "import re\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "import de_core_news_md\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_analysis(sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_alpha, token.is_stop,token.vector.shape)\n",
    "\n",
    "def entity_analysis(sentence,nlp):\n",
    "    print(sentence+\"\\n\"+\"\\n\"+\"Analysis\"+\"\\n\"+\"--------\")\n",
    "    doc = nlp(sentence)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "def extract_sentences(filename,lower=False):\n",
    "    if lower:\n",
    "        data = [l.lower().strip() for l in open(filename) if l.strip()]\n",
    "    else:\n",
    "        data = [l.strip() for l in open(filename) if l.strip()]\n",
    "    return data\n",
    "\n",
    "def remove_unnecessary_spaces(sentence):\n",
    "    return re.sub(' +',' ',sentence.strip())\n",
    "\n",
    "def remove_word_from_sentence(sentence,word):\n",
    "    new_sentence = sentence.replace(word,\"\")\n",
    "    return remove_unnecessary_spaces(new_sentence)\n",
    "\n",
    "def remove_words_from_list(list_1,list_2,isin=False):\n",
    "    if isin:\n",
    "        return [word for word in list_1 if word in list_2]\n",
    "    else:\n",
    "        return [word for word in list_1 if word not in list_2]\n",
    "\n",
    "def remove_multiple_words_from_sentence(sentence,words,isin=False):\n",
    "    if isin:\n",
    "        splited = [word for word in sentence.split() if word in words]\n",
    "    else:\n",
    "        splited = [word for word in sentence.split() if word not in words]\n",
    "    return \" \".join(splited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = en_core_web_md.load()\n",
    "nlp_ge = de_core_news_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.src')\n",
    "path_train_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.mt')\n",
    "path_train_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_en = pd.DataFrame(extract_sentences(path_train_en),columns = ['sentences_en'])\n",
    "train_sentences_ge = pd.DataFrame(extract_sentences(path_train_ge),columns = ['sentences_ge'])\n",
    "train_scores = pd.read_csv(path_train_scores,header=None)\n",
    "train_scores = train_scores.rename(columns={0:\"scores\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.merge(train_sentences_en,train_sentences_ge,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dev_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.src')\n",
    "path_dev_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.mt')\n",
    "path_dev_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences_en = pd.DataFrame(extract_sentences(path_dev_en),columns = ['sentences_en'])\n",
    "dev_sentences_ge = pd.DataFrame(extract_sentences(path_dev_ge),columns = ['sentences_ge'])\n",
    "dev_scores = pd.read_csv(path_dev_scores,header=None)\n",
    "dev_scores = dev_scores.rename(columns={0:\"scores\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = pd.merge(dev_sentences_en,dev_sentences_ge,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'test.ende.src')\n",
    "path_test_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'test.ende.mt')\n",
    "path_test_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'test.ende.scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences_en = pd.DataFrame(extract_sentences(path_test_en),columns = ['sentences_en'])\n",
    "test_sentences_ge = pd.DataFrame(extract_sentences(path_test_ge),columns = ['sentences_ge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.merge(test_sentences_en,test_sentences_ge,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([train_dataset, dev_dataset])\n",
    "dataset = pd.concat([dataset,test_dataset])\n",
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading MUSE embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    '''\n",
    "    Loading function to load MUSE embeddings\n",
    "    '''\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'data','muse')\n",
    "\n",
    "src_path = path+\"/wiki.multi.en.vec\"\n",
    "tgt_path = path+\"/wiki.multi.de.vec\"\n",
    "nmax = 300000  # maximum number of word embeddings to load\n",
    "\n",
    "# Here src_embeddings corresponds to the English language and tgt_embeddings to the German Language\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading nltk utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/marcdelaferriere/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"popular\")\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer #stem for english\n",
    "from nltk.stem.cistem import Cistem\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "porter = PorterStemmer()\n",
    "cistem = Cistem()\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "stopwords_ge = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_length(sentence):\n",
    "    '''\n",
    "    Helper function to compute the length of a sentence\n",
    "    '''\n",
    "    leng=0\n",
    "    sentence=sentence.split(\" \")\n",
    "    for token in sentence:\n",
    "        leng+=1\n",
    "    return leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_length(a, b):\n",
    "    '''\n",
    "    Helper function to compute the difference between the length of two sentences\n",
    "    '''\n",
    "    lenga=0\n",
    "    a=a.split(\" \")\n",
    "    for token in a:\n",
    "        lenga+=1\n",
    "    \n",
    "    lengb=0\n",
    "    b=b.split(\" \")\n",
    "    for token in b:\n",
    "        lengb+=1\n",
    "    \n",
    "    return abs(lenga-lengb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the difference in sentence length between the English and the German sentence\n",
    "\n",
    "dataset['english_sentence_length'] = dataset['sentences_en'].apply(sentence_length)\n",
    "dataset['german_sentence_length'] = dataset['sentences_ge'].apply(sentence_length)\n",
    "dataset['sentence_length_difference'] = dataset['english_sentence_length'] - dataset['german_sentence_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos(pos,sentence,nlp):\n",
    "    '''\n",
    "    Helper function to extract find the words in the sentence corresponding to a specific Part-of-speech using Spacy \n",
    "    nlp: corresponds to the Spacy nlp corresponding to the language of the sentence\n",
    "    '''\n",
    "    doc = nlp(sentence)\n",
    "    res = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_==pos:\n",
    "            res += token.text + \" \"\n",
    "    res = res.strip()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting verbs and counting their difference in number between the two sentences\n",
    "\n",
    "dataset[\"german_verbs\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"VERB\",x,nlp_ge))\n",
    "dataset[\"english_verbs\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"VERB\",x,nlp_en))\n",
    "dataset['verbs_diff'] = dataset.apply(lambda row: difference_length(row['english_verbs'], row['german_verbs']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting adjectives and counting their difference in number between the two sentences\n",
    "\n",
    "dataset[\"german_adjectives\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"ADJ\",x,nlp_ge))\n",
    "dataset[\"english_adjectives\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"ADJ\",x,nlp_en))\n",
    "dataset['adjectives_diff'] = dataset.apply(lambda row: difference_length(row['english_adjectives'], row['german_adjectives']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting adverbs and counting their difference in number between the two sentences\n",
    "\n",
    "dataset[\"german_adverbs\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"ADV\",x,nlp_ge))\n",
    "dataset[\"english_adverbs\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"ADV\",x,nlp_en))\n",
    "dataset['adverbs_diff'] = dataset.apply(lambda row: difference_length(row['english_adverbs'], row['german_adverbs']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting nouns and counting their difference in number between the two sentences\n",
    "\n",
    "dataset[\"german_nouns\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"NOUN\",x,nlp_ge))\n",
    "dataset[\"english_nouns\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"NOUN\",x,nlp_en))\n",
    "dataset['nouns_diff'] = dataset.apply(lambda row: difference_length(row['english_nouns'], row['german_nouns']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(sentence, nlp):\n",
    "    '''\n",
    "    Helper function to lemmatize a sentence using Spacy \n",
    "    nlp: corresponds to the Spacy nlp corresponding to the language of the sentence\n",
    "    '''\n",
    "    sentence = nlp(sentence)\n",
    "    lemmatized_sentence = \"\"\n",
    "    for token in sentence:\n",
    "        lemmatized_sentence += token.lemma_ + \" \"\n",
    "    \n",
    "    lemmatized_sentence = lemmatized_sentence.strip()\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing sentences\n",
    "\n",
    "dataset['english_lemma'] = dataset['sentences_en'].apply (lambda x: lemmatizer(x, nlp_en))\n",
    "dataset['german_lemma'] = dataset['sentences_ge'].apply (lambda x: lemmatizer(x, nlp_ge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob as textblob_en\n",
    "from textblob_de import TextBlobDE as textblob_ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(sentence, textblob):\n",
    "    '''\n",
    "    Function to compute the sentiment of a sentence using textblob\n",
    "    '''\n",
    "    text = textblob(sentence)\n",
    "    score = text.sentiment.polarity\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the values coming out of the sentiment function for each sentence\n",
    "\n",
    "dataset['english_sentence_sentiment'] = dataset['sentences_en'].apply(lambda x: sentiment(x, textblob_en))\n",
    "dataset['german_sentence_sentiment'] = dataset['sentences_ge'].apply(lambda x: sentiment(x, textblob_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the values coming out of the sentiment function for each lemmatized sentence\n",
    "\n",
    "dataset['english_sentence_lemma_sentiment'] = dataset['english_lemma'].apply(lambda x: sentiment(x, textblob_en))\n",
    "dataset['german_sentence_lemma_sentiment'] = dataset['german_lemma'].apply(lambda x: sentiment(x, textblob_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdz(data):\n",
    "    '''\n",
    "    Function to standardize a given dataset\n",
    "    '''\n",
    "    data_stdz = (data - data.mean())/data.std()\n",
    "    return data_stdz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the sentiment values\n",
    "\n",
    "dataset['std_english_sentence_sentiment']= stdz(dataset['english_sentence_sentiment'])\n",
    "dataset['std_german_sentence_sentiment']= stdz(dataset['german_sentence_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the maximum absolute values coming out of the sentiment function between the lemmatized sentences\n",
    "# and the original ones.\n",
    "\n",
    "dataset[\"max_sentiment_english_bool\"] = dataset.apply(lambda row:abs(row['english_sentence_lemma_sentiment'])>abs(row['english_sentence_sentiment']),axis=1)\n",
    "dataset[\"max_sentiment_german_bool\"] = dataset.apply(lambda row:abs(row['german_sentence_lemma_sentiment'])>abs(row['german_sentence_sentiment']),axis=1)\n",
    "\n",
    "dataset['max_sentiment_english'] = dataset.apply(lambda row: row[\"english_sentence_lemma_sentiment\"] \n",
    "                                                 if row[\"max_sentiment_english_bool\"]\n",
    "                                                 else row[\"english_sentence_sentiment\"],axis=1)\n",
    "\n",
    "dataset = dataset.drop(columns=[\"max_sentiment_english_bool\"])\n",
    "\n",
    "dataset['max_sentiment_german'] = dataset.apply(lambda row: row[\"german_sentence_lemma_sentiment\"] \n",
    "                                                 if row[\"max_sentiment_german_bool\"]\n",
    "                                                 else row[\"german_sentence_sentiment\"],axis=1)\n",
    "\n",
    "dataset = dataset.drop(columns=[\"max_sentiment_german_bool\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the sentiment values\n",
    "\n",
    "dataset['std_max_english_sentiment']= stdz(dataset['max_sentiment_english'])\n",
    "dataset['std_max_german_sentiment']= stdz(dataset['max_sentiment_german'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the full vocabluary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(sentence, nlp):\n",
    "    '''\n",
    "    Function to remove the punctuation of a sentence given its Spacy nlp\n",
    "    '''\n",
    "    sentence=nlp(sentence)\n",
    "    clean_sentence = \"\"\n",
    "    for token in sentence:\n",
    "        if (token.is_alpha):\n",
    "            clean_sentence+= (token.text.lower() + \" \")\n",
    "    clean_sentence=clean_sentence.strip()\n",
    "    return clean_sentence\n",
    "\n",
    "def remove_entities(sentence,entity_list):\n",
    "    '''\n",
    "    Function to remove the entities from a sentence given an entity list\n",
    "    '''\n",
    "    sentence_without_entities = sentence\n",
    "    for person in entity_list:\n",
    "        sentence_without_entities = sentence_without_entities.replace(person,\"\")\n",
    "    return sentence_without_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sentence_en,sentence_ge):\n",
    "    '''\n",
    "    Helper function to extract Entities using Spacy\n",
    "    '''\n",
    "    doc = nlp_en(sentence_en)\n",
    "    persons_list = []\n",
    "    for ent in doc.ents:\n",
    "        # the entities extracted are part of these general categories (organizations,persons,county...etc)\n",
    "        if (ent.label_==\"PERSON\" or ent.label_==\"ORG\" or \n",
    "            ent.label_==\"LOC\" or ent.label_==\"GPE\" \n",
    "            or ent.label == \"FAC\" or ent.label == \"NORP\"):\n",
    "            if ent.text in sentence_ge:\n",
    "                persons_list += [ent.text]\n",
    "            else:\n",
    "                ent_text_clean = ent.text.replace(\"the\",\"\").replace(\"The\",\"\").strip()\n",
    "                if ent_text_clean in sentence_ge:\n",
    "                    persons_list += [ent_text_clean]\n",
    "    return persons_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the entities\n",
    "\n",
    "dataset['entities']=dataset.apply(lambda row: get_entities(row[\"sentences_en\"],\n",
    "                                                         row[\"sentences_ge\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing entities from sentences\n",
    "\n",
    "dataset['sentences_en_no_entities'] = dataset.apply(lambda row: remove_entities(row['sentences_en'], row[\"entities\"]), axis=1)\n",
    "dataset['sentences_ge_no_entities'] = dataset.apply(lambda row: remove_entities(row['sentences_ge'], row[\"entities\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctuation from sentences\n",
    "\n",
    "dataset['sentences_en_clean'] = dataset.apply(lambda row: remove_punct(row['sentences_en_no_entities'], nlp_en), axis=1)\n",
    "dataset['sentences_ge_clean'] = dataset.apply(lambda row: remove_punct(row['sentences_ge_no_entities'], nlp_ge), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a list of some entities removed from sentences\n",
      "\n",
      "8721                    [Carly, Jack, Hal Munson]\n",
      "177     [Henry, David, Emma, Mary Margaret, Neal]\n",
      "8659                [Poirot, Monsieur Desjardeux]\n",
      "Name: entities, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Here is a list of some entities removed from sentences\"+\"\\n\")\n",
    "print(dataset[dataset[\"entities\"].apply(lambda x:x!=[])][\"entities\"].sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create first version of vocabulary for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_words_to_vocab(vocab,sentence,nlp,word_2id,stemmer):\n",
    "    '''\n",
    "    Function creating a mapping between the vocabulary and a MUSE embedding in the corresponding language. It tests\n",
    "    first if the word can be find directly, then if its tokenized form can be found, then its synonyms and then\n",
    "    its stemmed form.\n",
    "    Returns: vocab which is the mapping, out_of_vocab which corresponds to the words which weren't found\n",
    "    '''\n",
    "    mini_sentence = nlp(sentence)\n",
    "    out_of_vocab = []\n",
    "    for token in mini_sentence:\n",
    "        if token.text not in vocab.keys():\n",
    "            try:\n",
    "                vocab[token.text] = word_2id[token.text]\n",
    "            except:\n",
    "                try:\n",
    "                    vocab[token.text] = word_2id[token.lemma_]\n",
    "                except:\n",
    "                    try:\n",
    "                        synonyms = wordnet.synsets(token.text)[0].lemmas()\n",
    "                        for i in range(10):\n",
    "                            synonym = synonyms[i].name()\n",
    "                            try:\n",
    "                                vocab[token.text] = word_2id[synonym]\n",
    "                                break\n",
    "                            except:\n",
    "                                continue\n",
    "                    except:\n",
    "                        try:\n",
    "                            vocab[token.text] = word_2id[stemmer.stem(token.text)]\n",
    "                        except:\n",
    "                            out_of_vocab.append(token.text)\n",
    "    return vocab,out_of_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(sentences,nlp,word2id,stemmer):\n",
    "    '''\n",
    "    Function which iterates the function add_words_to_vocab to compute a vocabulary mapping\n",
    "    '''\n",
    "    vocab_2_embedding_idx = {}\n",
    "    out_of_vocab = []\n",
    "    \n",
    "    for i,sentence in enumerate(sentences):\n",
    "        vocab_2_embedding_idx,out_of_vocab_current = add_words_to_vocab(vocab_2_embedding_idx,sentence,\n",
    "                                                                        nlp,word2id,stemmer)\n",
    "        \n",
    "        for out_of_vocab_word in out_of_vocab_current:\n",
    "            out_of_vocab += [(out_of_vocab_word,i)]\n",
    "            \n",
    "    return vocab_2_embedding_idx,out_of_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_2_embedding_idx_en,out_of_vocab_en = create_vocab(dataset.sentences_en_clean,nlp_en,src_word2id,porter)\n",
    "vocab_2_embedding_idx_ge,out_of_vocab_ge = create_vocab(dataset.sentences_ge_clean,nlp_ge,tgt_word2id,cistem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non german words from german vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiprocessing_remove_words_from_list(list_1,list_2):\n",
    "    '''\n",
    "    Function using multiprocessing to check if a word from a list is not in another list\n",
    "    '''\n",
    "    n_cpu = multiprocessing.cpu_count()\n",
    "    pool = Pool(n_cpu)\n",
    "    n = len(list_1)\n",
    "    split_idx = int(n/n_cpu)\n",
    "    splited_list_1 = [list_1[split_idx*i:split_idx*(i+1)] \n",
    "                                   if i<n_cpu-1 \n",
    "                                   else list_1[split_idx*i:]\n",
    "                                   for i in range(n_cpu)]\n",
    "    \n",
    "    words_to_be_removed_list = pool.starmap(remove_words_from_list,\n",
    "                                            zip(splited_list_1,repeat(list_2)))\n",
    "    \n",
    "    words_to_be_removed = []\n",
    "    for i in range(n_cpu):\n",
    "        words_to_be_removed += words_to_be_removed_list[i]\n",
    "        \n",
    "    return words_to_be_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current vocab computed in the corpus\n",
    "current_german_vocab_corpus = list(vocab_2_embedding_idx_ge.keys())\n",
    "\n",
    "#large list of true german words \n",
    "#source : https://gist.github.com/MarvinJWendt/2f4f4154b8ae218600eb091a5706b5f4#file-wordlist-german-txt\n",
    "german_words = list(pd.read_csv(\"data/german_words/wordlist-german.txt\",header=None)[0])\n",
    "german_words = [str(word).lower() for word in german_words]\n",
    "\n",
    "words_to_be_removed = multiprocessing_remove_words_from_list(current_german_vocab_corpus,german_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a list of some of the words which were embedded in MUSE as vectors in german language\n",
      "\n",
      "['colloquium' 'screenplay' 'tibia' 'proprios' 'unitary' 'falling' 'durga'\n",
      " 'volunteers' 'nuevo' 'quit']\n"
     ]
    }
   ],
   "source": [
    "#Example of words removed from german vocabulary\n",
    "idxs = np.random.choice(np.arange(len(words_to_be_removed)),10)\n",
    "\n",
    "print(\"Here is a list of some of the words which were embedded in MUSE as vectors in german language\"+\"\\n\")\n",
    "print(np.array(words_to_be_removed)[idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_vocabulary(dataset,vocab_2_idx,out_of_vocab,words_to_be_removed):\n",
    "    '''\n",
    "    Function which updates the values of out_of_vocab and vocab_2_idx knowing the words which must be removed\n",
    "    '''\n",
    "    new_out_of_vocab = []\n",
    "    \n",
    "    print(\"Vocabulary length before:{}\".format(len(vocab_2_idx)))\n",
    "    print(\"Out of vocabulary length before:{}\".format(len(out_of_vocab))+\"\\n\")\n",
    "    \n",
    "    for word in words_to_be_removed:\n",
    "        idxs =  list(dataset[dataset.sentences_ge_clean.apply(lambda x: word in x)].index)\n",
    "        if len(idxs)<=3:\n",
    "            for idx in idxs:\n",
    "                new_out_of_vocab += [(word,idx)]       \n",
    "    \n",
    "    for word in [pair[0] for pair in new_out_of_vocab]:\n",
    "        try:\n",
    "            vocab_2_idx.pop(word,None)\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    new_out_of_vocab += out_of_vocab\n",
    "    \n",
    "    print(\"Vocabulary length after:{}\".format(len(vocab_2_idx)))\n",
    "    print(\"Out of vocabulary length after:{}\".format(len(new_out_of_vocab)))\n",
    "    \n",
    "    return vocab_2_idx,new_out_of_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length before:19762\n",
      "Out of vocabulary length before:4962\n",
      "\n",
      "Vocabulary length after:18007\n",
      "Out of vocabulary length after:7195\n"
     ]
    }
   ],
   "source": [
    "vocab_2_embedding_idx_ge,out_of_vocab_ge = update_vocabulary(dataset,\n",
    "                                                             vocab_2_embedding_idx_ge,\n",
    "                                                             out_of_vocab_ge,\n",
    "                                                             words_to_be_removed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process out of vocab german words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_translated_out_of_vocab_words(out_of_vocab_german,word2id,dataset,out_of_vocab_english,\n",
    "                                          muse_word2id,german_words):\n",
    "    '''\n",
    "    Function which checks for each sentence in the dataset which has out of vocab german words. Then, it checks if \n",
    "    the exact same word is found in the English sentence and removes it from both (with incrementing a counter of \n",
    "    non_translated_words if relevant). Then it checks if the out of vocab German words could be split in two\n",
    "    meaning full German words which are embedded in MUSE and separates them if it is the case.\n",
    "    Returns: the updated dataset and the new corpus vocabulary\n",
    "    '''\n",
    "    \n",
    "    dataset[\"non_translated_words\"] = 0\n",
    "    dataset[\"sentences_en_cleaner\"] = dataset[\"sentences_en_clean\"].copy()\n",
    "    dataset[\"sentences_ge_cleaner\"] = dataset[\"sentences_ge_clean\"].copy()\n",
    "    \n",
    "    for k,pair in enumerate(out_of_vocab_german):\n",
    "        word,idx = pair[0],pair[1]\n",
    "        en_sentence = dataset.loc[idx][\"sentences_en_cleaner\"]\n",
    "        ge_sentence = dataset.loc[idx][\"sentences_ge_cleaner\"]\n",
    "        count_non_translated = dataset.loc[idx][\"non_translated_words\"]\n",
    "\n",
    "        ### If the word is exactly the same as a word in the source sentence\n",
    "        if word in en_sentence:\n",
    "            if word not in out_of_vocab_english:\n",
    "                ### if the word is in the english vocab -> it could have been translated\n",
    "                ### count it as a non translated word and remove it from both sentences\n",
    "                dataset.at[idx,\"sentences_en_cleaner\"] = remove_word_from_sentence(en_sentence,word)\n",
    "                dataset.at[idx,\"sentences_ge_cleaner\"] = remove_word_from_sentence(ge_sentence,word)\n",
    "                dataset.at[idx,\"non_translated_words\"] = count_non_translated+1\n",
    "            else:\n",
    "                ### if the word is not in the english vocab -> it could not have been translated\n",
    "                ### don't count it as a non translated word and remove it from both sentences\n",
    "                dataset.at[idx,\"sentences_en_cleaner\"] = remove_word_from_sentence(en_sentence,word)\n",
    "                dataset.at[idx,\"sentences_ge_cleaner\"] = remove_word_from_sentence(ge_sentence,word)\n",
    "                \n",
    "            continue\n",
    "\n",
    "        ### Test for subwords\n",
    "        subword = word[:-1]\n",
    "        while len(subword)>0:\n",
    "            if (subword in (muse_word2id.keys())):\n",
    "                subword1 = subword\n",
    "                subword2 = word[len(subword1):]\n",
    "                if (subword2 in muse_word2id.keys() and (subword1 in german_words) and subword2 in (german_words)):\n",
    "                    if len(subword2)>2 and len(subword1)>2:\n",
    "                        ### if the word can be splitted in 2 meaningful words do it\n",
    "                        dataset.at[idx,\"sentences_ge_cleaner\"] = ge_sentence.replace(word,subword1+\" \"+subword2)\n",
    "                        word2id[subword1] = muse_word2id[subword1]\n",
    "                        word2id[subword2] = muse_word2id[subword2]\n",
    "                        break\n",
    "                    elif len(subword1)>2 and len(subword2)<=2:\n",
    "                        ### if the word can be found by using a shorter version which can be found in vocab\n",
    "                        dataset.at[idx,\"sentences_ge_cleaner\"] = ge_sentence.replace(word,subword1)\n",
    "                        word2id[subword1] = muse_word2id[subword1]\n",
    "                        break\n",
    "                    elif len(subword1)<=2 and len(subword2)>2:\n",
    "                        ### if the word can be found by using a shorter version which can be found in vocab\n",
    "                        dataset.at[idx,\"sentences_ge_cleaner\"] = ge_sentence.replace(word,subword2)\n",
    "                        word2id[subword2] = muse_word2id[subword2]\n",
    "                        break\n",
    "                    else:\n",
    "                        subword = subword[:-1]\n",
    "                else:\n",
    "                    subword = subword[:-1]\n",
    "\n",
    "            else:\n",
    "                subword = subword[:-1]\n",
    "    return dataset,word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the dataset and the corpus vocabulary\n",
    "dataset,new_vocab_2_embedding = process_translated_out_of_vocab_words(out_of_vocab_ge,vocab_2_embedding_idx_ge,\n",
    "                                                                      dataset,\n",
    "                                                                      out_of_vocab_en,tgt_word2id,german_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sentences for embedding by removing out of vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final vocabulary for the corpus in English and in German\n",
    "vocab_fin_en = list(vocab_2_embedding_idx_en.keys())\n",
    "vocab_fin_ge = list(vocab_2_embedding_idx_ge.keys())\n",
    "\n",
    "#Sentences preprocessed for embedding\n",
    "dataset[\"sentences_en_preprocessed\"] = dataset.sentences_en_cleaner.apply(lambda x:remove_multiple_words_from_sentence(x,vocab_fin_en,True))\n",
    "dataset[\"sentences_ge_preprocessed\"] = dataset.sentences_ge_cleaner.apply(lambda x:remove_multiple_words_from_sentence(x,vocab_fin_ge,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute correlation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation(src_emb,tgt_emb):\n",
    "    '''\n",
    "    Function implemented in MUSE Github which computes the correlation between two vectors\n",
    "    '''\n",
    "    corr = (src_emb / np.linalg.norm(src_emb)).dot(tgt_emb / np.linalg.norm(tgt_emb))\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(word,language):\n",
    "    '''\n",
    "    Helper function which fetches the embedding of a word given the chosen language in [\"en\",\"ge\"]\n",
    "    '''\n",
    "    if language==\"en\":\n",
    "        return src_embeddings[src_word2id[word]]\n",
    "    else:\n",
    "        return tgt_embeddings[tgt_word2id[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_matrix(words_en_list,words_ge_list):\n",
    "    '''\n",
    "    Function which computes the correlation matrix (discussed in the report)\n",
    "    '''\n",
    "    n = len(words_en_list)\n",
    "    m = len(words_ge_list)\n",
    "    corr_matrix = np.zeros((n,m))\n",
    "    for i,word_en in enumerate(words_en_list):\n",
    "        for j,word_ge in enumerate(words_ge_list):\n",
    "            corr_matrix[i,j] = get_correlation(get_emb(word_en,\"en\"),get_emb(word_ge,\"ge\"))\n",
    "            \n",
    "    return corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_matches(corr_matrix):\n",
    "    '''\n",
    "    Function which finds the best word matches\n",
    "    '''\n",
    "    if len(corr_matrix)==0:\n",
    "        return {}\n",
    "    best_match_row = np.argmax(corr_matrix,axis=0)\n",
    "    best_match_col = np.argmax(corr_matrix,axis=1)\n",
    "    couples = {}\n",
    "    tmp_corr_matrix = corr_matrix.copy()\n",
    "    n = corr_matrix.shape[0] \n",
    "    m = corr_matrix.shape[1]\n",
    "    dim = min(n,m)\n",
    "    while len(couples.keys())<dim:\n",
    "        # iterates over the rows of the correlation matrix\n",
    "        for i in range(n):\n",
    "            # If a value is the maximum of the row and the column, it corresponds to a good word couple.\n",
    "            # The column and the row containing this word are set to zero so they can't be picked in the\n",
    "            # following pass (because correlation values are >0)\n",
    "            if (i == best_match_row[best_match_col[i]]) and (i not in couples.keys()):\n",
    "                couples[i] = best_match_col[i]\n",
    "                tmp_corr_matrix[i,:] = np.zeros(m)\n",
    "                tmp_corr_matrix[:,best_match_col[i]] = np.zeros(n)\n",
    "                best_match_row = np.argmax(tmp_corr_matrix,axis=0)\n",
    "                best_match_col = np.argmax(tmp_corr_matrix,axis=1)\n",
    "    return couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_couples(words_en,words_ge):\n",
    "    '''\n",
    "    Function which returns the best word matches, the correlation scores and the remaining words which did not match\n",
    "    '''\n",
    "    if len(words_en)==0 or len(words_ge)==0:\n",
    "        return np.nan,np.nan,np.nan,np.nan\n",
    "    \n",
    "    words_en_list = words_en.split()\n",
    "    words_ge_list = words_ge.split()\n",
    "    #computing the correlation matrix\n",
    "    corr_mat = get_corr_matrix(words_en_list,words_ge_list)\n",
    "    #computing the best word matches\n",
    "    word_couples_idx = get_word_matches(corr_mat)\n",
    "    score = 0\n",
    "    \n",
    "    for i in word_couples_idx.keys():\n",
    "        score+=corr_mat[i,word_couples_idx[i]]\n",
    "    score/=len(word_couples_idx)\n",
    "    \n",
    "    if len(words_en_list)>len(words_ge_list):\n",
    "        #Dealing with non picked words english words \n",
    "        kept_words_idx = np.array(list(word_couples_idx.keys()))\n",
    "        left_words_idx = np.setdiff1d(np.arange(len(words_en_list)),kept_words_idx)\n",
    "        left_words = [words_en_list[i] for i in left_words_idx]\n",
    "        non_match_corr = np.mean(np.max(corr_mat[left_words_idx,:],axis=1))\n",
    "    elif len(words_en_list)<len(words_ge_list):\n",
    "        #Dealing with non picked words german words \n",
    "        kept_words_idx = np.array(list(word_couples_idx.values()))\n",
    "        left_words_idx = np.setdiff1d(np.arange(len(words_ge_list)),kept_words_idx)\n",
    "        left_words = [words_ge_list[i] for i in left_words_idx]\n",
    "        non_match_corr = np.mean(np.max(corr_mat[:,left_words_idx],axis=0))\n",
    "    else:\n",
    "        left_words = []\n",
    "        non_match_corr=np.nan\n",
    "        \n",
    "    word_couples = {}\n",
    "    for key,val in word_couples_idx.items():\n",
    "        word_couples[words_en_list[key]] = words_ge_list[val]\n",
    "        \n",
    "        \n",
    "    return word_couples,score,left_words,non_match_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the correlation features\n",
    "\n",
    "dataset[\"correlation\"] = dataset.apply(lambda row:get_word_couples(row[\"sentences_en_preprocessed\"],row[\"sentences_ge_preprocessed\"])[1],axis=1)\n",
    "dataset[\"left_words\"] = dataset.apply(lambda row:get_word_couples(row[\"sentences_en_preprocessed\"],row[\"sentences_ge_preprocessed\"])[2],axis=1)\n",
    "dataset[\"non_match_correlation\"] = dataset.apply(lambda row:get_word_couples(row[\"sentences_en_preprocessed\"],row[\"sentences_ge_preprocessed\"])[3],axis=1)\n",
    "dataset[\"n_left_words\"] = dataset[\"left_words\"].apply(lambda x:len(x) if isinstance(x,list) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computed the ratio between non_translated words and sentence length\n",
    "\n",
    "dataset[\"max_len_en_len_ge\"] = dataset.apply(lambda row:max(len(row[\"sentences_en_preprocessed\"].split()),\n",
    "                                                len(row[\"sentences_ge_preprocessed\"].split())),axis=1)\n",
    "dataset[\"non_translated_ratio\"] = dataset[\"non_translated_words\"]/dataset[\"max_len_en_len_ge\"]\n",
    "dataset[\"non_translated_ratio\"] = dataset[\"non_translated_ratio\"].fillna(0)\n",
    "dataset[\"non_translated_ratio\"] = dataset[\"non_translated_ratio\"].apply(lambda x:x if not np.isinf(x) else 0)\n",
    "dataset = dataset.drop(columns=[\"max_len_en_len_ge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values with the mean of the features\n",
    "\n",
    "dataset[\"correlation\"] = dataset[\"correlation\"].fillna(dataset[\"correlation\"].mean())\n",
    "dataset[\"non_match_correlation\"] = dataset[\"non_match_correlation\"].fillna(dataset[\"non_match_correlation\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from laserembeddings import Laser\n",
    "\n",
    "laser = Laser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_laser_correlation(src_sentence,tgt_sentence):\n",
    "    '''\n",
    "    Function to compute the laser correlation between vectors\n",
    "    '''\n",
    "    embeddings = laser.embed_sentences([src_sentence, tgt_sentence], lang=['en', 'de'])\n",
    "    src_emb, tgt_emb = embeddings[0], embeddings[1]\n",
    "    corr = (src_emb / np.linalg.norm(src_emb)).dot(tgt_emb / np.linalg.norm(tgt_emb))\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The two following cells should not be run as it is very long to compute the laser embeddings (around 40 minutes for the whole\n",
    "#dataset) so these features are provided and merged with the dataset just below\n",
    "\n",
    "#dataset['sentence_correlation'] = dataset.apply(lambda row: \n",
    "#                                                get_laser_correlation(row[\"sentences_en\"], row[\"sentences_ge\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting the sentence level embeddings from LASER\n",
    "# set of 1024 features per sentence \n",
    "\n",
    "# full_laser_english = laser.embed_sentences(dataset['sentences_en'].tolist(), lang='en')\n",
    "# full_laser_german = laser.embed_sentences(dataset['sentences_ge'].tolist(), lang='de')\n",
    "\n",
    "# these are saved in the following lines, and loaded for the neural network regression model\n",
    "# np.save('laser_1024_english', full_laser_english)\n",
    "# np.save('laser_1024_german', full_laser_german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "laser_test = pd.read_pickle('data/laser_embeddings/test_dataset_corrleations_laser.pickle')[[\"sentence_correlation\"]]\n",
    "laser_train_val =  pd.read_pickle('data/laser_embeddings/dataset_corrleations_laser.pickle')[[\"sentence_correlation\"]]\n",
    "all_laser = pd.concat([laser_train_val,laser_test])\n",
    "all_laser = all_laser.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset = pd.merge(dataset,all_laser,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_dataset.to_pickle(\"data/datasets/dataset_with_features.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
