{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_embeddings = pd.read_pickle('../data/dataset_correlations_v1.pickle')\n",
    "dataset_embeddings = pd.read_pickle('../data/dataset_correlations_v2.pickle')\n",
    "dataset_baseline = pd.read_pickle('../data/dataset_v1.pickle')\n",
    "dataset_laser = pd.read_pickle('../data/dataset_corrleations_laser.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentences_en', 'sentences_ge', 'scores', 'person',\n",
       "       'sentences_en_no_propnouns', 'sentences_ge_no_propnouns',\n",
       "       'sentences_en_clean', 'sentences_ge_clean', 'non_translated_words',\n",
       "       'sentences_en_cleaner', 'sentences_ge_cleaner', 'sentences_en_final',\n",
       "       'sentences_ge_final', 'length_ge', 'length_en', 'distance',\n",
       "       'correlation', 'embedded_words_matched_max',\n",
       "       'embedded_words_matched_min', 'weights', 'weighted_corr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_embeddings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences_en</th>\n",
       "      <th>sentences_ge</th>\n",
       "      <th>scores</th>\n",
       "      <th>person</th>\n",
       "      <th>sentences_en_no_propnouns</th>\n",
       "      <th>sentences_ge_no_propnouns</th>\n",
       "      <th>sentences_en_clean</th>\n",
       "      <th>sentences_ge_clean</th>\n",
       "      <th>non_translated_words</th>\n",
       "      <th>sentences_en_cleaner</th>\n",
       "      <th>...</th>\n",
       "      <th>sentences_en_final</th>\n",
       "      <th>sentences_ge_final</th>\n",
       "      <th>length_ge</th>\n",
       "      <th>length_en</th>\n",
       "      <th>distance</th>\n",
       "      <th>correlation</th>\n",
       "      <th>embedded_words_matched_max</th>\n",
       "      <th>embedded_words_matched_min</th>\n",
       "      <th>weights</th>\n",
       "      <th>weighted_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>José Ortega y Gasset visited Husserl at Freibu...</td>\n",
       "      <td>1934 besuchte José Ortega y Gasset Husserl in ...</td>\n",
       "      <td>1.101697</td>\n",
       "      <td>[José Ortega y Gasset, Husserl, Freiburg]</td>\n",
       "      <td>visited  at  in 1934.</td>\n",
       "      <td>1934 besuchte   in .</td>\n",
       "      <td>visited at in</td>\n",
       "      <td>besuchte in</td>\n",
       "      <td>0</td>\n",
       "      <td>visited</td>\n",
       "      <td>...</td>\n",
       "      <td>visited</td>\n",
       "      <td>besuchte</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.518761</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.518761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>However, a disappointing ninth in China meant ...</td>\n",
       "      <td>Eine enttäuschende Neunte in China bedeutete j...</td>\n",
       "      <td>-0.516656</td>\n",
       "      <td>[China]</td>\n",
       "      <td>However, a disappointing ninth in  meant that ...</td>\n",
       "      <td>Eine enttäuschende Neunte in  bedeutete jedoch...</td>\n",
       "      <td>however a disappointing ninth in meant that he...</td>\n",
       "      <td>eine enttäuschende neunte in bedeutete jedoch ...</td>\n",
       "      <td>0</td>\n",
       "      <td>however disappointing ninth meant dropped back...</td>\n",
       "      <td>...</td>\n",
       "      <td>however disappointing ninth meant dropped back...</td>\n",
       "      <td>enttäuschende neunte bedeutete jedoch gesamtwe...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.619618</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.619618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his diary, Chase wrote that the release of ...</td>\n",
       "      <td>In seinem Tagebuch, Chase schrieb, dass die Ve...</td>\n",
       "      <td>-2.226388</td>\n",
       "      <td>[Chase, Mason, Slidell]</td>\n",
       "      <td>In his diary,  wrote that the release of  and ...</td>\n",
       "      <td>In seinem Tagebuch,  schrieb, dass die Veröffe...</td>\n",
       "      <td>in his diary wrote that the release of and was...</td>\n",
       "      <td>in seinem tagebuch schrieb dass die veröffentl...</td>\n",
       "      <td>0</td>\n",
       "      <td>diary wrote release like gall wormwood</td>\n",
       "      <td>...</td>\n",
       "      <td>diary wrote release like gall wormwood</td>\n",
       "      <td>tagebuch schrieb veröffentlichung galle wermut</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.633080</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.527567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>Schwere Arquebuses auf Waggons montiert wurden...</td>\n",
       "      <td>-0.827379</td>\n",
       "      <td>[]</td>\n",
       "      <td>Heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>Schwere Arquebuses auf Waggons montiert wurden...</td>\n",
       "      <td>heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>schwere arquebuses auf waggons montiert wurden...</td>\n",
       "      <td>4</td>\n",
       "      <td>heavy mounted wagons called</td>\n",
       "      <td>...</td>\n",
       "      <td>heavy mounted wagons called</td>\n",
       "      <td>schwere waggons montiert wurden genannt</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.626568</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.278475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once North Pacific salmon die off after spawni...</td>\n",
       "      <td>Sobald der nordpazifische Lachs nach dem Laich...</td>\n",
       "      <td>0.364695</td>\n",
       "      <td>[]</td>\n",
       "      <td>Once North Pacific salmon die off after spawni...</td>\n",
       "      <td>Sobald der nordpazifische Lachs nach dem Laich...</td>\n",
       "      <td>once north pacific salmon die off after spawni...</td>\n",
       "      <td>sobald der nordpazifische lachs nach dem laich...</td>\n",
       "      <td>0</td>\n",
       "      <td>north pacific salmon die spawning usually loca...</td>\n",
       "      <td>...</td>\n",
       "      <td>north pacific salmon die spawning usually loca...</td>\n",
       "      <td>sobald lachs laichen abstirbt fressen regel lo...</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0.583080</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.458134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sentences_en  \\\n",
       "0  José Ortega y Gasset visited Husserl at Freibu...   \n",
       "1  However, a disappointing ninth in China meant ...   \n",
       "2  In his diary, Chase wrote that the release of ...   \n",
       "3  Heavy arquebuses mounted on wagons were called...   \n",
       "4  Once North Pacific salmon die off after spawni...   \n",
       "\n",
       "                                        sentences_ge    scores  \\\n",
       "0  1934 besuchte José Ortega y Gasset Husserl in ...  1.101697   \n",
       "1  Eine enttäuschende Neunte in China bedeutete j... -0.516656   \n",
       "2  In seinem Tagebuch, Chase schrieb, dass die Ve... -2.226388   \n",
       "3  Schwere Arquebuses auf Waggons montiert wurden... -0.827379   \n",
       "4  Sobald der nordpazifische Lachs nach dem Laich...  0.364695   \n",
       "\n",
       "                                      person  \\\n",
       "0  [José Ortega y Gasset, Husserl, Freiburg]   \n",
       "1                                    [China]   \n",
       "2                    [Chase, Mason, Slidell]   \n",
       "3                                         []   \n",
       "4                                         []   \n",
       "\n",
       "                           sentences_en_no_propnouns  \\\n",
       "0                              visited  at  in 1934.   \n",
       "1  However, a disappointing ninth in  meant that ...   \n",
       "2  In his diary,  wrote that the release of  and ...   \n",
       "3  Heavy arquebuses mounted on wagons were called...   \n",
       "4  Once North Pacific salmon die off after spawni...   \n",
       "\n",
       "                           sentences_ge_no_propnouns  \\\n",
       "0                               1934 besuchte   in .   \n",
       "1  Eine enttäuschende Neunte in  bedeutete jedoch...   \n",
       "2  In seinem Tagebuch,  schrieb, dass die Veröffe...   \n",
       "3  Schwere Arquebuses auf Waggons montiert wurden...   \n",
       "4  Sobald der nordpazifische Lachs nach dem Laich...   \n",
       "\n",
       "                                  sentences_en_clean  \\\n",
       "0                                      visited at in   \n",
       "1  however a disappointing ninth in meant that he...   \n",
       "2  in his diary wrote that the release of and was...   \n",
       "3  heavy arquebuses mounted on wagons were called...   \n",
       "4  once north pacific salmon die off after spawni...   \n",
       "\n",
       "                                  sentences_ge_clean  non_translated_words  \\\n",
       "0                                        besuchte in                     0   \n",
       "1  eine enttäuschende neunte in bedeutete jedoch ...                     0   \n",
       "2  in seinem tagebuch schrieb dass die veröffentl...                     0   \n",
       "3  schwere arquebuses auf waggons montiert wurden...                     4   \n",
       "4  sobald der nordpazifische lachs nach dem laich...                     0   \n",
       "\n",
       "                                sentences_en_cleaner  ...  \\\n",
       "0                                            visited  ...   \n",
       "1  however disappointing ninth meant dropped back...  ...   \n",
       "2             diary wrote release like gall wormwood  ...   \n",
       "3                        heavy mounted wagons called  ...   \n",
       "4  north pacific salmon die spawning usually loca...  ...   \n",
       "\n",
       "                                  sentences_en_final  \\\n",
       "0                                            visited   \n",
       "1  however disappointing ninth meant dropped back...   \n",
       "2             diary wrote release like gall wormwood   \n",
       "3                        heavy mounted wagons called   \n",
       "4  north pacific salmon die spawning usually loca...   \n",
       "\n",
       "                                  sentences_ge_final length_ge  length_en  \\\n",
       "0                                           besuchte         1          1   \n",
       "1  enttäuschende neunte bedeutete jedoch gesamtwe...         8          8   \n",
       "2     tagebuch schrieb veröffentlichung galle wermut         5          6   \n",
       "3            schwere waggons montiert wurden genannt         5          4   \n",
       "4  sobald lachs laichen abstirbt fressen regel lo...        11         14   \n",
       "\n",
       "   distance  correlation  embedded_words_matched_max  \\\n",
       "0         0     0.518761                           1   \n",
       "1         0     0.619618                           8   \n",
       "2         1     0.633080                           6   \n",
       "3        -1     0.626568                           5   \n",
       "4         3     0.583080                          14   \n",
       "\n",
       "   embedded_words_matched_min   weights  weighted_corr  \n",
       "0                           1  1.000000       0.518761  \n",
       "1                           8  1.000000       0.619618  \n",
       "2                           5  0.833333       0.527567  \n",
       "3                           4  0.444444       0.278475  \n",
       "4                          11  0.785714       0.458134  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_embeddings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_embeddings_features = dataset_embeddings[['non_translated_words',\n",
    "                                                 'distance',\n",
    "                                                 'correlation',\n",
    "                                                 'weights']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentences_en', 'sentences_ge', 'scores', 'english_sentence_length',\n",
       "       'german_sentence_length', 'sentence_length_difference', 'german_verbs',\n",
       "       'english_verbs', 'german_adjectives', 'english_adjectives',\n",
       "       'german_adverbs', 'english_adverbs', 'german_nouns', 'english_nouns',\n",
       "       'english_no_punctuation', 'german_no_punctuation',\n",
       "       'english_no_stop_words', 'german_no_stop_words', 'english_lemma',\n",
       "       'german_lemma', 'english_sentence_sentiment',\n",
       "       'german_sentence_sentiment', 'std_english_sentence_sentiment',\n",
       "       'std_german_sentence_sentiment', 'english_sentence_lemma_sentiment',\n",
       "       'german_sentence_lemma_sentiment', 'max_sentiment_english',\n",
       "       'max_sentiment_german', 'std_max_english_sentiment',\n",
       "       'std_max_german_sentiment', 'verbs_diff', 'adjectives_diff',\n",
       "       'adverbs_diff', 'nouns_diff'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_baseline.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4717                                                     \n",
       "5761                                                leere\n",
       "471                                                Zivile\n",
       "3540                                                     \n",
       "2860    Tatsächlich breiten populistischen amerikanischer\n",
       "1015                                        Zentrale kurz\n",
       "3716                                          demontierte\n",
       "3630                                              weitere\n",
       "4957                                                     \n",
       "3928                                       amerikanischer\n",
       "2368                                          nordöstlich\n",
       "392                                                      \n",
       "4980                                                     \n",
       "1964                                                     \n",
       "3701                                   fünfzehnte rechten\n",
       "478                                                      \n",
       "7747                                                     \n",
       "3227                                                     \n",
       "2398                                          gemeinsamen\n",
       "2679                                                 25 .\n",
       "4787                       Vereinigten große kommerzielle\n",
       "4355                                                 11 .\n",
       "2178                                                  3 .\n",
       "611                                                      \n",
       "5483                                                     \n",
       "5398                                                     \n",
       "3267                                              Weitere\n",
       "5872                                        21 . internen\n",
       "6378                                              anderen\n",
       "864                                         vorübergehend\n",
       "4878                                                     \n",
       "3889                                                     \n",
       "1060                                                     \n",
       "7891                                                kurze\n",
       "2403                                                     \n",
       "5824                            makaronischen rumänischen\n",
       "7430                                                     \n",
       "22                                               gesamtes\n",
       "4588                                                     \n",
       "1848                                                     \n",
       "Name: german_adjectives, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.width=None\n",
    "dataset_baseline['german_adjectives'].sample(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_baseline_features = dataset_baseline[['english_sentence_length',\n",
    "                                              'std_max_english_sentiment',\n",
    "                                              'german_sentence_length',\n",
    "                                              'std_max_german_sentiment',\n",
    "                                              'max_sentiment_english',\n",
    "                                              'max_sentiment_german',\n",
    "                                              'sentence_length_difference',\n",
    "                                              'verbs_diff', \n",
    "                                              'adjectives_diff',\n",
    "                                              'adverbs_diff',\n",
    "                                              'nouns_diff'\n",
    "                                             ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentences_en', 'sentences_ge', 'scores', 'person',\n",
       "       'sentences_en_no_propnouns', 'sentences_ge_no_propnouns',\n",
       "       'sentences_en_clean', 'sentences_ge_clean', 'non_translated_words',\n",
       "       'sentences_en_cleaner', 'sentences_ge_cleaner', 'sentences_en_final',\n",
       "       'sentences_ge_final', 'length_ge', 'length_en', 'distance',\n",
       "       'correlation', 'std_correlations', 'sentence_correlation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_laser.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_laser_features = dataset_laser[['sentence_correlation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_features = pd.concat((dataset_baseline_features, dataset_embeddings_features,\n",
    "                              dataset_laser_features),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_features = dataset_features[['correlation','non_translated_words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correlation</th>\n",
       "      <th>non_translated_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.518761</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.619618</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.633080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.626568</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.583080</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>0.582558</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>0.684928</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>0.585751</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>0.659042</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>0.637429</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      correlation  non_translated_words\n",
       "0        0.518761                     0\n",
       "1        0.619618                     0\n",
       "2        0.633080                     0\n",
       "3        0.626568                     4\n",
       "4        0.583080                     0\n",
       "...           ...                   ...\n",
       "7995     0.582558                     2\n",
       "7996     0.684928                     0\n",
       "7997     0.585751                     3\n",
       "7998     0.659042                     0\n",
       "7999     0.637429                     0\n",
       "\n",
       "[8000 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_features_list = list(dataset_features.columns)\n",
    "dataset_features_arr = np.array(dataset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labels = dataset_embeddings['scores']\n",
    "dataset_labels_arr = np.array(dataset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_features = dataset_features_arr[:7000]\n",
    "# train_labels = dataset_labels_arr[:7000]\n",
    "\n",
    "# val_features = dataset_features_arr[7000:]\n",
    "# val_labels = dataset_labels_arr[7000:]\n",
    "\n",
    "train_features = np.concatenate((dataset_features_arr[:6000], dataset_features_arr[7000:]))\n",
    "train_labels = np.concatenate((dataset_labels_arr[:6000], dataset_labels_arr[7000:]))\n",
    "\n",
    "val_features = dataset_features_arr[6000:7000]\n",
    "val_labels = dataset_labels_arr[6000:7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (7000, 2)\n",
      "Training Labels Shape: (7000,)\n",
      "Testing Features Shape: (1000, 2)\n",
      "Testing Labels Shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', val_features.shape)\n",
    "print('Testing Labels Shape:', val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Depth 6\n",
      "RMSE: 0.8589019911175123\n",
      "Pearson 0.10972793723320023\n",
      "Mean Absolute Error: 0.5194\n",
      "\n",
      "Max Depth 7\n",
      "RMSE: 0.8597558268113434\n",
      "Pearson 0.10490388283102567\n",
      "Mean Absolute Error: 0.5203\n",
      "\n",
      "Max Depth 8\n",
      "RMSE: 0.860927066160044\n",
      "Pearson 0.09938762746855294\n",
      "Mean Absolute Error: 0.5214\n",
      "\n",
      "Max Depth 9\n",
      "RMSE: 0.8628274311378396\n",
      "Pearson 0.0917707464901201\n",
      "Mean Absolute Error: 0.5231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for max_depth in range(1,8):\n",
    "    rf = RandomForestRegressor(n_estimators = 1000, random_state = 666, max_depth=max_depth)\n",
    "    rf.fit(train_features, train_labels);\n",
    "    predictions = rf.predict(val_features)\n",
    "    pearson = pearsonr(val_labels, predictions)\n",
    "    errors = abs(predictions - val_labels)\n",
    "    print('Max Depth', max_depth)\n",
    "    print('RMSE:', rmse(predictions,val_labels))\n",
    "    print(f\"Pearson {pearson[0]}\")\n",
    "    print('Mean Absolute Error:', round(np.mean(errors), 4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: sentence_correlation Importance: 0.27\n",
      "Variable: correlation          Importance: 0.22\n",
      "Variable: non_translated_words Importance: 0.12\n",
      "Variable: english_sentence_length Importance: 0.07\n",
      "Variable: german_sentence_length Importance: 0.05\n",
      "Variable: std_max_english_sentiment Importance: 0.04\n",
      "Variable: max_sentiment_english Importance: 0.04\n",
      "Variable: nouns_diff           Importance: 0.04\n",
      "Variable: distance             Importance: 0.04\n",
      "Variable: weights              Importance: 0.04\n",
      "Variable: verbs_diff           Importance: 0.02\n",
      "Variable: std_max_german_sentiment Importance: 0.01\n",
      "Variable: max_sentiment_german Importance: 0.01\n",
      "Variable: sentence_length_difference Importance: 0.01\n",
      "Variable: adjectives_diff      Importance: 0.01\n",
      "Variable: adverbs_diff         Importance: 0.0\n"
     ]
    }
   ],
   "source": [
    "importances = list(rf.feature_importances_)\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(dataset_features_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear\n",
      "RMSE: 0.872707092652995\n",
      "Pearson 0.1485700358409105\n",
      "Mean Absolute Error: 0.4844\n",
      "\n",
      "poly\n",
      "RMSE: 0.8770801165456373\n",
      "Pearson 0.09819970342325901\n",
      "Mean Absolute Error: 0.4881\n",
      "\n",
      "rbf\n",
      "RMSE: 0.8770824454598339\n",
      "Pearson 0.1019492145778241\n",
      "Mean Absolute Error: 0.4872\n",
      "\n",
      "sigmoid\n",
      "RMSE: 117.05570695945646\n",
      "Pearson -0.07324938173141567\n",
      "Mean Absolute Error: 83.2121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in ['linear','poly','rbf','sigmoid']:\n",
    "    clf_t = SVR(kernel=k)\n",
    "    clf_t.fit(train_features, train_labels)\n",
    "    print(k)\n",
    "    predictions = clf_t.predict(val_features)\n",
    "    pearson = pearsonr(val_labels, predictions)\n",
    "    errors = abs(predictions - val_labels)\n",
    "    print(f'RMSE: {rmse(predictions,val_labels)}')\n",
    "    print(f'Pearson {pearson[0]}')\n",
    "    print('Mean Absolute Error:', round(np.mean(errors), 4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=16, out_features=4, bias=True)\n",
      "  (fc2): Linear(in_features=4, out_features=2, bias=True)\n",
      "  (fc3): Linear(in_features=2, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, x):\n",
    "        super(Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(x.shape[1], 4) \n",
    "        nn.init.xavier_uniform_(self.fc1.weight, gain=1.0)\n",
    "        self.fc2 = nn.Linear(4, 2)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight, gain=1.0)\n",
    "        self.fc3 = nn.Linear(2, 1)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight, gain=1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net(train_features)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_ten, train_labels_ten = Tensor(train_features), Tensor(train_labels.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = train_features_ten.mean(dim=0, keepdim=True)\n",
    "stds = train_features_ten.std(dim=0, keepdim=True)\n",
    "normalized_train = (train_features_ten - means) / stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features_ten = (Tensor(val_features)- means) / stds\n",
    "test_labels = Tensor((val_labels).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TensorDataset(normalized_train, train_labels_ten)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Epoch:  0   -0.035912439776535826\n",
      "Train Epoch:  0   -0.009848891028516292\n",
      "Test Epoch:  1   -0.02448051079446147\n",
      "Train Epoch:  1   -0.0028065159247310473\n",
      "Test Epoch:  2   -0.006077330332305691\n",
      "Train Epoch:  2   0.01265311443772594\n",
      "Test Epoch:  3   0.01549981453290309\n",
      "Train Epoch:  3   0.033033111672348354\n",
      "Test Epoch:  4   0.04045530992625532\n",
      "Train Epoch:  4   0.05473110857656334\n",
      "Test Epoch:  5   0.06007420435868921\n",
      "Train Epoch:  5   0.07010967271360326\n",
      "Test Epoch:  6   0.08049852225648294\n",
      "Train Epoch:  6   0.08465015512908337\n",
      "Test Epoch:  7   0.08669786421597145\n",
      "Train Epoch:  7   0.09410430353974464\n",
      "Test Epoch:  8   0.09606216354017541\n",
      "Train Epoch:  8   0.1024164968229119\n",
      "Test Epoch:  9   0.09808805085023252\n",
      "Train Epoch:  9   0.10934727057539548\n",
      "Test Epoch:  10   0.1026454269717271\n",
      "Train Epoch:  10   0.11638492884978263\n",
      "Test Epoch:  11   0.1016423847069589\n",
      "Train Epoch:  11   0.12113895371286013\n",
      "Test Epoch:  12   0.10013760398659262\n",
      "Train Epoch:  12   0.12500286515840692\n",
      "Test Epoch:  13   0.10185592082203425\n",
      "Train Epoch:  13   0.12862337028500415\n",
      "Test Epoch:  14   0.10227263795738827\n",
      "Train Epoch:  14   0.13172021025612604\n",
      "Test Epoch:  15   0.10395034374755419\n",
      "Train Epoch:  15   0.13379141692900687\n",
      "Test Epoch:  16   0.10331974310943867\n",
      "Train Epoch:  16   0.13548055017014254\n",
      "Test Epoch:  17   0.10259967018174514\n",
      "Train Epoch:  17   0.1376258196780734\n",
      "Test Epoch:  18   0.10129356177216689\n",
      "Train Epoch:  18   0.1391705048690896\n",
      "Test Epoch:  19   0.10215454280780173\n",
      "Train Epoch:  19   0.14154282036927318\n",
      "Test Epoch:  20   0.09884522836452976\n",
      "Train Epoch:  20   0.14232062062129047\n",
      "Test Epoch:  21   0.09302950847633573\n",
      "Train Epoch:  21   0.1443088954545919\n",
      "Test Epoch:  22   0.0971982477459189\n",
      "Train Epoch:  22   0.14671943509481242\n",
      "Test Epoch:  23   0.10044819510996884\n",
      "Train Epoch:  23   0.14740876345085727\n",
      "Test Epoch:  24   0.09547278428665631\n",
      "Train Epoch:  24   0.14810377319179366\n",
      "Test Epoch:  25   0.09524230784996035\n",
      "Train Epoch:  25   0.14928835142151445\n",
      "Test Epoch:  26   0.09654943954349704\n",
      "Train Epoch:  26   0.15219928452672948\n",
      "Test Epoch:  27   0.09370295607022329\n",
      "Train Epoch:  27   0.15294525623916874\n",
      "Test Epoch:  28   0.09314364710808778\n",
      "Train Epoch:  28   0.15489506181133217\n",
      "Test Epoch:  29   0.09123186504975797\n",
      "Train Epoch:  29   0.15626683700638205\n",
      "Test Epoch:  30   0.08783230300440438\n",
      "Train Epoch:  30   0.1578570242936554\n",
      "Test Epoch:  31   0.09231837123358334\n",
      "Train Epoch:  31   0.1584457405165441\n",
      "Test Epoch:  32   0.0921612165998576\n",
      "Train Epoch:  32   0.16014840164467026\n",
      "Test Epoch:  33   0.0920360076822977\n",
      "Train Epoch:  33   0.16103112542993706\n",
      "Test Epoch:  34   0.09332519626246803\n",
      "Train Epoch:  34   0.16185597900650261\n",
      "Test Epoch:  35   0.09636935565612012\n",
      "Train Epoch:  35   0.16223421446060243\n",
      "Test Epoch:  36   0.09579621668178716\n",
      "Train Epoch:  36   0.16297992507490833\n",
      "Test Epoch:  37   0.09552515807831506\n",
      "Train Epoch:  37   0.1637513466992105\n",
      "Test Epoch:  38   0.09675847229624707\n",
      "Train Epoch:  38   0.16439947149687414\n",
      "Test Epoch:  39   0.09564195685774426\n",
      "Train Epoch:  39   0.16524904336984692\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(40):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    with torch.no_grad():\n",
    "        predictions = net(val_features_ten)\n",
    "        test_loss = criterion(predictions, test_labels).item()\n",
    "        pearson = pearsonr(val_labels, predictions.numpy().reshape(-1,))[0]\n",
    "        print('Test Epoch: ', epoch, \" \", pearson)\n",
    "        \n",
    "        #train correlation\n",
    "        predictions = net(normalized_train)\n",
    "        pearson = pearsonr(train_labels, predictions.numpy().reshape(-1,))[0]\n",
    "        print('Train Epoch: ', epoch, \" \", pearson)\n",
    "        \n",
    "#     print('Train loss', epoch, running_loss)\n",
    "#     print('Test loss', epoch, test_loss)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "predictions = net(val_features_ten).detach().numpy().reshape(-1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8615368467116323\n",
      "Pearson 0.09564195685774426\n",
      "Mean Absolute Error: 0.5158\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pearson = pearsonr(val_labels, predictions)\n",
    "errors = abs(predictions - val_labels)\n",
    "print(f'RMSE: {rmse(predictions,val_labels)}')\n",
    "print(f'Pearson {pearson[0]}')\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 4))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
