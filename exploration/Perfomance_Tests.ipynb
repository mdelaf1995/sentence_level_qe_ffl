{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from scipy.stats.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_embeddings = pd.read_pickle('../data/dataset_correlations_v1.pickle')\n",
    "dataset_embeddings = pd.read_pickle('../data/dataset_correlations_v2.pickle')\n",
    "dataset_baseline = pd.read_pickle('../data/dataset_v1.pickle')\n",
    "dataset_laser = pd.read_pickle('../data/dataset_corrleations_laser.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentences_en', 'sentences_ge', 'scores', 'person',\n",
       "       'sentences_en_no_propnouns', 'sentences_ge_no_propnouns',\n",
       "       'sentences_en_clean', 'sentences_ge_clean', 'non_translated_words',\n",
       "       'sentences_en_cleaner', 'sentences_ge_cleaner', 'sentences_en_final',\n",
       "       'sentences_ge_final', 'length_ge', 'length_en', 'distance',\n",
       "       'correlation', 'embedded_words_matched_max',\n",
       "       'embedded_words_matched_min', 'weights', 'weighted_corr'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_embeddings.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_embeddings_features = dataset_embeddings[['non_translated_words',\n",
    "                                                 'distance',\n",
    "                                                 'weighted_corr',\n",
    "                                                 'correlation',\n",
    "                                                 'weighted_corr']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentences_en', 'sentences_ge', 'scores', 'english_sentence_length',\n",
       "       'german_sentence_length', 'sentence_length_difference', 'german_verbs',\n",
       "       'english_verbs', 'german_adjectives', 'english_adjectives',\n",
       "       'german_adverbs', 'english_adverbs', 'german_nouns', 'english_nouns',\n",
       "       'english_no_punctuation', 'german_no_punctuation',\n",
       "       'english_no_stop_words', 'german_no_stop_words', 'english_lemma',\n",
       "       'german_lemma', 'english_sentence_sentiment',\n",
       "       'german_sentence_sentiment', 'std_english_sentence_sentiment',\n",
       "       'std_german_sentence_sentiment', 'english_sentence_lemma_sentiment',\n",
       "       'german_sentence_lemma_sentiment', 'max_sentiment_english',\n",
       "       'max_sentiment_german', 'std_max_english_sentiment',\n",
       "       'std_max_german_sentiment', 'verbs_diff', 'adjectives_diff',\n",
       "       'adverbs_diff', 'nouns_diff'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_baseline.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_baseline_features = dataset_baseline[['std_max_english_sentiment',\n",
    "                                              'std_max_german_sentiment',\n",
    "                                              'max_sentiment_english',\n",
    "                                              'max_sentiment_german',\n",
    "                                              'sentence_length_difference',\n",
    "                                              'verbs_diff', \n",
    "                                              'adjectives_diff',\n",
    "                                              'adverbs_diff',\n",
    "                                              'nouns_diff'\n",
    "                                             ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentences_en', 'sentences_ge', 'scores', 'person',\n",
       "       'sentences_en_no_propnouns', 'sentences_ge_no_propnouns',\n",
       "       'sentences_en_clean', 'sentences_ge_clean', 'non_translated_words',\n",
       "       'sentences_en_cleaner', 'sentences_ge_cleaner', 'sentences_en_final',\n",
       "       'sentences_ge_final', 'length_ge', 'length_en', 'distance',\n",
       "       'correlation', 'std_correlations', 'sentence_correlation'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_laser.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_laser_features = dataset_laser[['sentence_correlation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_features = pd.concat((dataset_baseline_features, dataset_embeddings_features,\n",
    "                              dataset_laser_features),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_features = dataset_features[['correlation','non_translated_words', 'sentence_correlation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weighted_corr</th>\n",
       "      <th>weighted_corr</th>\n",
       "      <th>non_translated_words</th>\n",
       "      <th>sentence_correlation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.518761</td>\n",
       "      <td>0.518761</td>\n",
       "      <td>0</td>\n",
       "      <td>0.938219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.619618</td>\n",
       "      <td>0.619618</td>\n",
       "      <td>0</td>\n",
       "      <td>0.910342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.527567</td>\n",
       "      <td>0.527567</td>\n",
       "      <td>0</td>\n",
       "      <td>0.932145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.278475</td>\n",
       "      <td>0.278475</td>\n",
       "      <td>4</td>\n",
       "      <td>0.934417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.458134</td>\n",
       "      <td>0.458134</td>\n",
       "      <td>0</td>\n",
       "      <td>0.907760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>0.485465</td>\n",
       "      <td>0.485465</td>\n",
       "      <td>2</td>\n",
       "      <td>0.966300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>0.587081</td>\n",
       "      <td>0.587081</td>\n",
       "      <td>0</td>\n",
       "      <td>0.914410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>0.376554</td>\n",
       "      <td>0.376554</td>\n",
       "      <td>3</td>\n",
       "      <td>0.926388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>0.659042</td>\n",
       "      <td>0.659042</td>\n",
       "      <td>0</td>\n",
       "      <td>0.960786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>0.463585</td>\n",
       "      <td>0.463585</td>\n",
       "      <td>0</td>\n",
       "      <td>0.905830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      weighted_corr  weighted_corr  non_translated_words  sentence_correlation\n",
       "0          0.518761       0.518761                     0              0.938219\n",
       "1          0.619618       0.619618                     0              0.910342\n",
       "2          0.527567       0.527567                     0              0.932145\n",
       "3          0.278475       0.278475                     4              0.934417\n",
       "4          0.458134       0.458134                     0              0.907760\n",
       "...             ...            ...                   ...                   ...\n",
       "7995       0.485465       0.485465                     2              0.966300\n",
       "7996       0.587081       0.587081                     0              0.914410\n",
       "7997       0.376554       0.376554                     3              0.926388\n",
       "7998       0.659042       0.659042                     0              0.960786\n",
       "7999       0.463585       0.463585                     0              0.905830\n",
       "\n",
       "[8000 rows x 4 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_features_list = list(dataset_features.columns)\n",
    "dataset_features_arr = np.array(dataset_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_labels = dataset_embeddings['scores']\n",
    "dataset_labels_arr = np.array(dataset_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Train and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = dataset_features_arr[:7000]\n",
    "train_labels = dataset_labels_arr[:7000]\n",
    "\n",
    "val_features = dataset_features_arr[7000:]\n",
    "val_labels = dataset_labels_arr[7000:]\n",
    "\n",
    "# train_features = np.concatenate((dataset_features_arr[:6000], dataset_features_arr[7000:]))\n",
    "# train_labels = np.concatenate((dataset_labels_arr[:6000], dataset_labels_arr[7000:]))\n",
    "\n",
    "# val_features = dataset_features_arr[6000:7000]\n",
    "# val_labels = dataset_labels_arr[6000:7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Shape: (7000, 4)\n",
      "Training Labels Shape: (7000,)\n",
      "Testing Features Shape: (1000, 4)\n",
      "Testing Labels Shape: (1000,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Features Shape:', train_features.shape)\n",
    "print('Training Labels Shape:', train_labels.shape)\n",
    "print('Testing Features Shape:', val_features.shape)\n",
    "print('Testing Labels Shape:', val_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8590193766171741\n",
      "Pearson 0.10737972547221139\n",
      "Mean Absolute Error: 0.5213\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 666, max_depth = 2)\n",
    "rf.fit(train_features, train_labels);\n",
    "predictions = rf.predict(val_features)\n",
    "pearson = pearsonr(val_labels, predictions)\n",
    "errors = abs(predictions - val_labels)\n",
    "print('RMSE:', rmse(predictions,val_labels))\n",
    "print(f\"Pearson {pearson[0]}\")\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: sentence_correlation Importance: 0.62\n",
      "Variable: non_translated_words Importance: 0.17\n",
      "Variable: weighted_corr        Importance: 0.11\n",
      "Variable: weighted_corr        Importance: 0.1\n"
     ]
    }
   ],
   "source": [
    "importances = list(rf.feature_importances_)\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(dataset_features_list, importances)]\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear\n",
      "RMSE: 0.8781714603894802\n",
      "Pearson 0.09125672746481546\n",
      "Mean Absolute Error: 0.4906\n",
      "\n",
      "poly\n",
      "RMSE: 0.8759406511439161\n",
      "Pearson 0.12691700084650853\n",
      "Mean Absolute Error: 0.4891\n",
      "\n",
      "rbf\n",
      "RMSE: 0.8767300579884044\n",
      "Pearson 0.10712159566033594\n",
      "Mean Absolute Error: 0.4896\n",
      "\n",
      "sigmoid\n",
      "RMSE: 14.054784759725278\n",
      "Pearson -0.059415335134824276\n",
      "Mean Absolute Error: 10.4566\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in ['linear','poly','rbf','sigmoid']:\n",
    "    clf_t = SVR(kernel=k)\n",
    "    clf_t.fit(train_features, train_labels)\n",
    "    print(k)\n",
    "    predictions = clf_t.predict(val_features)\n",
    "    pearson = pearsonr(val_labels, predictions)\n",
    "    errors = abs(predictions - val_labels)\n",
    "    print(f'RMSE: {rmse(predictions,val_labels)}')\n",
    "    print(f'Pearson {pearson[0]}')\n",
    "    print('Mean Absolute Error:', round(np.mean(errors), 4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
