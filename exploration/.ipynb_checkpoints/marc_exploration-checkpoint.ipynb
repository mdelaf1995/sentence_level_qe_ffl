{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate a translation and compare it to the translation to give a score\n",
    "- Number of words\n",
    "- Embedded space distance between vectors\n",
    "- Use punctuation to delimiter some subsample of the phrase and try to evaluate the proximity between these\n",
    "- Fraction of simple words\n",
    "- Evaluer la complexité synthaxique de la phrase en anglais -> phrase simple, traduction devrait être de bonne qualité\n",
    "- Mots rares -> chercher si le mot a été traduit ou non"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import operator\n",
    "import pandas as pd\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# some helper functions\n",
    "def prepare_data(filename):\n",
    "    data = [l.strip().split() + ['</s>'] for l in open(filename) if l.strip()]\n",
    "    corpus = flatten(data)\n",
    "    vocab = set(corpus)\n",
    "    return vocab, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(filename,lower=False):\n",
    "    if lower:\n",
    "        data = [l.lower().strip() for l in open(filename) if l.strip()]\n",
    "    else:\n",
    "        data = [l.strip() for l in open(filename) if l.strip()]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en('oppressively')[0].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_en = pd.DataFrame(extract_sentences('../data/en_de/train.ende.src'),columns = ['sentences_en'])\n",
    "sentences_ge = pd.DataFrame(extract_sentences('../data/en_de/train.ende.mt'),columns = ['sentences_ge'])\n",
    "scores = pd.read_csv('../data/en_de/train.ende.scores',header=None)\n",
    "scores = scores.rename(columns={0:\"scores\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(sentences_en,sentences_ge,left_index=True,right_index=True)\n",
    "dataset = pd.merge(dataset,scores,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_10 = dataset.sort_values('scores').reset_index(drop=True).iloc[0:10]\n",
    "top_10 = dataset.sort_values('scores').reset_index(drop=True).iloc[-10:]\n",
    "middle = dataset.sort_values('scores').reset_index(drop=True).iloc[4000:4010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset.sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.style.set_properties(subset=['sentences_en'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10.style.set_properties(subset=['sentences_en'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_10.style.set_properties(subset=['sentences_en'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle.style.set_properties(subset=['sentences_en'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[20][\"sentences_ge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ge = nlp_german(\"Er regierte unterdrückerisch und fast bankrott Mali mit seinen verschwenderischen Ausgaben.\")\n",
    "for token in doc_ge:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop,token.vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prop noun in english we are quite sure -> doesn't matter if not translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = spacy.load(\"en_core_web_md\")\n",
    "nlp_ge = spacy.load(\"de_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en(\"obama\").vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_ge = \"Danach unterwarf er Tughlaqpur Fort und die Stadt Salwan, bevor Loni Fort belagern und schließlich marschieren auf Delhi.\"\n",
    "phrase_en = \"Afterward, he subdued Tughlaqpur's fort and the town of Salwan before besieging Loni's fort and ultimately marching on Delhi.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_non_translated_words(en_phrase,ge_phrase,nlp_en=nlp_en,nlp_ge=nlp_ge):\n",
    "    doc_en = nlp_en(en_phrase)\n",
    "    prop_nouns = []\n",
    "\n",
    "    for token in doc_en:\n",
    "        if token.pos_==\"PROPN\" or token.pos_==\"NUM\":\n",
    "            prop_nouns += [token.text.lower()]\n",
    "    print(prop_nouns)\n",
    "        \n",
    "    phrase_ge_without_np = \" \".join([token.text for token in nlp_german(ge_phrase) if token.text.lower() not in prop_nouns])\n",
    "    phrase_ge_without_np = phrase_ge_without_np.translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "    phrase_ge_without_np = re.sub(' +', ' ', phrase_ge_without_np)\n",
    "    \n",
    "    count = 0\n",
    "    for token in nlp_ge(phrase_ge_without_np):\n",
    "        if token.vector.sum() == 0:\n",
    "            print(token.text)\n",
    "            count+=1\n",
    "        \n",
    "    return count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return count_non_translated_words(x[\"sentences_en\"],x[\"sentences_ge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ge(\"Kreuzfire\").lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in nlp_ge(ex_ge):\n",
    "    print(token.text,token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in nlp_en(ex_en):\n",
    "    print(token.text,token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in nlp_en(ex_en):\n",
    "    print(token.text,token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in nlp_ge(ex_ge):\n",
    "    print(token.text,token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 20\n",
    "ex_en = dataset[\"sentences_en\"].iloc[val]\n",
    "ex_ge = dataset[\"sentences_ge\"].iloc[val]\n",
    "rating = dataset[\"scores\"].iloc[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ex_en+\"\\n\"+ex_ge+\"\\n\"+str(rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_non_translated_words(ex_en,ex_ge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ge(\"Hauptantagonisten\").vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en(\"crossfire\").vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_ge(\"bestätigung\").vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in nlp(phrase_en):\n",
    "    print(token.text,token.pos_,token.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_en = \"confirmation of president obama's first nominee, andrew traver, stalled in 2011 after the nra expressed strong opposition.\"\n",
    "phrase_ge = \"die bestätigung des ersten kandidaten von präsident obama, andrew traver, kam 2011 ins stocken, nachdem die nra starke opposition zum ausdruck gebracht hatte.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_en_2 = \"Ben Schwartz talks about Season 3 of House of Lies with Red Crab, Haardvark, and Paul rating Ben's impersonations.\"\n",
    "phrase_ge_2 = \"die bestätigung des ersten kandidaten von präsident obama, andrew traver, kam 2011 ins stocken, nachdem die nra starke opposition zum ausdruck gebracht hatte.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(\"doldrums\").vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_german(\"fghjk\").vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(phrase_en_2)\n",
    "decomposition = []\n",
    "for token in doc:\n",
    "    decomposition += [[token.text,token.pos_]]\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)\n",
    "decomposition = np.array(decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"die bestätigung des ersten kandidaten von präsident obama, andrew traver, kam 2011 ins stocken, nachdem die nra starke opposition zum ausdruck gebracht hatte.\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ge.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ge = nlp_german(\"die bestätigung des ersten kandidaten von präsident obama, andrew traver, kam 2011 ins stocken, nachdem die nra starke opposition zum ausdruck gebracht hatte.\")\n",
    "decomposition_ge = []\n",
    "for token in doc_ge:\n",
    "    decomposition_ge += [[token.text,token.pos_]]\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop,token.vector.shape)\n",
    "decomposition_ge = np.array(decomposition_ge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_en = set(stopwords.words('english'))\n",
    "stopwords_ge = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = [elt for elt in decomposition if elt[0] not in stopwords_en]\n",
    "decomposition_ge = [elt for elt in decomposition_ge if elt[0] not in stopwords_ge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition_ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English-Deutsch dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = pd.read_csv('../data/de-en.txt',sep=' ',header=None)\n",
    "dic = dic.rename(columns={0:'ge',1:'en'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_first_elt(array):\n",
    "    return [x[1:] if (isinstance(x,str) and x[0]==\"#\") else x for x in array ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic['ge'] = dic['ge'].apply(lambda x:x[1:] if (isinstance(x,str) and x[0]==\"#\") else x)\n",
    "dic['en'] = dic['en'].apply(lambda x:x[1:] if (isinstance(x,str) and x[0]==\"#\") else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x):\n",
    "    return list(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = dic.groupby('en').agg({'ge':add}).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correspondance_dict = res.set_index(\"en\").to_dict()[\"ge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_translated_words(phrase_en,phrase_ge):\n",
    "    count=0\n",
    "    sep_german_words = phrase_ge.split(\" \")\n",
    "    sep_english_words = phrase_en.split(\" \")\n",
    "    n_german_words = len(sep_english_words)\n",
    "    for word in sep_english_words:\n",
    "        for word_ge in sep_german_words:\n",
    "            if word in correspondance_dict.keys() and word_ge in correspondance_dict[word]:\n",
    "                count+=1\n",
    "                break\n",
    "    \n",
    "    return (count,n_german_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_translated_words(phrase_en,phrase_ge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tr(x):\n",
    "    return count_translated_words(x[\"sentences_en\"],x[\"sentences_ge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"corresp\"] = dataset.apply(tr,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"scores\"].corr(dataset[\"corresp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
