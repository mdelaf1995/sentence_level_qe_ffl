{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import en_core_web_md\n",
    "import de_core_news_md\n",
    "nlp_en = en_core_web_md.load()\n",
    "nlp_ge = de_core_news_md.load()\n",
    "\n",
    "def spacy_analysis(sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_alpha, token.is_stop,token.vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"../data/dataset_train_val_final.pickle\")\n",
    "with open(\"../data/vocab_en.pkl\", 'rb') as f:\n",
    "    vocab_2_embedding_idx_en = pickle.load(f)\n",
    "with open(\"../data/vocab_ge.pkl\", 'rb') as f:\n",
    "    vocab_2_embedding_idx_ge = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "\n",
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'data','muse')\n",
    "\n",
    "src_path = path+\"/wiki.multi.en.vec\"\n",
    "tgt_path = path+\"/wiki.multi.de.vec\"\n",
    "nmax = 300000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation(src_emb,tgt_emb):\n",
    "    corr = (src_emb / np.linalg.norm(src_emb)).dot(tgt_emb / np.linalg.norm(tgt_emb))\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(word,language):\n",
    "    if language==\"en\":\n",
    "        return src_embeddings[vocab_2_embedding_idx_en[word]]\n",
    "    else:\n",
    "        return tgt_embeddings[vocab_2_embedding_idx_ge[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_matrix(words_en_list,words_ge_list):\n",
    "    n = len(words_en_list)\n",
    "    m = len(words_ge_list)\n",
    "    corr_matrix = np.zeros((n,m))\n",
    "    for i,word_en in enumerate(words_en_list):\n",
    "        for j,word_ge in enumerate(words_ge_list):\n",
    "            corr_matrix[i,j] = get_correlation(get_emb(word_en,\"en\"),get_emb(word_ge,\"ge\"))\n",
    "            \n",
    "    return corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_matches(corr_matrix):\n",
    "    if len(corr_matrix)==0:\n",
    "        return {}\n",
    "    best_match_row = np.argmax(corr_matrix,axis=0)\n",
    "    best_match_col = np.argmax(corr_matrix,axis=1)\n",
    "    couples = {}\n",
    "    tmp_corr_matrix = corr_matrix.copy()\n",
    "    n = corr_matrix.shape[0] \n",
    "    m = corr_matrix.shape[1]\n",
    "    dim = min(n,m)\n",
    "    while len(couples.keys())<dim:\n",
    "        for i in range(n):\n",
    "            if (i == best_match_row[best_match_col[i]]) and (i not in couples.keys()):\n",
    "                couples[i] = best_match_col[i]\n",
    "                tmp_corr_matrix[i,:] = np.zeros(m)\n",
    "                tmp_corr_matrix[:,best_match_col[i]] = np.zeros(n)\n",
    "                best_match_row = np.argmax(tmp_corr_matrix,axis=0)\n",
    "                best_match_col = np.argmax(tmp_corr_matrix,axis=1)\n",
    "    return couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.array([[1,2,3],\n",
    "                [4,5,6],\n",
    "                [7,8,9],\n",
    "                [10,11,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_couples(words_en,words_ge):\n",
    "    if len(words_en)==0 or len(words_ge)==0:\n",
    "        return np.nan,np.nan,np.nan\n",
    "    \n",
    "    words_en_list = words_en.split()\n",
    "    words_ge_list = words_ge.split()\n",
    "    corr_mat = get_corr_matrix(words_en_list,words_ge_list)\n",
    "    word_couples_idx = get_word_matches(corr_mat)\n",
    "    score = 0\n",
    "    \n",
    "    for i in word_couples_idx.keys():\n",
    "        score+=corr_mat[i,word_couples_idx[i]]\n",
    "    score/=len(word_couples_idx)\n",
    "    \n",
    "    if len(words_en_list)>len(words_ge_list):\n",
    "        kept_words_idx = np.array(list(word_couples_idx.keys()))\n",
    "        left_words_idx = np.setdiff1d(np.arange(len(words_en_list)),kept_words_idx)\n",
    "        left_words = [words_en_list[i] for i in left_words_idx]\n",
    "        \n",
    "    elif len(words_en_list)<len(words_ge_list):\n",
    "        kept_words_idx = np.array(list(word_couples_idx.values()))\n",
    "        left_words_idx = np.setdiff1d(np.arange(len(words_en_list)),kept_words_idx)\n",
    "        left_words = [words_ge_list[i] for i in left_words_idx]\n",
    "        \n",
    "    else:\n",
    "        left_words = []\n",
    "        \n",
    "    word_couples = {}\n",
    "    for key,val in word_couples_idx.items():\n",
    "        word_couples[words_en_list[key]] = words_ge_list[val]\n",
    "        \n",
    "    return word_couples,score,left_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"correlation\"] = dataset.apply(lambda row:get_word_couples(row[\"sentences_en_final\"],row[\"sentences_ge_final\"])[1],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"correlation\"] = dataset[\"correlation\"].fillna(dataset[\"correlation\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[[\"correlation\",\"scores\"]].corr()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
