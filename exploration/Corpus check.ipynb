{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "import en_core_web_md\n",
    "import de_core_news_md\n",
    "nlp_en = en_core_web_md.load()\n",
    "nlp_ge = de_core_news_md.load()\n",
    "\n",
    "def spacy_analysis(sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_alpha, token.is_stop,token.vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(filename,lower=False):\n",
    "    if lower:\n",
    "        data = [l.lower().strip() for l in open(filename) if l.strip()]\n",
    "    else:\n",
    "        data = [l.strip() for l in open(filename) if l.strip()]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.src')\n",
    "path_train_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.mt')\n",
    "path_train_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_en = pd.DataFrame(extract_sentences(path_train_en),columns = ['sentences_en'])\n",
    "train_sentences_ge = pd.DataFrame(extract_sentences(path_train_ge),columns = ['sentences_ge'])\n",
    "train_scores = pd.read_csv(path_train_scores,header=None)\n",
    "train_scores = train_scores.rename(columns={0:\"scores\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.merge(train_sentences_en,train_sentences_ge,left_index=True,right_index=True)\n",
    "train_dataset = pd.merge(train_dataset,train_scores,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dev_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.src')\n",
    "path_dev_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.mt')\n",
    "path_dev_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences_en = pd.DataFrame(extract_sentences(path_dev_en),columns = ['sentences_en'])\n",
    "dev_sentences_ge = pd.DataFrame(extract_sentences(path_dev_ge),columns = ['sentences_ge'])\n",
    "dev_scores = pd.read_csv(path_dev_scores,header=None)\n",
    "dev_scores = dev_scores.rename(columns={0:\"scores\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = pd.merge(dev_sentences_en,dev_sentences_ge,left_index=True,right_index=True)\n",
    "dev_dataset = pd.merge(dev_dataset,dev_scores,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the two "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([train_dataset, dev_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pickle('../data/dataset_train_val.pickle')\n",
    "dataset = pd.read_pickle(\"../data/dataset_train_val.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the full vocabluary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_analysis(sentence,nlp):\n",
    "    print(sentence+\"\\n\"+\"\\n\"+\"Analysis\"+\"\\n\"+\"--------\")\n",
    "    doc = nlp(sentence)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(sentence, nlp):\n",
    "    sentence=nlp(sentence)\n",
    "    clean_sentence = \"\"\n",
    "    for token in sentence:\n",
    "        if (token.is_alpha):\n",
    "            clean_sentence+= (token.text + \" \")\n",
    "    clean_sentence=clean_sentence.strip()\n",
    "    return clean_sentence\n",
    "\n",
    "def remove_names(sentence,persons_list):\n",
    "    sentence_without_persons = sentence\n",
    "    for person in persons_list:\n",
    "        sentence_without_persons = sentence_without_persons.replace(person,\"\")\n",
    "    return sentence_without_persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_persons(sentence_en,sentence_ge):\n",
    "    doc = nlp_en(sentence_en)\n",
    "    persons_list = []\n",
    "    for ent in doc.ents:\n",
    "        if (ent.label_==\"PERSON\" or ent.label_==\"ORG\" or \n",
    "            ent.label_==\"LOC\" or ent.label_==\"GPE\" \n",
    "            or ent.label == \"FAC\" or ent.label == \"NORP\"):\n",
    "            if ent.text in sentence_ge:\n",
    "                persons_list += [ent.text]\n",
    "            else:\n",
    "                ent_text_clean = ent.text.replace(\"the\",\"\").replace(\"The\",\"\").strip()\n",
    "                if ent_text_clean in sentence_ge:\n",
    "                    persons_list += [ent_text_clean]\n",
    "    return persons_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['person']=dataset.apply(lambda row: get_persons(row[\"sentences_en\"],\n",
    "                                                         row[\"sentences_ge\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentences_en_no_propnouns'] = dataset.apply(lambda row: remove_names(row['sentences_en'], row[\"person\"]), axis=1)\n",
    "dataset['sentences_ge_propnouns'] = dataset.apply(lambda row: remove_names(row['sentences_ge'], row[\"person\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentences_en_clean'] = dataset.apply(lambda row: remove_punct(row['sentences_en_clean'], nlp_en), axis=1)\n",
    "dataset['sentences_ge_clean'] = dataset.apply(lambda row: remove_punct(row['sentences_ge_clean'], nlp_ge), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pickle('../data/dataset_train_val.pickle')\n",
    "dataset = pd.read_pickle(\"../data/dataset_train_val.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary with embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "\n",
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "src_path = '/Users/theophile/Documents/Masters_ML/NLP/coursework/MUSE/data/vectors/wiki.multi.en.vec'\n",
    "tgt_path = '/Users/theophile/Documents/Masters_ML/NLP/coursework/MUSE/data/vectors/wiki.multi.de.vec'\n",
    "nmax = 300000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer #stem for english\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_sentence = nlp_en(\"Hi encompasse John destine dismaste misperceive chocolate loving recurving hope joheg jealousy betroth casemated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "vocabulary = []\n",
    "out_of_vocab = []\n",
    "\n",
    "for token in mini_sentence:\n",
    "    if token.text not in vocabulary:\n",
    "        try:\n",
    "            embeddings.append(src_embeddings[src_word2id[token.text]])\n",
    "            vocabulary.append(token.text)\n",
    "        except:\n",
    "            try:\n",
    "                embeddings.append(src_embeddings[src_word2id[token.lemma_]])\n",
    "                vocabulary.append(token.text)\n",
    "            except:\n",
    "                n = False\n",
    "                if n == True:\n",
    "                    for i in range(3):\n",
    "                        synonym = (wordnet.synsets(token.text)[0].lemmas()[i].name())\n",
    "                        try:\n",
    "                            embeddings.append(src_embeddings[src_word2id[synonym]])\n",
    "                            vocabulary.append(token.text)\n",
    "                            n = True\n",
    "                        except:\n",
    "                            continue\n",
    "                else:\n",
    "                    try:\n",
    "                        embeddings.append(src_embeddings[src_word2id[porter.stem(token.text)]])\n",
    "                        vocabulary.append(token.text)\n",
    "                    except:\n",
    "                        out_of_vocab.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " 'encompasse',\n",
       " 'John',\n",
       " 'destine',\n",
       " 'chocolate',\n",
       " 'loving',\n",
       " 'hope',\n",
       " 'jealousy']"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dismaste', 'misperceive', 'recurving', 'joheg', 'betroth', 'casemated']"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_of_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'David' in dataset.iloc[3839]['person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[261][['sentences_en', 'sentences_en_no_punct','person']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[10]['sentences_en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[7000]['sentences_en_no_punct']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
