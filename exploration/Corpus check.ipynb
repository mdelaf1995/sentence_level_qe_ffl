{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import en_core_web_md\n",
    "import de_core_news_md\n",
    "nlp_en = en_core_web_md.load()\n",
    "nlp_ge = de_core_news_md.load()\n",
    "\n",
    "def spacy_analysis(sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_alpha, token.is_stop,token.vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(filename,lower=False):\n",
    "    if lower:\n",
    "        data = [l.lower().strip() for l in open(filename) if l.strip()]\n",
    "    else:\n",
    "        data = [l.strip() for l in open(filename) if l.strip()]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.src')\n",
    "path_train_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.mt')\n",
    "path_train_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_en = pd.DataFrame(extract_sentences(path_train_en),columns = ['sentences_en'])\n",
    "train_sentences_ge = pd.DataFrame(extract_sentences(path_train_ge),columns = ['sentences_ge'])\n",
    "train_scores = pd.read_csv(path_train_scores,header=None)\n",
    "train_scores = train_scores.rename(columns={0:\"scores\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.merge(train_sentences_en,train_sentences_ge,left_index=True,right_index=True)\n",
    "train_dataset = pd.merge(train_dataset,train_scores,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dev_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.src')\n",
    "path_dev_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.mt')\n",
    "path_dev_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences_en = pd.DataFrame(extract_sentences(path_dev_en),columns = ['sentences_en'])\n",
    "dev_sentences_ge = pd.DataFrame(extract_sentences(path_dev_ge),columns = ['sentences_ge'])\n",
    "dev_scores = pd.read_csv(path_dev_scores,header=None)\n",
    "dev_scores = dev_scores.rename(columns={0:\"scores\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = pd.merge(dev_sentences_en,dev_sentences_ge,left_index=True,right_index=True)\n",
    "dev_dataset = pd.merge(dev_dataset,dev_scores,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the two "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([train_dataset, dev_dataset])\n",
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the full vocabluary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_analysis(sentence,nlp):\n",
    "    print(sentence+\"\\n\"+\"\\n\"+\"Analysis\"+\"\\n\"+\"--------\")\n",
    "    doc = nlp(sentence)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(sentence, nlp):\n",
    "    sentence=nlp(sentence)\n",
    "    clean_sentence = \"\"\n",
    "    for token in sentence:\n",
    "        if (token.is_alpha):\n",
    "            clean_sentence+= (token.text.lower() + \" \")\n",
    "    clean_sentence=clean_sentence.strip()\n",
    "    return clean_sentence\n",
    "\n",
    "def remove_names(sentence,persons_list):\n",
    "    sentence_without_persons = sentence\n",
    "    for person in persons_list:\n",
    "        sentence_without_persons = sentence_without_persons.replace(person,\"\")\n",
    "    return sentence_without_persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_persons(sentence_en,sentence_ge):\n",
    "    doc = nlp_en(sentence_en)\n",
    "    persons_list = []\n",
    "    for ent in doc.ents:\n",
    "        if (ent.label_==\"PERSON\" or ent.label_==\"ORG\" or \n",
    "            ent.label_==\"LOC\" or ent.label_==\"GPE\" \n",
    "            or ent.label == \"FAC\" or ent.label == \"NORP\"):\n",
    "            if ent.text in sentence_ge:\n",
    "                persons_list += [ent.text]\n",
    "            else:\n",
    "                ent_text_clean = ent.text.replace(\"the\",\"\").replace(\"The\",\"\").strip()\n",
    "                if ent_text_clean in sentence_ge:\n",
    "                    persons_list += [ent_text_clean]\n",
    "    return persons_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['person']=dataset.apply(lambda row: get_persons(row[\"sentences_en\"],\n",
    "#                                                         row[\"sentences_ge\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['sentences_en_no_propnouns'] = dataset.apply(lambda row: remove_names(row['sentences_en'], row[\"person\"]), axis=1)\n",
    "#dataset['sentences_ge_no_propnouns'] = dataset.apply(lambda row: remove_names(row['sentences_ge'], row[\"person\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['sentences_en_clean'] = dataset.apply(lambda row: remove_punct(row['sentences_en_no_propnouns'], nlp_en), axis=1)\n",
    "#dataset['sentences_ge_clean'] = dataset.apply(lambda row: remove_punct(row['sentences_ge_no_propnouns'], nlp_ge), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.to_pickle('../data/dataset_train_val.pickle')\n",
    "dataset = pd.read_pickle(\"../data/dataset_train_val.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary with embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "\n",
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'data','muse')\n",
    "\n",
    "src_path = path+\"/wiki.multi.en.vec\"\n",
    "tgt_path = path+\"/wiki.multi.de.vec\"\n",
    "nmax = 300000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer #stem for english\n",
    "from nltk.stem.cistem import Cistem\n",
    "porter = PorterStemmer()\n",
    "cistem = Cistem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_words_to_vocab(vocab,sentence,nlp,word_2id,stemmer,language=\"en\"):\n",
    "    mini_sentence = nlp(sentence)\n",
    "    out_of_vocab = []\n",
    "    for token in mini_sentence:\n",
    "        if token.text not in vocab.keys():\n",
    "            try:\n",
    "                vocab[token.text] = word_2id[token.text]\n",
    "            except:\n",
    "                try:\n",
    "                    vocab[token.text] = word_2id[token.lemma_]\n",
    "                except:\n",
    "                    try:\n",
    "                        synonyms = wordnet.synsets(token.text)[0].lemmas()\n",
    "                        for i in range(10):\n",
    "                            synonym = synonyms[i].name()\n",
    "                            try:\n",
    "                                vocab[token.text] = word_2id[synonym]\n",
    "                                break\n",
    "                            except:\n",
    "                                continue\n",
    "                    except:\n",
    "                        try:\n",
    "                            vocab[token.text] = word_2id[stemmer.stem(token.text)]\n",
    "                        except:\n",
    "                            out_of_vocab.append(token.text)\n",
    "    return vocab,out_of_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_2_embedding_idx_en = {}\n",
    "out_of_vocab_en = []\n",
    "\n",
    "sentences_en = dataset.sentences_en_clean\n",
    "\n",
    "for i,sentence in enumerate(sentences_en):\n",
    "    vocab_2_embedding_idx_en,out_of_vocab_current = add_words_to_vocab(vocab_2_embedding_idx_en,sentence,\n",
    "                                                                       nlp_en,src_word2id,porter)\n",
    "    \n",
    "    for out_of_vocab_word in out_of_vocab_current:\n",
    "        out_of_vocab_en += [(out_of_vocab_word,i)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_2_embedding_idx_ge = {}\n",
    "out_of_vocab_ge = []\n",
    "\n",
    "sentences_ge = dataset.sentences_ge_clean\n",
    "\n",
    "for i,sentence in enumerate(sentences_ge):\n",
    "    vocab_2_embedding_idx_ge,out_of_vocab_current = add_words_to_vocab(vocab_2_embedding_idx_ge,sentence,\n",
    "                                                                nlp_ge,tgt_word2id,cistem)\n",
    "    for out_of_vocab_word in out_of_vocab_current:\n",
    "        out_of_vocab_ge += [(out_of_vocab_word,i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non german words from german vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load list of true german words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_german_vocab_corpus = list(vocab_2_embedding_idx_ge.keys())\n",
    "#large list of german words downloaded here : https://gist.github.com/MarvinJWendt/2f4f4154b8ae218600eb091a5706b5f4#file-wordlist-german-txt\n",
    "german_words = list(pd.read_csv(\"../data/wordlist-german.txt\",header=None)[0])\n",
    "german_words = [str(word).lower() for word in german_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Helpers methods ####\n",
    "\n",
    "def remove_unnecessary_spaces(sentence):\n",
    "    return re.sub(' +',' ',sentence.strip())\n",
    "\n",
    "def remove_word_from_sentence(sentence,word):\n",
    "    new_sentence = sentence.replace(word,\"\")\n",
    "    return remove_unnecessary_spaces(new_sentence)\n",
    "\n",
    "def remove_words_from_list(list_1,list_2 = german_words,isin=False):\n",
    "    if isin:\n",
    "        return [word for word in list_1 if word in list_2]\n",
    "    else:\n",
    "        return [word for word in list_1 if word not in list_2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding words in original vocabulary which are not german (using multiprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "pool = Pool(4)\n",
    "n = len(current_german_vocab_corpus)\n",
    "split = int(n/4)\n",
    "\n",
    "words_to_be_removed_list = pool.map(remove_words_from_list,[current_german_vocab_corpus[0:split],\n",
    "                                                 current_german_vocab_corpus[split:split*2],\n",
    "                                                 current_german_vocab_corpus[split*2:split*3],\n",
    "                                                 current_german_vocab_corpus[split*3:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_be_removed = words_to_be_removed_list[0] + words_to_be_removed_list[1] + \\\n",
    "                      words_to_be_removed_list[2] + words_to_be_removed_list[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add these words to the out_of_vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out_of_vocab_ge = []\n",
    "for word in words_to_be_removed:\n",
    "    idxs =  list(dataset[dataset.sentences_ge_clean.apply(lambda x: word in x)].index)\n",
    "    if len(idxs)<=3:\n",
    "        for idx in idxs:\n",
    "            new_out_of_vocab_ge += [(word,idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of old out of vocab words: 4418\n",
      "Number of new out of vocab words: 2019\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of old out of vocab words: {}\".format(len(out_of_vocab_ge)))\n",
    "print(\"Number of new out of vocab words: {}\".format(len(new_out_of_vocab_ge)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_vocab_ge += new_out_of_vocab_ge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing these words from the vocab dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of old out vocab words: 18457\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of old out vocab words: {}\".format(len(vocab_2_embedding_idx_ge)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in [pair[0] for pair in new_out_of_vocab_ge]:\n",
    "    try:\n",
    "        vocab_2_embedding_idx_ge.pop(word,None)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of new out vocab words: 16847\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of new out vocab words: {}\".format(len(vocab_2_embedding_idx_ge)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save vocab dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#with open(\"../data/vocab_en.pkl\",\"wb\") as f:\n",
    "#    pickle.dump(vocab_2_embedding_idx_en, f, pickle.HIGHEST_PROTOCOL)\n",
    "#with open(\"../data/vocab_ge.pkl\",\"wb\") as f:\n",
    "#    pickle.dump(vocab_2_embedding_idx_ge, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load vocab dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/vocab_en.pkl\", 'rb') as f:\n",
    "    vocab_2_embedding_idx_en = pickle.load(f)\n",
    "with open(\"../data/vocab_ge.pkl\", 'rb') as f:\n",
    "    vocab_2_embedding_idx_ge = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find non translated english words and split german words which can be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"../data/dataset_train_val.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"non_translated_words\"] = 0\n",
    "dataset[\"sentences_en_cleaner\"] = dataset[\"sentences_en_clean\"].copy()\n",
    "dataset[\"sentences_ge_cleaner\"] = dataset[\"sentences_ge_clean\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_translated_out_of_vocab_words(pair,word2id,dataset,out_of_vocab_english):\n",
    "    word,idx = pair[0],pair[1]\n",
    "    en_sentence = dataset.loc[idx][\"sentences_en_cleaner\"]\n",
    "    ge_sentence = dataset.loc[idx][\"sentences_ge_cleaner\"]\n",
    "    count_non_translated = dataset.loc[idx][\"non_translated_words\"]\n",
    "    \n",
    "    ### If the word is exactly the same as a word in the source sentence\n",
    "    if word in en_sentence:\n",
    "        if word not in out_of_vocab_english:\n",
    "            ### if the word is in the english vocab -> it could have been translated\n",
    "            ### count it as a non translated word and remove it from both sentences\n",
    "            dataset.at[idx,\"sentences_en_cleaner\"] = remove_word_from_sentence(en_sentence,word)\n",
    "            dataset.at[idx,\"sentences_ge_cleaner\"] = remove_word_from_sentence(ge_sentence,word)\n",
    "            dataset.at[idx,\"non_translated_words\"] = count_non_translated+1\n",
    "            return dataset\n",
    "        else:\n",
    "            ### if the word is not in the english vocab -> it could not have been translated\n",
    "            ### don't count it as a non translated word and remove it from both sentences\n",
    "            dataset.at[idx,\"sentences_en_cleaner\"] = remove_word_from_sentence(en_sentence,word)\n",
    "            dataset.at[idx,\"sentences_ge_cleaner\"] = remove_word_from_sentence(ge_sentence,word)\n",
    "            return dataset\n",
    "        \n",
    "    ### Test for subwords\n",
    "    subword = word[:-1]\n",
    "    while len(subword)>0:\n",
    "        if subword in word2id.keys():\n",
    "            subword1 = subword\n",
    "            subword2 = word[len(subword1):]\n",
    "            if subword2 in word2id.keys():\n",
    "                if len(subword2)>2:\n",
    "                    ### if the word can be splitted in 2 meaningful words do it\n",
    "                    dataset.at[idx,\"sentences_ge_cleaner\"] = ge_sentence.replace(word,subword1+\" \"+subword2)\n",
    "                    return dataset\n",
    "                else:\n",
    "                    ### if the word can be found by using a shorter version which can be found in vocab\n",
    "                    dataset.at[idx,\"sentences_ge_cleaner\"] = ge_sentence.replace(word,subword1)\n",
    "                    return dataset\n",
    "            else:\n",
    "                return dataset\n",
    "            \n",
    "        else:\n",
    "            subword = subword[:-1]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in out_of_vocab_ge:\n",
    "    dataset = process_translated_out_of_vocab_words(pair,vocab_2_embedding_idx_ge,dataset,out_of_vocab_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.to_pickle('../data/dataset_train_val_plus.pickle')\n",
    "dataset = pd.read_pickle(\"../data/dataset_train_val_plus.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "stopwords_ge = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multiple_words_from_sentence(sentence,words,isin=False):\n",
    "    if isin:\n",
    "        splited = [word for word in sentence.split() if word in words]\n",
    "    else:\n",
    "        splited = [word for word in sentence.split() if word not in words]\n",
    "    return \" \".join(splited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"sentences_en_cleaner\"] = dataset.sentences_en_cleaner.apply(lambda x:remove_multiple_words_from_sentence(x,stopwords_en))\n",
    "dataset[\"sentences_ge_cleaner\"] = dataset.sentences_ge_cleaner.apply(lambda x:remove_multiple_words_from_sentence(x,stopwords_ge))\n",
    "dataset[\"sentences_ge_cleaner\"] = dataset.sentences_ge_cleaner.apply(lambda x:remove_multiple_words_from_sentence(x,stopwords_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_fin_en = list(vocab_2_embedding_idx_en.keys())\n",
    "vocab_fin_ge = list(vocab_2_embedding_idx_ge.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"sentences_en_final\"] = dataset.sentences_en_cleaner.apply(lambda x:remove_multiple_words_from_sentence(x,vocab_fin_en,True))\n",
    "dataset[\"sentences_ge_final\"] = dataset.sentences_ge_cleaner.apply(lambda x:remove_multiple_words_from_sentence(x,vocab_fin_ge,True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"length_ge\"] = dataset[\"sentences_ge_final\"].apply(lambda x:len(x.split()))\n",
    "dataset[\"length_en\"] = dataset[\"sentences_en_final\"].apply(lambda x:len(x.split()))\n",
    "dataset[\"distance\"] = dataset[\"length_en\"] - dataset[\"length_ge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.to_pickle('../data/dataset_train_val_final.pickle')\n",
    "dataset = pd.read_pickle(\"../data/dataset_train_val_final.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
