{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "import en_core_web_md\n",
    "import de_core_news_md\n",
    "nlp_en = en_core_web_md.load()\n",
    "nlp_ge = de_core_news_md.load()\n",
    "\n",
    "def spacy_analysis(sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_alpha, token.is_stop,token.vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(filename,lower=False):\n",
    "    if lower:\n",
    "        data = [l.lower().strip() for l in open(filename) if l.strip()]\n",
    "    else:\n",
    "        data = [l.strip() for l in open(filename) if l.strip()]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.src')\n",
    "path_train_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.mt')\n",
    "path_train_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_en = pd.DataFrame(extract_sentences(path_train_en),columns = ['sentences_en'])\n",
    "train_sentences_ge = pd.DataFrame(extract_sentences(path_train_ge),columns = ['sentences_ge'])\n",
    "train_scores = pd.read_csv(path_train_scores,header=None)\n",
    "train_scores = train_scores.rename(columns={0:\"scores\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.merge(train_sentences_en,train_sentences_ge,left_index=True,right_index=True)\n",
    "train_dataset = pd.merge(train_dataset,train_scores,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dev_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.src')\n",
    "path_dev_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.mt')\n",
    "path_dev_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences_en = pd.DataFrame(extract_sentences(path_dev_en),columns = ['sentences_en'])\n",
    "dev_sentences_ge = pd.DataFrame(extract_sentences(path_dev_ge),columns = ['sentences_ge'])\n",
    "dev_scores = pd.read_csv(path_dev_scores,header=None)\n",
    "dev_scores = dev_scores.rename(columns={0:\"scores\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = pd.merge(dev_sentences_en,dev_sentences_ge,left_index=True,right_index=True)\n",
    "dev_dataset = pd.merge(dev_dataset,dev_scores,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the two "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([train_dataset, dev_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the full vocabluary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_analysis(sentence,nlp):\n",
    "    print(sentence+\"\\n\"+\"\\n\"+\"Analysis\"+\"\\n\"+\"--------\")\n",
    "    doc = nlp(sentence)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(sentence, nlp):\n",
    "    sentence=nlp(sentence)\n",
    "    clean_sentence = \"\"\n",
    "    for token in sentence:\n",
    "        if (token.is_alpha):\n",
    "            clean_sentence+= (token.text + \" \")\n",
    "    clean_sentence=clean_sentence.strip()\n",
    "    return clean_sentence\n",
    "\n",
    "def remove_names(sentence,persons_list):\n",
    "    sentence_without_persons = sentence\n",
    "    for person in persons_list:\n",
    "        sentence_without_persons = sentence_without_persons.replace(person,\"\")\n",
    "    return sentence_without_persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentences_en_no_punct'] = dataset.apply(lambda row: remove_punct(row['sentences_en'], nlp_en), axis=1)\n",
    "dataset['sentences_ge_no_punct'] = dataset.apply(lambda row: remove_punct(row['sentences_ge'], nlp_ge), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_persons(sentence_en,sentence_ge):\n",
    "    doc = nlp_en(sentence_en)\n",
    "    persons_list = []\n",
    "    for ent in doc.ents:\n",
    "        if (ent.label_==\"PERSON\" or ent.label_==\"ORG\") and (ent.text in sentence_ge):\n",
    "            persons_list += [ent.text]\n",
    "    return persons_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['person']=dataset.apply(lambda row: get_persons(row[\"sentences_en_no_punct\"],\n",
    "                                                         row[\"sentences_ge_no_punct\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentences_en_clean'] = dataset.apply(lambda row: remove_names(row['sentences_en_no_punct'], row[\"person\"]), axis=1)\n",
    "dataset['sentences_ge_clean'] = dataset.apply(lambda row: remove_names(row['sentences_ge_no_punct'], row[\"person\"]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(dataset):\n",
    "    vocabulary=[]\n",
    "    for sentence in dataset:\n",
    "        for token in sentence:\n",
    "            vocabulary.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'David' in dataset.iloc[3839]['person']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[261][['sentences_en', 'sentences_en_no_punct','person']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loc[10]['sentences_en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.iloc[7000]['sentences_en_no_punct']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
