{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Generate a translation and compare it to the translation to give a score\n",
    "- Number of words\n",
    "- Embedded space distance between vectors\n",
    "- Use punctuation to delimiter some subsample of the phrase and try to evaluate the proximity between these\n",
    "- Fraction of simple words\n",
    "- Evaluer la complexité synthaxique de la phrase en anglais -> phrase simple, traduction devrait être de bonne qualité\n",
    "- Mots rares -> chercher si le mot a été traduit ou non"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import operator\n",
    "import pandas as pd\n",
    "\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# some helper functions\n",
    "def prepare_data(filename):\n",
    "    data = [l.strip().split() + ['</s>'] for l in open(filename) if l.strip()]\n",
    "    corpus = flatten(data)\n",
    "    vocab = set(corpus)\n",
    "    return vocab, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(filename,lower=False):\n",
    "    if lower:\n",
    "        data = [l.lower().strip() for l in open(filename) if l.strip()]\n",
    "    else:\n",
    "        data = [l.strip() for l in open(filename) if l.strip()]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_en = pd.DataFrame(extract_sentences('../data/en_de/train.ende.src'),columns = ['sentences_en'])\n",
    "sentences_ge = pd.DataFrame(extract_sentences('../data/en_de/train.ende.mt'),columns = ['sentences_ge'])\n",
    "scores = pd.read_csv('../data/en_de/train.ende.scores',header=None)\n",
    "scores = scores.rename(columns={0:\"scores\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(sentences_en,sentences_ge,left_index=True,right_index=True)\n",
    "dataset = pd.merge(dataset,scores,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_10 = dataset.sort_values('scores').reset_index(drop=True).iloc[0:10]\n",
    "top_10 = dataset.sort_values('scores').reset_index(drop=True).iloc[-10:]\n",
    "middle = dataset.sort_values('scores').reset_index(drop=True).iloc[4000:4010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset.sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.style.set_properties(subset=['sentences_en'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10.style.set_properties(subset=['sentences_en'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_10.style.set_properties(subset=['sentences_en'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle.style.set_properties(subset=['sentences_en'], **{'width': '300px'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing pipeline\n",
    "\n",
    "- index of sentences\n",
    "- full english sentence (without preprocessing)\n",
    "- full german sentence  (without preprocessing)\n",
    "- english sentnece no stop words, punctuation\n",
    "- german sentnece no stop words, punctuation\n",
    "- score\n",
    "- verbs in english (separated by a space and lemmatized)\n",
    "- verbs in german (separated by a space and lemmatized)\n",
    "- adjectives in english (separated by a space and lemmatized)\n",
    "- adjectives in german (separated by a space and lemmatized)\n",
    "- common nouns in english (separated by a space and lemmatized)\n",
    "- common nouns in german (separated by a space and lemmatized)\n",
    "- Nouns of persons\n",
    "- Entities or organizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp_en = spacy.load(\"en_core_web_md\")\n",
    "nlp_ge = spacy.load(\"de_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_analysis(sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_alpha, token.is_stop,token.vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos(pos,sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    res = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_==pos:\n",
    "            res += token.text + \" \"\n",
    "    res = res.strip()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "\n",
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "src_path = '/Users/marcdelaferriere/Documents/Imperial/NLP/MUSE/data/vectors/wiki.multi.en.vec'\n",
    "tgt_path = '/Users/marcdelaferriere/Documents/Imperial/NLP/MUSE/data/vectors/wiki.multi.de.vec'\n",
    "nmax = 300000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle(\"../data/dataset_v1.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(sentence, nlp):\n",
    "    sentence = nlp(sentence)\n",
    "    lemmatized_sentence = \"\"\n",
    "    for token in sentence:\n",
    "        lemmatized_sentence += token.lemma_ + \" \"\n",
    "    \n",
    "    lemmatized_sentence = lemmatized_sentence.strip()\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_index(sentence,word2id):\n",
    "    words = sentence.split()\n",
    "    res = \"\"\n",
    "    not_trslted_words = \"\"\n",
    "    not_trslted_count = 0\n",
    "    for word in words:\n",
    "        if word in word2id.keys():\n",
    "            res += str(word2id[word])+\" \"\n",
    "        else:\n",
    "            res += \"None \"\n",
    "            not_trslted_count +=1\n",
    "            not_trslted_words += word + \",\"\n",
    "            \n",
    "    res += \"not_trslted:\"+not_trslted_words+\" \"\n",
    "    res += \"not_found={} \".format(not_trslted_count)\n",
    "    return res.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"lemmatized_ge_verbs\"] = dataset.german_verbs.apply(lambda x:lemmatizer(x,nlp_ge).lower())\n",
    "dataset[\"lemmatized_en_verbs\"] = dataset.english_verbs.apply(lambda x:lemmatizer(x,nlp_en).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"idx_verbs_english\"] = dataset.lemmatized_en_verbs.apply(lambda x:find_word_index(x,src_word2id))\n",
    "dataset[\"idx_verbs_german\"] = dataset.lemmatized_ge_verbs.apply(lambda x:find_word_index(x,tgt_word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"idx_verbs_english_no_lemma\"] = dataset.english_verbs.apply(lambda x:find_word_index(x.lower(),src_word2id))\n",
    "dataset[\"idx_verbs_german_no_lemma\"] = dataset.german_verbs.apply(lambda x:find_word_index(x.lower(),tgt_word2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"verbs_not_found_en_count\"] = dataset.idx_verbs_english.apply(lambda x: int(x.split()[-1][-1]))\n",
    "dataset[\"verbs_not_found_ge_count\"] = dataset.idx_verbs_german.apply(lambda x: int(x.split()[-1][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"verbs_not_found_en_words\"] = dataset.idx_verbs_english.apply(lambda x: x.split()[-2][12:])\n",
    "dataset[\"verbs_not_found_ge_words\"] = dataset.idx_verbs_german.apply(lambda x: x.split()[-2][12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"verbs_not_found_en_count_no_lemma\"] = dataset.idx_verbs_english_no_lemma.apply(lambda x: int(x.split()[-1][-1]))\n",
    "dataset[\"verbs_not_found_ge_count_no_lemma\"] = dataset.idx_verbs_german_no_lemma.apply(lambda x: int(x.split()[-1][-1]))\n",
    "dataset[\"verbs_not_found_en_words_no_lemma\"] = dataset.idx_verbs_english_no_lemma.apply(lambda x: x.split()[-2][12:])\n",
    "dataset[\"verbs_not_found_ge_words_no_lemma\"] = dataset.idx_verbs_german_no_lemma.apply(lambda x: x.split()[-2][12:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation(src_emb,tgt_emb):\n",
    "    corr = (src_emb / np.linalg.norm(src_emb)).dot(tgt_emb / np.linalg.norm(tgt_emb))\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"english_verbs\"] = dataset[\"english_verbs\"].apply(lambda x:x.lower())\n",
    "dataset[\"count_english_verbs\"] = dataset[\"english_verbs\"].apply(lambda x:len(x.split()))\n",
    "dataset[\"count_german_verbs\"] = dataset[\"german_verbs\"].apply(lambda x:len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_dataset = dataset[dataset[\"count_english_verbs\"] == dataset[\"count_german_verbs\"]]\n",
    "sub_dataset = sub_dataset[sub_dataset[\"verbs_not_found_en_count\"]==sub_dataset[\"verbs_not_found_ge_count\"]]\n",
    "sub_dataset = sub_dataset[sub_dataset[\"verbs_not_found_en_count\"]==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## irrelevant\n",
    "def get_dict_corr(verbs_en,verbs_ge):\n",
    "    list_verbs_en = verbs_en.split()\n",
    "    list_verbs_ge = verbs_ge.split()\n",
    "    matches_en_ge = {}\n",
    "    correlations = {}\n",
    "    for verb_en in list_verbs_en:\n",
    "        best_match = 0\n",
    "        verb_ge_match = None\n",
    "        for verb_ge in list_verbs_ge:\n",
    "            corr = get_correlation(src_embeddings[src_word2id[verb_en]],tgt_embeddings[tgt_word2id[verb_ge]])\n",
    "            if corr>best_match:\n",
    "                best_match = corr\n",
    "                verb_ge_match = verb_ge\n",
    "                \n",
    "        matches_en_ge[verb_en] = verb_ge\n",
    "        #matches_ge_en[verb_ge] = verb_en\n",
    "        correlations[verb_ge] = best_match\n",
    "        \n",
    "    return str(matches_en_ge)+str(correlations)\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_corr(verbs_en,verbs_ge):\n",
    "    list_verbs_en = verbs_en.split()\n",
    "    list_verbs_ge = verbs_ge.split()\n",
    "    n = len(list_verbs_en)\n",
    "    \n",
    "    verb_en_2idx = {}\n",
    "    verb_ge_2idx = {}\n",
    "    idx_2verb_en = {}\n",
    "    idx_2verb_ge = {}\n",
    "    corr_matrix = np.zeros((n,n))\n",
    "    \n",
    "    for i,verb_en in enumerate(list_verbs_en):\n",
    "        verb_en_2idx[verb_en] = i\n",
    "        idx_2verb_en[i] = verb_en\n",
    "\n",
    "    for i,verb_ge in enumerate(list_verbs_ge):\n",
    "        verb_ge_2idx[verb_ge] = i\n",
    "        idx_2verb_ge[i] = verb_ge\n",
    "\n",
    "    for verb_en in list_verbs_en:\n",
    "        for verb_ge in list_verbs_ge:\n",
    "            corr = get_correlation(src_embeddings[src_word2id[verb_en]],tgt_embeddings[tgt_word2id[verb_ge]])\n",
    "            corr_matrix[verb_en_2idx[verb_en],verb_ge_2idx[verb_ge]] = corr\n",
    "            \n",
    "    return corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_matrix(words_en_list,words_ge_list):\n",
    "    n = len(words_en_list)\n",
    "    corr_matrix = np.zeros((n,n))\n",
    "    for i,word_en in enumerate(words_en_list):\n",
    "        for j,word_ge in enumerate(words_ge_list):\n",
    "            corr_matrix[i,j] = get_correlation(get_emb(word_en,\"en\"),get_emb(word_ge,\"ge\"))\n",
    "            \n",
    "    return corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_matches(corr_matrix):\n",
    "    if len(corr_matrix)==0:\n",
    "        return {}\n",
    "    best_match_row = np.argmax(corr_matrix,axis=0)\n",
    "    best_match_col = np.argmax(corr_matrix,axis=1)\n",
    "    couples = {}\n",
    "    tmp_corr_matrix = corr_matrix.copy()\n",
    "    n = corr_matrix.shape[0] \n",
    "    while len(couples.keys())<n:\n",
    "        for i in range(n):\n",
    "            if (i == best_match_row[best_match_col[i]]) and (i not in couples.keys()):\n",
    "                couples[i] = best_match_col[i]\n",
    "                tmp_corr_matrix[i,:] = np.zeros(n)\n",
    "                tmp_corr_matrix[:,best_match_col[i]] = np.zeros(n)\n",
    "                best_match_row = np.argmax(tmp_corr_matrix,axis=0)\n",
    "                best_match_col = np.argmax(tmp_corr_matrix,axis=1)\n",
    "    return couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(word,language):\n",
    "    if language==\"en\":\n",
    "        return src_embeddings[src_word2id[word]]\n",
    "    else:\n",
    "        return tgt_embeddings[tgt_word2id[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_corr(words_en,words_ge):\n",
    "    words_en_list = words_en.split()\n",
    "    words_ge_list = words_ge.split()\n",
    "    \n",
    "    corr_mat = get_corr_matrix(words_en_list,words_ge_list)\n",
    "    word_couples = get_word_matches(corr_mat)\n",
    "    sum_corr = 0\n",
    "    n = len(word_couples.keys())\n",
    "    for key,val in word_couples.items():\n",
    "        sum_corr += corr_mat[key,val]\n",
    "        \n",
    "    if n==0:\n",
    "        return 0\n",
    "    mean_corr = sum_corr/n\n",
    "    return mean_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_couples(words_en,words_ge):\n",
    "    words_en_list = words_en.split()\n",
    "    words_ge_list = words_ge.split()\n",
    "    corr_mat = get_corr_matrix(words_en_list,words_ge_list)\n",
    "    word_couples_idx = get_word_matches(corr_mat)\n",
    "    word_couples = {}\n",
    "    for key,val in word_couples_idx.items():\n",
    "        word_couples[words_en_list[key]] = words_ge_list[val]\n",
    "\n",
    "    return word_couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = sub_dataset.sample()\n",
    "en = sample.iloc[0][\"lemmatized_en_verbs\"]\n",
    "ge = sample.iloc[0][\"lemmatized_ge_verbs\"]\n",
    "print(en)\n",
    "print(ge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mean_corr(en,ge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prop Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_analysis(sentence,nlp):\n",
    "    print(sentence+\"\\n\"+\"\\n\"+\"Analysis\"+\"\\n\"+\"--------\")\n",
    "    doc = nlp(sentence)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample =dataset.sample()\n",
    "sentence = sample.iloc[0][\"sentences_en\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2269\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_analysis(sentence,nlp_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities(sentence,nlp_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_list(x):\n",
    "    return [\"fdf\",\"fdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    persons_list = []\n",
    "    for ent in doc.ents:\n",
    "        if (ent.label_==\"PERSON\"):\n",
    "            persons_list += [ent.text]\n",
    "    return persons_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset[\"sentences_en\"]==sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"FAC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
