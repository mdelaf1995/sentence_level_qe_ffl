{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing pipeline\n",
    "\n",
    "- index of sentences\n",
    "- full english sentence (without preprocessing)\n",
    "- full german sentence  (without preprocessing)\n",
    "- english sentnece no stop words, punctuation\n",
    "- german sentnece no stop words, punctuation\n",
    "- score\n",
    "- verbs in english (separated by a space and lemmatized)\n",
    "- verbs in german (separated by a space and lemmatized)\n",
    "- adjectives in english (separated by a space and lemmatized)\n",
    "- adjectives in german (separated by a space and lemmatized)\n",
    "- common nouns in english (separated by a space and lemmatized)\n",
    "- common nouns in german (separated by a space and lemmatized)\n",
    "- Nouns of persons\n",
    "- Entities or organizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_en = spacy.load(\"en_core_web_md\")\n",
    "# nlp_ge = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "import en_core_web_md\n",
    "import de_core_news_md\n",
    "nlp_en = en_core_web_md.load()\n",
    "nlp_ge = de_core_news_md.load()\n",
    "\n",
    "def spacy_analysis(sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_alpha, token.is_stop,token.vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# some helper functions\n",
    "def prepare_data(filename):\n",
    "    data = [l.strip().split() + ['</s>'] for l in open(filename) if l.strip()]\n",
    "    corpus = flatten(data)\n",
    "    vocab = set(corpus)\n",
    "    return vocab, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(filename,lower=False):\n",
    "    if lower:\n",
    "        data = [l.lower().strip() for l in open(filename) if l.strip()]\n",
    "    else:\n",
    "        data = [l.strip() for l in open(filename) if l.strip()]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_en = pd.DataFrame(extract_sentences('../data/en_de/train.ende.src'),columns = ['sentences_en'])\n",
    "# sentences_ge = pd.DataFrame(extract_sentences('../data/en_de/train.ende.mt'),columns = ['sentences_ge'])\n",
    "# scores = pd.read_csv('../data/en_de/train.ende.scores',header=None)\n",
    "# scores = scores.rename(columns={0:\"scores\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.merge(sentences_en,sentences_ge,left_index=True,right_index=True)\n",
    "# dataset = pd.merge(dataset,scores,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos(pos,sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    res = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_==pos:\n",
    "            res += token.text + \" \"\n",
    "    res = res.strip()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[\"german_verbs\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"VERB\",x,nlp_ge))\n",
    "#dataset[\"english_verbs\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"VERB\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[\"german_adjectives\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"ADJ\",x,nlp_ge))\n",
    "#dataset[\"english_adjectives\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"ADJ\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[\"german_adverbs\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"ADV\",x,nlp_ge))\n",
    "#dataset[\"english_adverbs\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"ADV\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset[\"german_nouns\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"NOUN\",x,nlp_ge))\n",
    "#dataset[\"english_nouns\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"NOUN\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS as sw_en\n",
    "from spacy.lang.de.stop_words import STOP_WORDS as sw_ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sw(sentence, nlp):\n",
    "    sentence = nlp(sentence)\n",
    "    no_sw_sentence = \"\"\n",
    "    \n",
    "    if nlp==nlp_en:\n",
    "        stop_words=sw_en\n",
    "    elif nlp==nlp_ge:\n",
    "        stop_words=sw_ge\n",
    "    else:\n",
    "        return('Use valid language: en or ge')\n",
    "    for token in sentence:\n",
    "        if token.text not in stop_words:\n",
    "            no_sw_sentence += token.text + \" \"\n",
    "    \n",
    "    no_sw_sentence=no_sw_sentence.strip()\n",
    "    return no_sw_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['english_no_stop_words'] = dataset['sentences_en'].apply(lambda x: remove_sw(x, nlp_en))\n",
    "# dataset['german_no_stop_words'] = dataset['sentences_ge'].apply(lambda x: remove_sw(x, nlp_ge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(sentence, nlp):\n",
    "    sentence = nlp(sentence)\n",
    "    lemmatized_sentence = \"\"\n",
    "    for token in sentence:\n",
    "        lemmatized_sentence += token.lemma_ + \" \"\n",
    "    \n",
    "    lemmatized_sentence = lemmatized_sentence.strip()\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['english_lemma'] = dataset['sentences_en'].apply (lambda x: lemmatizer(x, nlp_en))\n",
    "# dataset['german_lemma'] = dataset['sentences_ge'].apply (lambda x: lemmatizer(x, nlp_ge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob as textblob_en\n",
    "from textblob_de import TextBlobDE as textblob_ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(sentence, textblob):\n",
    "    text = textblob(sentence)\n",
    "    score = text.sentiment.polarity\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['english_sentence_sentiment'] = dataset['sentences_en'].apply(lambda x: sentiment(x, textblob_en))\n",
    "# dataset['german_sentence_sentiment'] = dataset['sentences_ge'].apply(lambda x: sentiment(x, textblob_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdz(data):\n",
    "    data_stdz = (data - data.mean())/data.std()\n",
    "    return data_stdz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['std_english_sentence_sentiment']= stdz(dataset['english_sentence_sentiment'])\n",
    "# dataset['std_german_sentence_sentiment']= stdz(dataset['german_sentence_sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store/load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.to_pickle('../data/dataset_v1.pickle')\n",
    "dataset = pd.read_pickle(\"../data/dataset_v1.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences_en</th>\n",
       "      <th>sentences_ge</th>\n",
       "      <th>scores</th>\n",
       "      <th>german_verbs</th>\n",
       "      <th>english_verbs</th>\n",
       "      <th>german_adjectives</th>\n",
       "      <th>english_adjectives</th>\n",
       "      <th>german_adverbs</th>\n",
       "      <th>english_adverbs</th>\n",
       "      <th>german_nouns</th>\n",
       "      <th>english_nouns</th>\n",
       "      <th>english_no_stop_words</th>\n",
       "      <th>german_no_stop_words</th>\n",
       "      <th>english_lemma</th>\n",
       "      <th>german_lemma</th>\n",
       "      <th>english_sentence_sentiment</th>\n",
       "      <th>german_sentence_sentiment</th>\n",
       "      <th>std_english_sentence_sentiment</th>\n",
       "      <th>std_german_sentence_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>José Ortega y Gasset visited Husserl at Freibu...</td>\n",
       "      <td>1934 besuchte José Ortega y Gasset Husserl in ...</td>\n",
       "      <td>1.101697</td>\n",
       "      <td>besuchte</td>\n",
       "      <td>visited</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>José Ortega y Gasset visited Husserl Freiburg ...</td>\n",
       "      <td>1934 besuchte José Ortega y Gasset Husserl Fre...</td>\n",
       "      <td>José Ortega y Gasset visit Husserl at Freiburg...</td>\n",
       "      <td>1934 besuchen José Ortega y Gasset Husserl in ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.176727</td>\n",
       "      <td>-0.004161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>However, a disappointing ninth in China meant ...</td>\n",
       "      <td>Eine enttäuschende Neunte in China bedeutete j...</td>\n",
       "      <td>-0.516656</td>\n",
       "      <td>bedeutete zurückfiel</td>\n",
       "      <td>meant dropped</td>\n",
       "      <td>enttäuschende sechsten</td>\n",
       "      <td>disappointing sixth</td>\n",
       "      <td>jedoch</td>\n",
       "      <td>However back</td>\n",
       "      <td>Neunte Gesamtwertung Platz</td>\n",
       "      <td>ninth standings</td>\n",
       "      <td>However , disappointing ninth China meant drop...</td>\n",
       "      <td>Eine enttäuschende Neunte China bedeutete , Ge...</td>\n",
       "      <td>however , a disappointing ninth in China mean ...</td>\n",
       "      <td>Eine enttäuschend Neunte in China bedeuten jed...</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.754750</td>\n",
       "      <td>-2.444859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In his diary, Chase wrote that the release of ...</td>\n",
       "      <td>In seinem Tagebuch, Chase schrieb, dass die Ve...</td>\n",
       "      <td>-2.226388</td>\n",
       "      <td>schrieb</td>\n",
       "      <td>wrote</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Tagebuch Chase Veröffentlichung Galle Wermut</td>\n",
       "      <td>diary release gall wormwood</td>\n",
       "      <td>In diary , Chase wrote release Mason Slidell \"...</td>\n",
       "      <td>In Tagebuch , Chase schrieb , Veröffentlichung...</td>\n",
       "      <td>in -PRON- diary , Chase write that the release...</td>\n",
       "      <td>In mein Tagebuch , Chase schreiben , dass der ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.176727</td>\n",
       "      <td>-0.004161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Heavy arquebuses mounted on wagons were called...</td>\n",
       "      <td>Schwere Arquebuses auf Waggons montiert wurden...</td>\n",
       "      <td>-0.827379</td>\n",
       "      <td>montiert genannt</td>\n",
       "      <td>mounted called</td>\n",
       "      <td>Schwere</td>\n",
       "      <td>Heavy</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Arquebuses Waggons</td>\n",
       "      <td>arquebuses wagons</td>\n",
       "      <td>Heavy arquebuses mounted wagons called arquebu...</td>\n",
       "      <td>Schwere Arquebuses Waggons montiert Arquebus à...</td>\n",
       "      <td>heavy arquebus mount on wagon be call arquebus...</td>\n",
       "      <td>Schwere Arquebuses auf Waggon montieren werden...</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.228742</td>\n",
       "      <td>-0.004161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once North Pacific salmon die off after spawni...</td>\n",
       "      <td>Sobald der nordpazifische Lachs nach dem Laich...</td>\n",
       "      <td>0.364695</td>\n",
       "      <td>abstirbt fressen</td>\n",
       "      <td>die spawning eat</td>\n",
       "      <td>nordpazifische lokale</td>\n",
       "      <td>local bald</td>\n",
       "      <td>fast ausschließlich</td>\n",
       "      <td>usually almost exclusively</td>\n",
       "      <td>Lachs Laichen Regel Glatzadler Lachskörper</td>\n",
       "      <td>salmon eagles salmon carcasses</td>\n",
       "      <td>Once North Pacific salmon die spawning , usual...</td>\n",
       "      <td>Sobald nordpazifische Lachs Laichen abstirbt ,...</td>\n",
       "      <td>once North Pacific salmon die off after spawn ...</td>\n",
       "      <td>sobald der nordpazifische Lachs nach der Laich...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.176727</td>\n",
       "      <td>-0.004161</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sentences_en  \\\n",
       "0  José Ortega y Gasset visited Husserl at Freibu...   \n",
       "1  However, a disappointing ninth in China meant ...   \n",
       "2  In his diary, Chase wrote that the release of ...   \n",
       "3  Heavy arquebuses mounted on wagons were called...   \n",
       "4  Once North Pacific salmon die off after spawni...   \n",
       "\n",
       "                                        sentences_ge    scores  \\\n",
       "0  1934 besuchte José Ortega y Gasset Husserl in ...  1.101697   \n",
       "1  Eine enttäuschende Neunte in China bedeutete j... -0.516656   \n",
       "2  In seinem Tagebuch, Chase schrieb, dass die Ve... -2.226388   \n",
       "3  Schwere Arquebuses auf Waggons montiert wurden... -0.827379   \n",
       "4  Sobald der nordpazifische Lachs nach dem Laich...  0.364695   \n",
       "\n",
       "           german_verbs     english_verbs       german_adjectives  \\\n",
       "0              besuchte           visited                           \n",
       "1  bedeutete zurückfiel     meant dropped  enttäuschende sechsten   \n",
       "2               schrieb             wrote                           \n",
       "3      montiert genannt    mounted called                 Schwere   \n",
       "4      abstirbt fressen  die spawning eat   nordpazifische lokale   \n",
       "\n",
       "    english_adjectives       german_adverbs             english_adverbs  \\\n",
       "0                                                                         \n",
       "1  disappointing sixth               jedoch                However back   \n",
       "2                                                                         \n",
       "3                Heavy                                                    \n",
       "4           local bald  fast ausschließlich  usually almost exclusively   \n",
       "\n",
       "                                   german_nouns  \\\n",
       "0                                                 \n",
       "1                    Neunte Gesamtwertung Platz   \n",
       "2  Tagebuch Chase Veröffentlichung Galle Wermut   \n",
       "3                            Arquebuses Waggons   \n",
       "4    Lachs Laichen Regel Glatzadler Lachskörper   \n",
       "\n",
       "                    english_nouns  \\\n",
       "0                                   \n",
       "1                 ninth standings   \n",
       "2     diary release gall wormwood   \n",
       "3               arquebuses wagons   \n",
       "4  salmon eagles salmon carcasses   \n",
       "\n",
       "                               english_no_stop_words  \\\n",
       "0  José Ortega y Gasset visited Husserl Freiburg ...   \n",
       "1  However , disappointing ninth China meant drop...   \n",
       "2  In diary , Chase wrote release Mason Slidell \"...   \n",
       "3  Heavy arquebuses mounted wagons called arquebu...   \n",
       "4  Once North Pacific salmon die spawning , usual...   \n",
       "\n",
       "                                german_no_stop_words  \\\n",
       "0  1934 besuchte José Ortega y Gasset Husserl Fre...   \n",
       "1  Eine enttäuschende Neunte China bedeutete , Ge...   \n",
       "2  In Tagebuch , Chase schrieb , Veröffentlichung...   \n",
       "3  Schwere Arquebuses Waggons montiert Arquebus à...   \n",
       "4  Sobald nordpazifische Lachs Laichen abstirbt ,...   \n",
       "\n",
       "                                       english_lemma  \\\n",
       "0  José Ortega y Gasset visit Husserl at Freiburg...   \n",
       "1  however , a disappointing ninth in China mean ...   \n",
       "2  in -PRON- diary , Chase write that the release...   \n",
       "3  heavy arquebus mount on wagon be call arquebus...   \n",
       "4  once North Pacific salmon die off after spawn ...   \n",
       "\n",
       "                                        german_lemma  \\\n",
       "0  1934 besuchen José Ortega y Gasset Husserl in ...   \n",
       "1  Eine enttäuschend Neunte in China bedeuten jed...   \n",
       "2  In mein Tagebuch , Chase schreiben , dass der ...   \n",
       "3  Schwere Arquebuses auf Waggon montieren werden...   \n",
       "4  sobald der nordpazifische Lachs nach der Laich...   \n",
       "\n",
       "   english_sentence_sentiment  german_sentence_sentiment  \\\n",
       "0                         0.0                        0.0   \n",
       "1                        -0.3                       -1.0   \n",
       "2                         0.0                        0.0   \n",
       "3                        -0.2                        0.0   \n",
       "4                         0.0                        0.0   \n",
       "\n",
       "   std_english_sentence_sentiment  std_german_sentence_sentiment  \n",
       "0                       -0.176727                      -0.004161  \n",
       "1                       -1.754750                      -2.444859  \n",
       "2                       -0.176727                      -0.004161  \n",
       "3                       -1.228742                      -0.004161  \n",
       "4                       -0.176727                      -0.004161  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
