{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing pipeline\n",
    "\n",
    "- index of sentences\n",
    "- full english sentence (without preprocessing)\n",
    "- full german sentence  (without preprocessing)\n",
    "- english sentnece no stop words, punctuation\n",
    "- german sentnece no stop words, punctuation\n",
    "- score\n",
    "- verbs in english (separated by a space and lemmatized)\n",
    "- verbs in german (separated by a space and lemmatized)\n",
    "- adjectives in english (separated by a space and lemmatized)\n",
    "- adjectives in german (separated by a space and lemmatized)\n",
    "- common nouns in english (separated by a space and lemmatized)\n",
    "- common nouns in german (separated by a space and lemmatized)\n",
    "- Nouns of persons\n",
    "- Entities or organizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_en = spacy.load(\"en_core_web_md\")\n",
    "# nlp_ge = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "import en_core_web_md\n",
    "import de_core_news_md\n",
    "nlp_en = en_core_web_md.load()\n",
    "nlp_ge = de_core_news_md.load()\n",
    "\n",
    "def spacy_analysis(sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_alpha, token.is_stop,token.vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# some helper functions\n",
    "def prepare_data(filename):\n",
    "    data = [l.strip().split() + ['</s>'] for l in open(filename) if l.strip()]\n",
    "    corpus = flatten(data)\n",
    "    vocab = set(corpus)\n",
    "    return vocab, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(filename,lower=False):\n",
    "    if lower:\n",
    "        data = [l.lower().strip() for l in open(filename) if l.strip()]\n",
    "    else:\n",
    "        data = [l.strip() for l in open(filename) if l.strip()]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_en = pd.DataFrame(extract_sentences('../data/en_de/train.ende.src'),columns = ['sentences_en'])\n",
    "# sentences_ge = pd.DataFrame(extract_sentences('../data/en_de/train.ende.mt'),columns = ['sentences_ge'])\n",
    "# scores = pd.read_csv('../data/en_de/train.ende.scores',header=None)\n",
    "# scores = scores.rename(columns={0:\"scores\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.merge(sentences_en,sentences_ge,left_index=True,right_index=True)\n",
    "# dataset = pd.merge(dataset,scores,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets ie also validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.src')\n",
    "path_train_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.mt')\n",
    "path_train_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.scores')\n",
    "\n",
    "train_sentences_en = pd.DataFrame(extract_sentences(path_train_en),columns = ['sentences_en'])\n",
    "train_sentences_ge = pd.DataFrame(extract_sentences(path_train_ge),columns = ['sentences_ge'])\n",
    "train_scores = pd.read_csv(path_train_scores,header=None)\n",
    "train_scores = train_scores.rename(columns={0:\"scores\"})\n",
    "\n",
    "train_dataset = pd.merge(train_sentences_en,train_sentences_ge,left_index=True,right_index=True)\n",
    "train_dataset = pd.merge(train_dataset,train_scores,left_index=True,right_index=True)\n",
    "\n",
    "path_dev_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.src')\n",
    "path_dev_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.mt')\n",
    "path_dev_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.scores')\n",
    "\n",
    "dev_sentences_en = pd.DataFrame(extract_sentences(path_dev_en),columns = ['sentences_en'])\n",
    "dev_sentences_ge = pd.DataFrame(extract_sentences(path_dev_ge),columns = ['sentences_ge'])\n",
    "dev_scores = pd.read_csv(path_dev_scores,header=None)\n",
    "dev_scores = dev_scores.rename(columns={0:\"scores\"})\n",
    "\n",
    "dev_dataset = pd.merge(dev_sentences_en,dev_sentences_ge,left_index=True,right_index=True)\n",
    "dev_dataset = pd.merge(dev_dataset,dev_scores,left_index=True,right_index=True)\n",
    "\n",
    "dataset = pd.concat([train_dataset, dev_dataset])\n",
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_length(sentence):\n",
    "    leng=0\n",
    "    sentence=sentence.split(\" \")\n",
    "    for token in sentence:\n",
    "        leng+=1\n",
    "    return leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_length(a, b):\n",
    "    lenga=0\n",
    "    a=a.split(\" \")\n",
    "    for token in a:\n",
    "        lenga+=1\n",
    "    \n",
    "    lengb=0\n",
    "    b=b.split(\" \")\n",
    "    for token in b:\n",
    "        lengb+=1\n",
    "    \n",
    "    return abs(lenga-lengb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['english_sentence_length'] = dataset['sentences_en'].apply(sentence_length)\n",
    "dataset['german_sentence_length'] = dataset['sentences_ge'].apply(sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentence_length_difference'] = dataset['english_sentence_length'] - dataset['german_sentence_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos(pos,sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    res = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_==pos:\n",
    "            res += token.text + \" \"\n",
    "    res = res.strip()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"german_verbs\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"VERB\",x,nlp_ge))\n",
    "# dataset[\"english_verbs\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"VERB\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['verbs_diff'] = dataset.apply(lambda row: difference_length(row['english_verbs'], row['german_verbs']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"german_adjectives\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"ADJ\",x,nlp_ge))\n",
    "# dataset[\"english_adjectives\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"ADJ\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['adjectives_diff'] = dataset.apply(lambda row: difference_length(row['english_adjectives'], row['german_adjectives']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"german_adverbs\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"ADV\",x,nlp_ge))\n",
    "# dataset[\"english_adverbs\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"ADV\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['adverbs_diff'] = dataset.apply(lambda row: difference_length(row['english_adverbs'], row['german_adverbs']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"german_nouns\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"NOUN\",x,nlp_ge))\n",
    "# dataset[\"english_nouns\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"NOUN\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['nouns_diff'] = dataset.apply(lambda row: difference_length(row['english_nouns'], row['german_nouns']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(sentence, nlp):\n",
    "    sentence=nlp(sentence)\n",
    "    no_punct_sentence = \"\"\n",
    "    for token in sentence:\n",
    "        if not token.is_punct:\n",
    "            no_punct_sentence+= (token.text + \" \")\n",
    "    \n",
    "    no_punct_sentence=no_punct_sentence.strip()\n",
    "    return no_punct_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['english_no_punctuation'] = dataset['sentences_en'].apply(lambda x: remove_punct(x, nlp_en))\n",
    "# dataset['german_no_punctuation'] = dataset['sentences_ge'].apply(lambda x: remove_punct(x, nlp_ge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS as sw_en\n",
    "from spacy.lang.de.stop_words import STOP_WORDS as sw_ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sw(sentence, nlp):\n",
    "    sentence = nlp(sentence)\n",
    "    no_sw_sentence = \"\"\n",
    "    \n",
    "    if nlp==nlp_en:\n",
    "        stop_words=sw_en\n",
    "    elif nlp==nlp_ge:\n",
    "        stop_words=sw_ge\n",
    "    else:\n",
    "        return('Use valid language: en or ge')\n",
    "    for token in sentence:\n",
    "        if token.text not in stop_words:\n",
    "            no_sw_sentence += token.text + \" \"\n",
    "    \n",
    "    no_sw_sentence=no_sw_sentence.strip()\n",
    "    return no_sw_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['english_no_stop_words'] = dataset['sentences_en'].apply(lambda x: remove_sw(x, nlp_en))\n",
    "# dataset['german_no_stop_words'] = dataset['sentences_ge'].apply(lambda x: remove_sw(x, nlp_ge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(sentence, nlp):\n",
    "    sentence = nlp(sentence)\n",
    "    lemmatized_sentence = \"\"\n",
    "    for token in sentence:\n",
    "        lemmatized_sentence += token.lemma_ + \" \"\n",
    "    \n",
    "    lemmatized_sentence = lemmatized_sentence.strip()\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['english_lemma'] = dataset['sentences_en'].apply (lambda x: lemmatizer(x, nlp_en))\n",
    "# dataset['german_lemma'] = dataset['sentences_ge'].apply (lambda x: lemmatizer(x, nlp_ge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob as textblob_en\n",
    "from textblob_de import TextBlobDE as textblob_ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(sentence, textblob):\n",
    "    text = textblob(sentence)\n",
    "    score = text.sentiment.polarity\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['english_sentence_sentiment'] = dataset['sentences_en'].apply(lambda x: sentiment(x, textblob_en))\n",
    "# dataset['german_sentence_sentiment'] = dataset['sentences_ge'].apply(lambda x: sentiment(x, textblob_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['english_sentence_lemma_sentiment'] = dataset['english_lemma'].apply(lambda x: sentiment(x, textblob_en))\n",
    "# dataset['german_sentence_lemma_sentiment'] = dataset['german_lemma'].apply(lambda x: sentiment(x, textblob_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdz(data):\n",
    "    data_stdz = (data - data.mean())/data.std()\n",
    "    return data_stdz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['std_english_sentence_sentiment']= stdz(dataset['english_sentence_sentiment'])\n",
    "# dataset['std_german_sentence_sentiment']= stdz(dataset['german_sentence_sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the maximal Sentiment per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_lemm_english = abs(dataset['english_sentence_lemma_sentiment'])>abs(dataset['english_sentence_sentiment'])\n",
    "# mask_lemm_german = abs(dataset['german_sentence_lemma_sentiment'])>abs(dataset['german_sentence_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theophile/anaconda3/envs/nlp/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# dataset['max_sentiment_english'] = dataset['english_sentence_sentiment']\n",
    "# dataset['max_sentiment_english'].at[mask_lemm_english] = dataset['english_sentence_lemma_sentiment'][mask_lemm_english]\n",
    "\n",
    "# dataset['max_sentiment_german']=dataset['german_sentence_sentiment']\n",
    "# dataset['max_sentiment_german'].at[mask_lemm_english] = dataset['german_sentence_lemma_sentiment'][mask_lemm_english]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['std_max_english_sentiment']= stdz(dataset['max_sentiment_english'])\n",
    "# dataset['std_max_german_sentiment']= stdz(dataset['max_sentiment_german'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store/load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset.to_pickle('../data/dataset_v1.pickle')\n",
    "# dataset = pd.read_pickle(\"../data/dataset_v1.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 34)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Work on the correlations dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_em = pd.read_pickle('../data/dataset_correlations_v1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_em['std_correlations'] = stdz(dataset_em['correlation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_em['english_em_verbs'] = dataset_em['sentences_en_final'].apply(lambda x:extract_pos(\"VERB\",x,nlp_en))\n",
    "# dataset_em[\"german_verbs\"] = dataset_em['sentences_ge_final'].apply(lambda x:extract_pos(\"VERB\",x,nlp_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_em[\"german_adjectives\"] = dataset_em.sentences_ge.apply(lambda x:extract_pos(\"ADJ\",x,nlp_ge))\n",
    "# dataset_em[\"english_adjectives\"] = dataset_em.sentences_en.apply(lambda x:extract_pos(\"ADJ\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_em[\"german_adverbs\"] = dataset_em.sentences_ge.apply(lambda x:extract_pos(\"ADV\",x,nlp_ge))\n",
    "# dataset_em[\"english_adverbs\"] = dataset_em.sentences_en.apply(lambda x:extract_pos(\"ADV\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_em[\"german_nouns\"] = dataset_em.sentences_ge.apply(lambda x:extract_pos(\"NOUN\",x,nlp_ge))\n",
    "# dataset_em[\"english_nouns\"] = dataset_em.sentences_en.apply(lambda x:extract_pos(\"NOUN\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_em.to_pickle('../data/dataset_correlations_v2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
