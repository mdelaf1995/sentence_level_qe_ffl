{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing pipeline\n",
    "\n",
    "- index of sentences\n",
    "- full english sentence (without preprocessing)\n",
    "- full german sentence  (without preprocessing)\n",
    "- english sentnece no stop words, punctuation\n",
    "- german sentnece no stop words, punctuation\n",
    "- score\n",
    "- verbs in english (separated by a space and lemmatized)\n",
    "- verbs in german (separated by a space and lemmatized)\n",
    "- adjectives in english (separated by a space and lemmatized)\n",
    "- adjectives in german (separated by a space and lemmatized)\n",
    "- common nouns in english (separated by a space and lemmatized)\n",
    "- common nouns in german (separated by a space and lemmatized)\n",
    "- Nouns of persons\n",
    "- Entities or organizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_en = spacy.load(\"en_core_web_md\")\n",
    "# nlp_ge = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "import en_core_web_md\n",
    "import de_core_news_md\n",
    "nlp_en = en_core_web_md.load()\n",
    "nlp_ge = de_core_news_md.load()\n",
    "\n",
    "def spacy_analysis(sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_alpha, token.is_stop,token.vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "\n",
    "# some helper functions\n",
    "def prepare_data(filename):\n",
    "    data = [l.strip().split() + ['</s>'] for l in open(filename) if l.strip()]\n",
    "    corpus = flatten(data)\n",
    "    vocab = set(corpus)\n",
    "    return vocab, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(filename,lower=False):\n",
    "    if lower:\n",
    "        data = [l.lower().strip() for l in open(filename) if l.strip()]\n",
    "    else:\n",
    "        data = [l.strip() for l in open(filename) if l.strip()]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_en = pd.DataFrame(extract_sentences('../data/en_de/train.ende.src'),columns = ['sentences_en'])\n",
    "# sentences_ge = pd.DataFrame(extract_sentences('../data/en_de/train.ende.mt'),columns = ['sentences_ge'])\n",
    "# scores = pd.read_csv('../data/en_de/train.ende.scores',header=None)\n",
    "# scores = scores.rename(columns={0:\"scores\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = pd.merge(sentences_en,sentences_ge,left_index=True,right_index=True)\n",
    "# dataset = pd.merge(dataset,scores,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets ie also validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.src')\n",
    "path_train_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.mt')\n",
    "path_train_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.scores')\n",
    "\n",
    "train_sentences_en = pd.DataFrame(extract_sentences(path_train_en),columns = ['sentences_en'])\n",
    "train_sentences_ge = pd.DataFrame(extract_sentences(path_train_ge),columns = ['sentences_ge'])\n",
    "train_scores = pd.read_csv(path_train_scores,header=None)\n",
    "train_scores = train_scores.rename(columns={0:\"scores\"})\n",
    "\n",
    "train_dataset = pd.merge(train_sentences_en,train_sentences_ge,left_index=True,right_index=True)\n",
    "train_dataset = pd.merge(train_dataset,train_scores,left_index=True,right_index=True)\n",
    "\n",
    "path_dev_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.src')\n",
    "path_dev_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.mt')\n",
    "path_dev_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.scores')\n",
    "\n",
    "dev_sentences_en = pd.DataFrame(extract_sentences(path_dev_en),columns = ['sentences_en'])\n",
    "dev_sentences_ge = pd.DataFrame(extract_sentences(path_dev_ge),columns = ['sentences_ge'])\n",
    "dev_scores = pd.read_csv(path_dev_scores,header=None)\n",
    "dev_scores = dev_scores.rename(columns={0:\"scores\"})\n",
    "\n",
    "dev_dataset = pd.merge(dev_sentences_en,dev_sentences_ge,left_index=True,right_index=True)\n",
    "dev_dataset = pd.merge(dev_dataset,dev_scores,left_index=True,right_index=True)\n",
    "\n",
    "dataset = pd.concat([train_dataset, dev_dataset])\n",
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_length(sentence):\n",
    "    leng=0\n",
    "    sentence=sentence.split(\" \")\n",
    "    for token in sentence:\n",
    "        leng+=1\n",
    "    return leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_length(a, b):\n",
    "    lenga=0\n",
    "    a=a.split(\" \")\n",
    "    for token in a:\n",
    "        lenga+=1\n",
    "    \n",
    "    lengb=0\n",
    "    b=b.split(\" \")\n",
    "    for token in b:\n",
    "        lengb+=1\n",
    "    \n",
    "    return abs(lenga-lengb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['english_sentence_length'] = dataset['sentences_en'].apply(sentence_length)\n",
    "dataset['german_sentence_length'] = dataset['sentences_ge'].apply(sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentence_length_difference'] = dataset['english_sentence_length'] - dataset['german_sentence_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos(pos,sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    res = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_==pos:\n",
    "            res += token.text + \" \"\n",
    "    res = res.strip()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"german_verbs\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"VERB\",x,nlp_ge))\n",
    "# dataset[\"english_verbs\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"VERB\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['verbs_diff'] = dataset.apply(lambda row: difference_length(row['english_verbs'], row['german_verbs']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"german_adjectives\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"ADJ\",x,nlp_ge))\n",
    "# dataset[\"english_adjectives\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"ADJ\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['adjectives_diff'] = dataset.apply(lambda row: difference_length(row['english_adjectives'], row['german_adjectives']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"german_adverbs\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"ADV\",x,nlp_ge))\n",
    "# dataset[\"english_adverbs\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"ADV\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['adverbs_diff'] = dataset.apply(lambda row: difference_length(row['english_adverbs'], row['german_adverbs']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[\"german_nouns\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"NOUN\",x,nlp_ge))\n",
    "# dataset[\"english_nouns\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"NOUN\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['nouns_diff'] = dataset.apply(lambda row: difference_length(row['english_nouns'], row['german_nouns']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(sentence, nlp):\n",
    "    sentence=nlp(sentence)\n",
    "    no_punct_sentence = \"\"\n",
    "    for token in sentence:\n",
    "        if not token.is_punct:\n",
    "            no_punct_sentence+= (token.text + \" \")\n",
    "    \n",
    "    no_punct_sentence=no_punct_sentence.strip()\n",
    "    return no_punct_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['english_no_punctuation'] = dataset['sentences_en'].apply(lambda x: remove_punct(x, nlp_en))\n",
    "# dataset['german_no_punctuation'] = dataset['sentences_ge'].apply(lambda x: remove_punct(x, nlp_ge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS as sw_en\n",
    "from spacy.lang.de.stop_words import STOP_WORDS as sw_ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_sw(sentence, nlp):\n",
    "    sentence = nlp(sentence)\n",
    "    no_sw_sentence = \"\"\n",
    "    \n",
    "    if nlp==nlp_en:\n",
    "        stop_words=sw_en\n",
    "    elif nlp==nlp_ge:\n",
    "        stop_words=sw_ge\n",
    "    else:\n",
    "        return('Use valid language: en or ge')\n",
    "    for token in sentence:\n",
    "        if token.text not in stop_words:\n",
    "            no_sw_sentence += token.text + \" \"\n",
    "    \n",
    "    no_sw_sentence=no_sw_sentence.strip()\n",
    "    return no_sw_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['english_no_stop_words'] = dataset['sentences_en'].apply(lambda x: remove_sw(x, nlp_en))\n",
    "# dataset['german_no_stop_words'] = dataset['sentences_ge'].apply(lambda x: remove_sw(x, nlp_ge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(sentence, nlp):\n",
    "    sentence = nlp(sentence)\n",
    "    lemmatized_sentence = \"\"\n",
    "    for token in sentence:\n",
    "        lemmatized_sentence += token.lemma_ + \" \"\n",
    "    \n",
    "    lemmatized_sentence = lemmatized_sentence.strip()\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['english_lemma'] = dataset['sentences_en'].apply (lambda x: lemmatizer(x, nlp_en))\n",
    "# dataset['german_lemma'] = dataset['sentences_ge'].apply (lambda x: lemmatizer(x, nlp_ge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob as textblob_en\n",
    "from textblob_de import TextBlobDE as textblob_ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(sentence, textblob):\n",
    "    text = textblob(sentence)\n",
    "    score = text.sentiment.polarity\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['english_sentence_sentiment'] = dataset['sentences_en'].apply(lambda x: sentiment(x, textblob_en))\n",
    "# dataset['german_sentence_sentiment'] = dataset['sentences_ge'].apply(lambda x: sentiment(x, textblob_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['english_sentence_lemma_sentiment'] = dataset['english_lemma'].apply(lambda x: sentiment(x, textblob_en))\n",
    "# dataset['german_sentence_lemma_sentiment'] = dataset['german_lemma'].apply(lambda x: sentiment(x, textblob_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdz(data):\n",
    "    data_stdz = (data - data.mean())/data.std()\n",
    "    return data_stdz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['std_english_sentence_sentiment']= stdz(dataset['english_sentence_sentiment'])\n",
    "# dataset['std_german_sentence_sentiment']= stdz(dataset['german_sentence_sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the maximal Sentiment per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_lemm_english = abs(dataset['english_sentence_lemma_sentiment'])>abs(dataset['english_sentence_sentiment'])\n",
    "# mask_lemm_german = abs(dataset['german_sentence_lemma_sentiment'])>abs(dataset['german_sentence_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/theophile/anaconda3/envs/nlp/lib/python3.7/site-packages/pandas/core/indexing.py:670: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# dataset['max_sentiment_english'] = dataset['english_sentence_sentiment']\n",
    "# dataset['max_sentiment_english'].at[mask_lemm_english] = dataset['english_sentence_lemma_sentiment'][mask_lemm_english]\n",
    "\n",
    "# dataset['max_sentiment_german']=dataset['german_sentence_sentiment']\n",
    "# dataset['max_sentiment_german'].at[mask_lemm_english] = dataset['german_sentence_lemma_sentiment'][mask_lemm_english]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset['std_max_english_sentiment']= stdz(dataset['max_sentiment_english'])\n",
    "# dataset['std_max_german_sentiment']= stdz(dataset['max_sentiment_german'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Per POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store/load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# dataset.to_pickle('../data/dataset_v1.pickle')\n",
    "dataset = pd.read_pickle(\"../data/dataset_v1.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Work on the correlations dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_em = pd.read_pickle('../data/dataset_correlations_v1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_em['std_correlations'] = stdz(dataset_em['correlation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_em['english_em_verbs'] = dataset_em['sentences_en_final'].apply(lambda x:extract_pos(\"VERB\",x,nlp_en))\n",
    "# dataset_em[\"german_verbs\"] = dataset_em['sentences_ge_final'].apply(lambda x:extract_pos(\"VERB\",x,nlp_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_em[\"german_adjectives\"] = dataset_em.sentences_ge.apply(lambda x:extract_pos(\"ADJ\",x,nlp_ge))\n",
    "# dataset_em[\"english_adjectives\"] = dataset_em.sentences_en.apply(lambda x:extract_pos(\"ADJ\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_em[\"german_adverbs\"] = dataset_em.sentences_ge.apply(lambda x:extract_pos(\"ADV\",x,nlp_ge))\n",
    "# dataset_em[\"english_adverbs\"] = dataset_em.sentences_en.apply(lambda x:extract_pos(\"ADV\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_em[\"german_nouns\"] = dataset_em.sentences_ge.apply(lambda x:extract_pos(\"NOUN\",x,nlp_ge))\n",
    "# dataset_em[\"english_nouns\"] = dataset_em.sentences_en.apply(lambda x:extract_pos(\"NOUN\",x,nlp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_em['embedded_words_matched_max'] = dataset_em[['length_ge','length_en']].max(axis=1)\n",
    "dataset_em['embedded_words_matched_min'] = dataset_em[['length_ge','length_en']].min(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_em['weights'] = dataset_em['embedded_words_matched_min']/(dataset_em['embedded_words_matched_max'] + \n",
    "                        dataset_em['non_translated_words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_em['weights']=dataset_em['weights'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_em['weighted_corr'] = dataset_em['weights']*dataset_em['correlation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06959344364193254"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_em['weighted_corr'].corr(dataset_em['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08979190053718233"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_em['correlation'].corr(dataset_em['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences_en</th>\n",
       "      <th>sentences_ge</th>\n",
       "      <th>scores</th>\n",
       "      <th>person</th>\n",
       "      <th>sentences_en_no_propnouns</th>\n",
       "      <th>sentences_ge_no_propnouns</th>\n",
       "      <th>sentences_en_clean</th>\n",
       "      <th>sentences_ge_clean</th>\n",
       "      <th>non_translated_words</th>\n",
       "      <th>sentences_en_cleaner</th>\n",
       "      <th>...</th>\n",
       "      <th>sentences_en_final</th>\n",
       "      <th>sentences_ge_final</th>\n",
       "      <th>length_ge</th>\n",
       "      <th>length_en</th>\n",
       "      <th>distance</th>\n",
       "      <th>correlation</th>\n",
       "      <th>embedded_words_matched_max</th>\n",
       "      <th>embedded_words_matched_min</th>\n",
       "      <th>weights</th>\n",
       "      <th>weighted_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3917</th>\n",
       "      <td>The Mummy, A Handbook of Egyptian Funerary Arc...</td>\n",
       "      <td>The Mummy, A Handbook of Egyptian Funerary Arc...</td>\n",
       "      <td>-8.140713</td>\n",
       "      <td>[]</td>\n",
       "      <td>The Mummy, A Handbook of Egyptian Funerary Arc...</td>\n",
       "      <td>The Mummy, A Handbook of Egyptian Funerary Arc...</td>\n",
       "      <td>the mummy a handbook of egyptian funerary arch...</td>\n",
       "      <td>the mummy a handbook of egyptian funerary arch...</td>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.615833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6975</th>\n",
       "      <td>He vacated the WWA Cruiserweight title in Apri...</td>\n",
       "      <td>Nach seiner Rückkehr zum WWF verließ er im Apr...</td>\n",
       "      <td>-7.481519</td>\n",
       "      <td>[WWF]</td>\n",
       "      <td>He vacated the WWA Cruiserweight title in Apri...</td>\n",
       "      <td>Nach seiner Rückkehr zum  verließ er im April ...</td>\n",
       "      <td>he vacated the wwa cruiserweight title in apri...</td>\n",
       "      <td>nach seiner rückkehr zum verließ er im april d...</td>\n",
       "      <td>2</td>\n",
       "      <td>vacated title april returning</td>\n",
       "      <td>...</td>\n",
       "      <td>vacated title april returning</td>\n",
       "      <td>rückkehr verließ april titel</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.587837</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.391891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>Haroun and S.A. Mourad, Proceedings of Interna...</td>\n",
       "      <td>Haroun and S.A. Mourad, Proceedings of Interna...</td>\n",
       "      <td>-7.148534</td>\n",
       "      <td>[Haroun, S.A. Mourad, Cairo, Egypt]</td>\n",
       "      <td>and , Proceedings of International Colloquium...</td>\n",
       "      <td>and , Proceedings of International Colloquium...</td>\n",
       "      <td>and proceedings of international colloquium on...</td>\n",
       "      <td>and proceedings of international colloquium on...</td>\n",
       "      <td>1</td>\n",
       "      <td>proceedings international structural engineeri...</td>\n",
       "      <td>...</td>\n",
       "      <td>proceedings international structural engineeri...</td>\n",
       "      <td>proceedings international engineering april pp</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.630766</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.450547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6229</th>\n",
       "      <td>Boone Trails Grand Towers Gravois Trail New Ho...</td>\n",
       "      <td>Boone Trails Grand Towers Gravois Trail New Ho...</td>\n",
       "      <td>-7.028717</td>\n",
       "      <td>[Boone Trails Grand Towers Gravois Trail New H...</td>\n",
       "      <td>North Star Osage  Trailblazers  Trails Thunde...</td>\n",
       "      <td>North Star Osage  Trailblazers  Trails Thunde...</td>\n",
       "      <td>north star osage trailblazers trails thunderbird</td>\n",
       "      <td>north star osage trailblazers trails thunderbird</td>\n",
       "      <td>3</td>\n",
       "      <td>north star trails bird</td>\n",
       "      <td>...</td>\n",
       "      <td>north star trails bird</td>\n",
       "      <td>north star trails bird</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.570327</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.325901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7033</th>\n",
       "      <td>World Encyclopedia of Police Forces and Penal ...</td>\n",
       "      <td>World Encyclopedia of Police Forces and Straal...</td>\n",
       "      <td>-7.028717</td>\n",
       "      <td>[]</td>\n",
       "      <td>World Encyclopedia of Police Forces and Penal ...</td>\n",
       "      <td>World Encyclopedia of Police Forces and Straal...</td>\n",
       "      <td>world encyclopedia of police forces and penal ...</td>\n",
       "      <td>world encyclopedia of police forces and straal...</td>\n",
       "      <td>1</td>\n",
       "      <td>world encyclopedia police penal systems</td>\n",
       "      <td>...</td>\n",
       "      <td>world encyclopedia police penal systems</td>\n",
       "      <td>world encyclopedia police systems</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.569677</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.379785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1846</th>\n",
       "      <td>Faerie Apocalypse, Jason Franks, IFWG Publishi...</td>\n",
       "      <td>Faerie Apocalypse, Jason Franks, IFWG Publishi...</td>\n",
       "      <td>-6.776486</td>\n",
       "      <td>[Faerie Apocalypse, Jason Franks, IFWG Publish...</td>\n",
       "      <td>, , .</td>\n",
       "      <td>, , .</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.615833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>Pollock, Griselda, Generations and Geographies...</td>\n",
       "      <td>Pollock, Griselda, Generations and Geographies...</td>\n",
       "      <td>-6.776486</td>\n",
       "      <td>[Pollock, Griselda, Generations and, the Visua...</td>\n",
       "      <td>Geographies in , , , 1996.</td>\n",
       "      <td>Geographies in , , , 1996.</td>\n",
       "      <td>geographies in</td>\n",
       "      <td>geographies in</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.615833</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>Illustrated Encyclopedia of Woodworking Handto...</td>\n",
       "      <td>Illustrated Encyclopedia of Woodworking Handto...</td>\n",
       "      <td>-6.776486</td>\n",
       "      <td>[Woodworking Handtools, Instruments &amp; Devices]</td>\n",
       "      <td>Illustrated Encyclopedia of , .</td>\n",
       "      <td>Illustrated Encyclopedia of , .</td>\n",
       "      <td>illustrated encyclopedia of</td>\n",
       "      <td>illustrated encyclopedia of</td>\n",
       "      <td>1</td>\n",
       "      <td>encyclopedia</td>\n",
       "      <td>...</td>\n",
       "      <td>encyclopedia</td>\n",
       "      <td>encyclopedia</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.524171</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.262085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576</th>\n",
       "      <td>\"The Great Reform Act of 1832 and British Demo...</td>\n",
       "      <td>\"The Great Reform Act of 1832 and British Demo...</td>\n",
       "      <td>-6.776486</td>\n",
       "      <td>[]</td>\n",
       "      <td>\"The Great Reform Act of 1832 and British Demo...</td>\n",
       "      <td>\"The Great Reform Act of 1832 and British Demo...</td>\n",
       "      <td>the great reform act of and british democratiz...</td>\n",
       "      <td>the great reform act of and british democracy</td>\n",
       "      <td>0</td>\n",
       "      <td>great reform act british democratization</td>\n",
       "      <td>...</td>\n",
       "      <td>great reform act british democratization</td>\n",
       "      <td>great reform act british</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.661122</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.528898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2500</th>\n",
       "      <td>\"Radio Birdman\", in The Trouser Press Record G...</td>\n",
       "      <td>\"Radio Birdman\", in The Hose Press Record Guid...</td>\n",
       "      <td>-6.629883</td>\n",
       "      <td>[ed]</td>\n",
       "      <td>\"Radio Birdman\", in The Trouser Press Record G...</td>\n",
       "      <td>\"Radio Birdman\", in The Hose Press Record Guid...</td>\n",
       "      <td>radio birdman in the trouser press record guide</td>\n",
       "      <td>radio birdman in the hose press record guide</td>\n",
       "      <td>2</td>\n",
       "      <td>radio trouser press guide</td>\n",
       "      <td>...</td>\n",
       "      <td>radio trouser press guide</td>\n",
       "      <td>radio hose press guide</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.661558</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.441038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentences_en  \\\n",
       "3917  The Mummy, A Handbook of Egyptian Funerary Arc...   \n",
       "6975  He vacated the WWA Cruiserweight title in Apri...   \n",
       "2080  Haroun and S.A. Mourad, Proceedings of Interna...   \n",
       "6229  Boone Trails Grand Towers Gravois Trail New Ho...   \n",
       "7033  World Encyclopedia of Police Forces and Penal ...   \n",
       "1846  Faerie Apocalypse, Jason Franks, IFWG Publishi...   \n",
       "2163  Pollock, Griselda, Generations and Geographies...   \n",
       "2214  Illustrated Encyclopedia of Woodworking Handto...   \n",
       "2576  \"The Great Reform Act of 1832 and British Demo...   \n",
       "2500  \"Radio Birdman\", in The Trouser Press Record G...   \n",
       "\n",
       "                                           sentences_ge    scores  \\\n",
       "3917  The Mummy, A Handbook of Egyptian Funerary Arc... -8.140713   \n",
       "6975  Nach seiner Rückkehr zum WWF verließ er im Apr... -7.481519   \n",
       "2080  Haroun and S.A. Mourad, Proceedings of Interna... -7.148534   \n",
       "6229  Boone Trails Grand Towers Gravois Trail New Ho... -7.028717   \n",
       "7033  World Encyclopedia of Police Forces and Straal... -7.028717   \n",
       "1846  Faerie Apocalypse, Jason Franks, IFWG Publishi... -6.776486   \n",
       "2163  Pollock, Griselda, Generations and Geographies... -6.776486   \n",
       "2214  Illustrated Encyclopedia of Woodworking Handto... -6.776486   \n",
       "2576  \"The Great Reform Act of 1832 and British Demo... -6.776486   \n",
       "2500  \"Radio Birdman\", in The Hose Press Record Guid... -6.629883   \n",
       "\n",
       "                                                 person  \\\n",
       "3917                                                 []   \n",
       "6975                                              [WWF]   \n",
       "2080                [Haroun, S.A. Mourad, Cairo, Egypt]   \n",
       "6229  [Boone Trails Grand Towers Gravois Trail New H...   \n",
       "7033                                                 []   \n",
       "1846  [Faerie Apocalypse, Jason Franks, IFWG Publish...   \n",
       "2163  [Pollock, Griselda, Generations and, the Visua...   \n",
       "2214     [Woodworking Handtools, Instruments & Devices]   \n",
       "2576                                                 []   \n",
       "2500                                               [ed]   \n",
       "\n",
       "                              sentences_en_no_propnouns  \\\n",
       "3917  The Mummy, A Handbook of Egyptian Funerary Arc...   \n",
       "6975  He vacated the WWA Cruiserweight title in Apri...   \n",
       "2080   and , Proceedings of International Colloquium...   \n",
       "6229   North Star Osage  Trailblazers  Trails Thunde...   \n",
       "7033  World Encyclopedia of Police Forces and Penal ...   \n",
       "1846                                              , , .   \n",
       "2163                         Geographies in , , , 1996.   \n",
       "2214                    Illustrated Encyclopedia of , .   \n",
       "2576  \"The Great Reform Act of 1832 and British Demo...   \n",
       "2500  \"Radio Birdman\", in The Trouser Press Record G...   \n",
       "\n",
       "                              sentences_ge_no_propnouns  \\\n",
       "3917  The Mummy, A Handbook of Egyptian Funerary Arc...   \n",
       "6975  Nach seiner Rückkehr zum  verließ er im April ...   \n",
       "2080   and , Proceedings of International Colloquium...   \n",
       "6229   North Star Osage  Trailblazers  Trails Thunde...   \n",
       "7033  World Encyclopedia of Police Forces and Straal...   \n",
       "1846                                              , , .   \n",
       "2163                         Geographies in , , , 1996.   \n",
       "2214                    Illustrated Encyclopedia of , .   \n",
       "2576  \"The Great Reform Act of 1832 and British Demo...   \n",
       "2500  \"Radio Birdman\", in The Hose Press Record Guid...   \n",
       "\n",
       "                                     sentences_en_clean  \\\n",
       "3917  the mummy a handbook of egyptian funerary arch...   \n",
       "6975  he vacated the wwa cruiserweight title in apri...   \n",
       "2080  and proceedings of international colloquium on...   \n",
       "6229   north star osage trailblazers trails thunderbird   \n",
       "7033  world encyclopedia of police forces and penal ...   \n",
       "1846                                                      \n",
       "2163                                     geographies in   \n",
       "2214                        illustrated encyclopedia of   \n",
       "2576  the great reform act of and british democratiz...   \n",
       "2500    radio birdman in the trouser press record guide   \n",
       "\n",
       "                                     sentences_ge_clean  non_translated_words  \\\n",
       "3917  the mummy a handbook of egyptian funerary arch...                     5   \n",
       "6975  nach seiner rückkehr zum verließ er im april d...                     2   \n",
       "2080  and proceedings of international colloquium on...                     1   \n",
       "6229   north star osage trailblazers trails thunderbird                     3   \n",
       "7033  world encyclopedia of police forces and straal...                     1   \n",
       "1846                                                                        0   \n",
       "2163                                     geographies in                     1   \n",
       "2214                        illustrated encyclopedia of                     1   \n",
       "2576      the great reform act of and british democracy                     0   \n",
       "2500       radio birdman in the hose press record guide                     2   \n",
       "\n",
       "                                   sentences_en_cleaner  ...  \\\n",
       "3917                                                     ...   \n",
       "6975                      vacated title april returning  ...   \n",
       "2080  proceedings international structural engineeri...  ...   \n",
       "6229                             north star trails bird  ...   \n",
       "7033            world encyclopedia police penal systems  ...   \n",
       "1846                                                     ...   \n",
       "2163                                                     ...   \n",
       "2214                                       encyclopedia  ...   \n",
       "2576           great reform act british democratization  ...   \n",
       "2500                          radio trouser press guide  ...   \n",
       "\n",
       "                                     sentences_en_final  \\\n",
       "3917                                                      \n",
       "6975                      vacated title april returning   \n",
       "2080  proceedings international structural engineeri...   \n",
       "6229                             north star trails bird   \n",
       "7033            world encyclopedia police penal systems   \n",
       "1846                                                      \n",
       "2163                                                      \n",
       "2214                                       encyclopedia   \n",
       "2576           great reform act british democratization   \n",
       "2500                          radio trouser press guide   \n",
       "\n",
       "                                  sentences_ge_final length_ge  length_en  \\\n",
       "3917                                                         0          0   \n",
       "6975                    rückkehr verließ april titel         4          4   \n",
       "2080  proceedings international engineering april pp         5          6   \n",
       "6229                          north star trails bird         4          4   \n",
       "7033               world encyclopedia police systems         4          5   \n",
       "1846                                                         0          0   \n",
       "2163                                                         0          0   \n",
       "2214                                    encyclopedia         1          1   \n",
       "2576                        great reform act british         4          5   \n",
       "2500                          radio hose press guide         4          4   \n",
       "\n",
       "      distance  correlation  embedded_words_matched_max  \\\n",
       "3917         0     0.615833                           0   \n",
       "6975         0     0.587837                           4   \n",
       "2080         1     0.630766                           6   \n",
       "6229         0     0.570327                           4   \n",
       "7033         1     0.569677                           5   \n",
       "1846         0     0.615833                           0   \n",
       "2163         0     0.615833                           0   \n",
       "2214         0     0.524171                           1   \n",
       "2576         1     0.661122                           5   \n",
       "2500         0     0.661558                           4   \n",
       "\n",
       "      embedded_words_matched_min   weights  weighted_corr  \n",
       "3917                           0  0.000000       0.000000  \n",
       "6975                           4  0.666667       0.391891  \n",
       "2080                           5  0.714286       0.450547  \n",
       "6229                           4  0.571429       0.325901  \n",
       "7033                           4  0.666667       0.379785  \n",
       "1846                           0  0.000000       0.000000  \n",
       "2163                           0  0.000000       0.000000  \n",
       "2214                           1  0.500000       0.262085  \n",
       "2576                           4  0.800000       0.528898  \n",
       "2500                           4  0.666667       0.441038  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_em.nsmallest(10, 'scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_em.to_pickle('../data/dataset_correlations_v2.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
