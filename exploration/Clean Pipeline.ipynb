{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import en_core_web_md\n",
    "import de_core_news_md\n",
    "import pickle\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_analysis(sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "              token.shape_, token.is_alpha, token.is_stop,token.vector.shape)\n",
    "\n",
    "def entity_analysis(sentence,nlp):\n",
    "    print(sentence+\"\\n\"+\"\\n\"+\"Analysis\"+\"\\n\"+\"--------\")\n",
    "    doc = nlp(sentence)\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text, ent.start_char, ent.end_char, ent.label_)\n",
    "\n",
    "def extract_sentences(filename,lower=False):\n",
    "    if lower:\n",
    "        data = [l.lower().strip() for l in open(filename) if l.strip()]\n",
    "    else:\n",
    "        data = [l.strip() for l in open(filename) if l.strip()]\n",
    "    return data\n",
    "\n",
    "def remove_unnecessary_spaces(sentence):\n",
    "    return re.sub(' +',' ',sentence.strip())\n",
    "\n",
    "def remove_word_from_sentence(sentence,word):\n",
    "    new_sentence = sentence.replace(word,\"\")\n",
    "    return remove_unnecessary_spaces(new_sentence)\n",
    "\n",
    "def remove_multiple_words_from_sentence(sentence,words,isin=False):\n",
    "    if isin:\n",
    "        splited = [word for word in sentence.split() if word in words]\n",
    "    else:\n",
    "        splited = [word for word in sentence.split() if word not in words]\n",
    "    return \" \".join(splited)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_en = en_core_web_md.load()\n",
    "nlp_ge = de_core_news_md.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.src')\n",
    "path_train_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.mt')\n",
    "path_train_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'train.ende.scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_en = pd.DataFrame(extract_sentences(path_train_en),columns = ['sentences_en'])\n",
    "train_sentences_ge = pd.DataFrame(extract_sentences(path_train_ge),columns = ['sentences_ge'])\n",
    "train_scores = pd.read_csv(path_train_scores,header=None)\n",
    "train_scores = train_scores.rename(columns={0:\"scores\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = pd.merge(train_sentences_en,train_sentences_ge,left_index=True,right_index=True)\n",
    "#train_dataset = pd.merge(train_dataset,train_scores,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dev_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.src')\n",
    "path_dev_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.mt')\n",
    "path_dev_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'dev.ende.scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences_en = pd.DataFrame(extract_sentences(path_dev_en),columns = ['sentences_en'])\n",
    "dev_sentences_ge = pd.DataFrame(extract_sentences(path_dev_ge),columns = ['sentences_ge'])\n",
    "dev_scores = pd.read_csv(path_dev_scores,header=None)\n",
    "dev_scores = dev_scores.rename(columns={0:\"scores\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_dataset = pd.merge(dev_sentences_en,dev_sentences_ge,left_index=True,right_index=True)\n",
    "#dev_dataset = pd.merge(dev_dataset,dev_scores,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test_en = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'test.ende.src')\n",
    "path_test_ge = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'test.ende.mt')\n",
    "path_test_scores = os.path.join(os.path.dirname(os.getcwd()), 'data', 'en-de', 'test.ende.scores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences_en = pd.DataFrame(extract_sentences(path_test_en),columns = ['sentences_en'])\n",
    "test_sentences_ge = pd.DataFrame(extract_sentences(path_test_ge),columns = ['sentences_ge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = pd.merge(test_sentences_en,test_sentences_ge,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.concat([train_dataset, dev_dataset])\n",
    "dataset = pd.concat([dataset,test_dataset])\n",
    "dataset = dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vec(emb_path, nmax=50000):\n",
    "    vectors = []\n",
    "    word2id = {}\n",
    "    with io.open(emb_path, 'r', encoding='utf-8', newline='\\n', errors='ignore') as f:\n",
    "        next(f)\n",
    "        for i, line in enumerate(f):\n",
    "            word, vect = line.rstrip().split(' ', 1)\n",
    "            vect = np.fromstring(vect, sep=' ')\n",
    "            assert word not in word2id, 'word found twice'\n",
    "            vectors.append(vect)\n",
    "            word2id[word] = len(word2id)\n",
    "            if len(word2id) == nmax:\n",
    "                break\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    embeddings = np.vstack(vectors)\n",
    "    return embeddings, id2word, word2id\n",
    "\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'data','muse')\n",
    "\n",
    "src_path = path+\"/wiki.multi.en.vec\"\n",
    "tgt_path = path+\"/wiki.multi.de.vec\"\n",
    "nmax = 300000  # maximum number of word embeddings to load\n",
    "\n",
    "src_embeddings, src_id2word, src_word2id = load_vec(src_path, nmax)\n",
    "tgt_embeddings, tgt_id2word, tgt_word2id = load_vec(tgt_path, nmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading nltk utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer #stem for english\n",
    "from nltk.stem.cistem import Cistem\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "porter = PorterStemmer()\n",
    "cistem = Cistem()\n",
    "stopwords_en = set(stopwords.words('english'))\n",
    "stopwords_ge = set(stopwords.words('german'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the full vocabluary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(sentence, nlp):\n",
    "    sentence=nlp(sentence)\n",
    "    clean_sentence = \"\"\n",
    "    for token in sentence:\n",
    "        if (token.is_alpha):\n",
    "            clean_sentence+= (token.text.lower() + \" \")\n",
    "    clean_sentence=clean_sentence.strip()\n",
    "    return clean_sentence\n",
    "\n",
    "def remove_entities(sentence,persons_list):\n",
    "    sentence_without_persons = sentence\n",
    "    for person in persons_list:\n",
    "        sentence_without_persons = sentence_without_persons.replace(person,\"\")\n",
    "    return sentence_without_persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities(sentence_en,sentence_ge):\n",
    "    doc = nlp_en(sentence_en)\n",
    "    persons_list = []\n",
    "    for ent in doc.ents:\n",
    "        if (ent.label_==\"PERSON\" or ent.label_==\"ORG\" or \n",
    "            ent.label_==\"LOC\" or ent.label_==\"GPE\" \n",
    "            or ent.label == \"FAC\" or ent.label == \"NORP\"):\n",
    "            if ent.text in sentence_ge:\n",
    "                persons_list += [ent.text]\n",
    "            else:\n",
    "                ent_text_clean = ent.text.replace(\"the\",\"\").replace(\"The\",\"\").strip()\n",
    "                if ent_text_clean in sentence_ge:\n",
    "                    persons_list += [ent_text_clean]\n",
    "    return persons_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['person']=dataset.apply(lambda row: get_entities(row[\"sentences_en\"],\n",
    "                                                         row[\"sentences_ge\"]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentences_en_no_propnouns'] = dataset.apply(lambda row: remove_entities(row['sentences_en'], row[\"person\"]), axis=1)\n",
    "dataset['sentences_ge_no_propnouns'] = dataset.apply(lambda row: remove_entities(row['sentences_ge'], row[\"person\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['sentences_en_clean'] = dataset.apply(lambda row: remove_punct(row['sentences_en_no_propnouns'], nlp_en), axis=1)\n",
    "dataset['sentences_ge_clean'] = dataset.apply(lambda row: remove_punct(row['sentences_ge_no_propnouns'], nlp_ge), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create first version of vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_words_to_vocab(vocab,sentence,nlp,word_2id,stemmer):\n",
    "    mini_sentence = nlp(sentence)\n",
    "    out_of_vocab = []\n",
    "    for token in mini_sentence:\n",
    "        if token.text not in vocab.keys():\n",
    "            try:\n",
    "                vocab[token.text] = word_2id[token.text]\n",
    "            except:\n",
    "                try:\n",
    "                    vocab[token.text] = word_2id[token.lemma_]\n",
    "                except:\n",
    "                    try:\n",
    "                        synonyms = wordnet.synsets(token.text)[0].lemmas()\n",
    "                        for i in range(10):\n",
    "                            synonym = synonyms[i].name()\n",
    "                            try:\n",
    "                                vocab[token.text] = word_2id[synonym]\n",
    "                                break\n",
    "                            except:\n",
    "                                continue\n",
    "                    except:\n",
    "                        try:\n",
    "                            vocab[token.text] = word_2id[stemmer.stem(token.text)]\n",
    "                        except:\n",
    "                            out_of_vocab.append(token.text)\n",
    "    return vocab,out_of_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English\n",
    "\n",
    "vocab_2_embedding_idx_en = {}\n",
    "out_of_vocab_en = []\n",
    "\n",
    "sentences_en = dataset.sentences_en_clean\n",
    "\n",
    "for i,sentence in enumerate(sentences_en):\n",
    "    vocab_2_embedding_idx_en,out_of_vocab_current = add_words_to_vocab(vocab_2_embedding_idx_en,sentence,\n",
    "                                                                       nlp_en,src_word2id,porter)\n",
    "    \n",
    "    for out_of_vocab_word in out_of_vocab_current:\n",
    "        out_of_vocab_en += [(out_of_vocab_word,i)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#German\n",
    "\n",
    "vocab_2_embedding_idx_ge = {}\n",
    "out_of_vocab_ge = []\n",
    "\n",
    "sentences_ge = dataset.sentences_ge_clean\n",
    "\n",
    "for i,sentence in enumerate(sentences_ge):\n",
    "    vocab_2_embedding_idx_ge,out_of_vocab_current = add_words_to_vocab(vocab_2_embedding_idx_ge,sentence,\n",
    "                                                                nlp_ge,tgt_word2id,cistem)\n",
    "    for out_of_vocab_word in out_of_vocab_current:\n",
    "        out_of_vocab_ge += [(out_of_vocab_word,i)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing non german words from german vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words_from_list(list_1,list_2 = german_words,isin=False):\n",
    "    if isin:\n",
    "        return [word for word in list_1 if word in list_2]\n",
    "    else:\n",
    "        return [word for word in list_1 if word not in list_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_german_vocab_corpus = list(vocab_2_embedding_idx_ge.keys())\n",
    "#large list of german words downloaded here : https://gist.github.com/MarvinJWendt/2f4f4154b8ae218600eb091a5706b5f4#file-wordlist-german-txt\n",
    "german_words = list(pd.read_csv(\"../data/wordlist-german.txt\",header=None)[0])\n",
    "german_words = [str(word).lower() for word in german_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(4)\n",
    "n = len(current_german_vocab_corpus)\n",
    "split = int(n/4)\n",
    "\n",
    "words_to_be_removed_list = pool.map(remove_words_from_list,[current_german_vocab_corpus[0:split],\n",
    "                                                 current_german_vocab_corpus[split:split*2],\n",
    "                                                 current_german_vocab_corpus[split*2:split*3],\n",
    "                                                 current_german_vocab_corpus[split*3:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_be_removed = words_to_be_removed_list[0] + words_to_be_removed_list[1] + \\\n",
    "                      words_to_be_removed_list[2] + words_to_be_removed_list[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update out_of_vocab list and vocab_2_embedding_idx_ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_out_of_vocab_ge = []\n",
    "for word in words_to_be_removed:\n",
    "    idxs =  list(dataset[dataset.sentences_ge_clean.apply(lambda x: word in x)].index)\n",
    "    if len(idxs)<=3:\n",
    "        for idx in idxs:\n",
    "            new_out_of_vocab_ge += [(word,idx)]\n",
    "            \n",
    "out_of_vocab_ge += new_out_of_vocab_ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in [pair[0] for pair in new_out_of_vocab_ge]:\n",
    "    try:\n",
    "        vocab_2_embedding_idx_ge.pop(word,None)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load vocab dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save\n",
    "#with open(\"../data/vocab_en_tvl.pkl\",\"wb\") as f:\n",
    "#    pickle.dump(vocab_2_embedding_idx_en, f, pickle.HIGHEST_PROTOCOL)\n",
    "#with open(\"../data/vocab_ge_tvl.pkl\",\"wb\") as f:\n",
    "#    pickle.dump(vocab_2_embedding_idx_ge, f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#Load    \n",
    "#with open(\"../data/vocab_en.pkl\", 'rb') as f:\n",
    "#    vocab_2_embedding_idx_en = pickle.load(f)\n",
    "#with open(\"../data/vocab_ge.pkl\", 'rb') as f:\n",
    "#    vocab_2_embedding_idx_ge = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process out of vocab german words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"non_translated_words\"] = 0\n",
    "dataset[\"sentences_en_cleaner\"] = dataset[\"sentences_en_clean\"].copy()\n",
    "dataset[\"sentences_ge_cleaner\"] = dataset[\"sentences_ge_clean\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_translated_out_of_vocab_words(pair,word2id,dataset,out_of_vocab_english):\n",
    "    word,idx = pair[0],pair[1]\n",
    "    en_sentence = dataset.loc[idx][\"sentences_en_cleaner\"]\n",
    "    ge_sentence = dataset.loc[idx][\"sentences_ge_cleaner\"]\n",
    "    count_non_translated = dataset.loc[idx][\"non_translated_words\"]\n",
    "    \n",
    "    ### If the word is exactly the same as a word in the source sentence\n",
    "    if word in en_sentence:\n",
    "        if word not in out_of_vocab_english:\n",
    "            ### if the word is in the english vocab -> it could have been translated\n",
    "            ### count it as a non translated word and remove it from both sentences\n",
    "            dataset.at[idx,\"sentences_en_cleaner\"] = remove_word_from_sentence(en_sentence,word)\n",
    "            dataset.at[idx,\"sentences_ge_cleaner\"] = remove_word_from_sentence(ge_sentence,word)\n",
    "            dataset.at[idx,\"non_translated_words\"] = count_non_translated+1\n",
    "            return dataset\n",
    "        else:\n",
    "            ### if the word is not in the english vocab -> it could not have been translated\n",
    "            ### don't count it as a non translated word and remove it from both sentences\n",
    "            dataset.at[idx,\"sentences_en_cleaner\"] = remove_word_from_sentence(en_sentence,word)\n",
    "            dataset.at[idx,\"sentences_ge_cleaner\"] = remove_word_from_sentence(ge_sentence,word)\n",
    "            return dataset\n",
    "        \n",
    "    ### Test for subwords\n",
    "    subword = word[:-1]\n",
    "    while len(subword)>0:\n",
    "        if subword in word2id.keys():\n",
    "            subword1 = subword\n",
    "            subword2 = word[len(subword1):]\n",
    "            if subword2 in word2id.keys():\n",
    "                if len(subword2)>2:\n",
    "                    ### if the word can be splitted in 2 meaningful words do it\n",
    "                    dataset.at[idx,\"sentences_ge_cleaner\"] = ge_sentence.replace(word,subword1+\" \"+subword2)\n",
    "                    return dataset\n",
    "                else:\n",
    "                    ### if the word can be found by using a shorter version which can be found in vocab\n",
    "                    dataset.at[idx,\"sentences_ge_cleaner\"] = ge_sentence.replace(word,subword1)\n",
    "                    return dataset\n",
    "            else:\n",
    "                return dataset\n",
    "            \n",
    "        else:\n",
    "            subword = subword[:-1]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in out_of_vocab_ge:\n",
    "    dataset = process_translated_out_of_vocab_words(pair,vocab_2_embedding_idx_ge,dataset,out_of_vocab_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"sentences_en_cleaner\"] = dataset.sentences_en_cleaner.apply(lambda x:remove_multiple_words_from_sentence(x,stopwords_en))\n",
    "dataset[\"sentences_ge_cleaner\"] = dataset.sentences_ge_cleaner.apply(lambda x:remove_multiple_words_from_sentence(x,stopwords_ge))\n",
    "dataset[\"sentences_ge_cleaner\"] = dataset.sentences_ge_cleaner.apply(lambda x:remove_multiple_words_from_sentence(x,stopwords_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_fin_en = list(vocab_2_embedding_idx_en.keys())\n",
    "vocab_fin_ge = list(vocab_2_embedding_idx_ge.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sentences for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"sentences_en_final\"] = dataset.sentences_en_cleaner.apply(lambda x:remove_multiple_words_from_sentence(x,vocab_fin_en,True))\n",
    "dataset[\"sentences_ge_final\"] = dataset.sentences_ge_cleaner.apply(lambda x:remove_multiple_words_from_sentence(x,vocab_fin_ge,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute correlation features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_correlation(src_emb,tgt_emb):\n",
    "    corr = (src_emb / np.linalg.norm(src_emb)).dot(tgt_emb / np.linalg.norm(tgt_emb))\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(word,language):\n",
    "    if language==\"en\":\n",
    "        return src_embeddings[vocab_2_embedding_idx_en[word]]\n",
    "    else:\n",
    "        return tgt_embeddings[vocab_2_embedding_idx_ge[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corr_matrix(words_en_list,words_ge_list):\n",
    "    n = len(words_en_list)\n",
    "    m = len(words_ge_list)\n",
    "    corr_matrix = np.zeros((n,m))\n",
    "    for i,word_en in enumerate(words_en_list):\n",
    "        for j,word_ge in enumerate(words_ge_list):\n",
    "            corr_matrix[i,j] = get_correlation(get_emb(word_en,\"en\"),get_emb(word_ge,\"ge\"))\n",
    "            \n",
    "    return corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_matches(corr_matrix):\n",
    "    if len(corr_matrix)==0:\n",
    "        return {}\n",
    "    best_match_row = np.argmax(corr_matrix,axis=0)\n",
    "    best_match_col = np.argmax(corr_matrix,axis=1)\n",
    "    couples = {}\n",
    "    tmp_corr_matrix = corr_matrix.copy()\n",
    "    n = corr_matrix.shape[0] \n",
    "    m = corr_matrix.shape[1]\n",
    "    dim = min(n,m)\n",
    "    while len(couples.keys())<dim:\n",
    "        for i in range(n):\n",
    "            if (i == best_match_row[best_match_col[i]]) and (i not in couples.keys()):\n",
    "                couples[i] = best_match_col[i]\n",
    "                tmp_corr_matrix[i,:] = np.zeros(m)\n",
    "                tmp_corr_matrix[:,best_match_col[i]] = np.zeros(n)\n",
    "                best_match_row = np.argmax(tmp_corr_matrix,axis=0)\n",
    "                best_match_col = np.argmax(tmp_corr_matrix,axis=1)\n",
    "    return couples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_couples(words_en,words_ge):\n",
    "    if len(words_en)==0 or len(words_ge)==0:\n",
    "        return np.nan,np.nan,np.nan\n",
    "    \n",
    "    words_en_list = words_en.split()\n",
    "    words_ge_list = words_ge.split()\n",
    "    corr_mat = get_corr_matrix(words_en_list,words_ge_list)\n",
    "    word_couples_idx = get_word_matches(corr_mat)\n",
    "    score = 0\n",
    "    \n",
    "    for i in word_couples_idx.keys():\n",
    "        score+=corr_mat[i,word_couples_idx[i]]\n",
    "    score/=len(word_couples_idx)\n",
    "    \n",
    "    if len(words_en_list)>len(words_ge_list):\n",
    "        kept_words_idx = np.array(list(word_couples_idx.keys()))\n",
    "        left_words_idx = np.setdiff1d(np.arange(len(words_en_list)),kept_words_idx)\n",
    "        left_words = [words_en_list[i] for i in left_words_idx]\n",
    "        \n",
    "    elif len(words_en_list)<len(words_ge_list):\n",
    "        kept_words_idx = np.array(list(word_couples_idx.values()))\n",
    "        left_words_idx = np.setdiff1d(np.arange(len(words_en_list)),kept_words_idx)\n",
    "        left_words = [words_ge_list[i] for i in left_words_idx]\n",
    "        \n",
    "    else:\n",
    "        left_words = []\n",
    "        \n",
    "    word_couples = {}\n",
    "    for key,val in word_couples_idx.items():\n",
    "        word_couples[words_en_list[key]] = words_ge_list[val]\n",
    "        \n",
    "    return word_couples,score,left_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"correlation\"] = dataset.apply(lambda row:get_word_couples(row[\"sentences_en_final\"],row[\"sentences_ge_final\"])[1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"correlation\"] = dataset[\"correlation\"].fillna(dataset[\"correlation\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_length(sentence):\n",
    "    leng=0\n",
    "    sentence=sentence.split(\" \")\n",
    "    for token in sentence:\n",
    "        leng+=1\n",
    "    return leng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_length(a, b):\n",
    "    lenga=0\n",
    "    a=a.split(\" \")\n",
    "    for token in a:\n",
    "        lenga+=1\n",
    "    \n",
    "    lengb=0\n",
    "    b=b.split(\" \")\n",
    "    for token in b:\n",
    "        lengb+=1\n",
    "    \n",
    "    return abs(lenga-lengb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['english_sentence_length'] = dataset['sentences_en'].apply(sentence_length)\n",
    "dataset['german_sentence_length'] = dataset['sentences_ge'].apply(sentence_length)\n",
    "dataset['sentence_length_difference'] = dataset['english_sentence_length'] - dataset['german_sentence_length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos(pos,sentence,nlp):\n",
    "    doc = nlp(sentence)\n",
    "    res = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_==pos:\n",
    "            res += token.text + \" \"\n",
    "    res = res.strip()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"german_verbs\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"VERB\",x,nlp_ge))\n",
    "dataset[\"english_verbs\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"VERB\",x,nlp_en))\n",
    "dataset['verbs_diff'] = dataset.apply(lambda row: difference_length(row['english_verbs'], row['german_verbs']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"german_adjectives\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"ADJ\",x,nlp_ge))\n",
    "dataset[\"english_adjectives\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"ADJ\",x,nlp_en))\n",
    "dataset['adjectives_diff'] = dataset.apply(lambda row: difference_length(row['english_adjectives'], row['german_adjectives']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"german_adverbs\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"ADV\",x,nlp_ge))\n",
    "dataset[\"english_adverbs\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"ADV\",x,nlp_en))\n",
    "dataset['adverbs_diff'] = dataset.apply(lambda row: difference_length(row['english_adverbs'], row['german_adverbs']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"german_nouns\"] = dataset.sentences_ge.apply(lambda x:extract_pos(\"NOUN\",x,nlp_ge))\n",
    "dataset[\"english_nouns\"] = dataset.sentences_en.apply(lambda x:extract_pos(\"NOUN\",x,nlp_en))\n",
    "dataset['nouns_diff'] = dataset.apply(lambda row: difference_length(row['english_nouns'], row['german_nouns']),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizer(sentence, nlp):\n",
    "    sentence = nlp(sentence)\n",
    "    lemmatized_sentence = \"\"\n",
    "    for token in sentence:\n",
    "        lemmatized_sentence += token.lemma_ + \" \"\n",
    "    \n",
    "    lemmatized_sentence = lemmatized_sentence.strip()\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['english_lemma'] = dataset['sentences_en'].apply (lambda x: lemmatizer(x, nlp_en))\n",
    "dataset['german_lemma'] = dataset['sentences_ge'].apply (lambda x: lemmatizer(x, nlp_ge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob as textblob_en\n",
    "from textblob_de import TextBlobDE as textblob_ge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(sentence, textblob):\n",
    "    text = textblob(sentence)\n",
    "    score = text.sentiment.polarity\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['english_sentence_sentiment'] = dataset['sentences_en'].apply(lambda x: sentiment(x, textblob_en))\n",
    "dataset['german_sentence_sentiment'] = dataset['sentences_ge'].apply(lambda x: sentiment(x, textblob_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['english_sentence_lemma_sentiment'] = dataset['english_lemma'].apply(lambda x: sentiment(x, textblob_en))\n",
    "dataset['german_sentence_lemma_sentiment'] = dataset['german_lemma'].apply(lambda x: sentiment(x, textblob_ge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdz(data):\n",
    "    data_stdz = (data - data.mean())/data.std()\n",
    "    return data_stdz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['std_english_sentence_sentiment']= stdz(dataset['english_sentence_sentiment'])\n",
    "dataset['std_german_sentence_sentiment']= stdz(dataset['german_sentence_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"max_sentiment_english_bool\"] = dataset.apply(lambda row:abs(row['english_sentence_lemma_sentiment'])>abs(row['english_sentence_sentiment']),axis=1)\n",
    "dataset[\"max_sentiment_german_bool\"] = dataset.apply(lambda row:abs(row['german_sentence_lemma_sentiment'])>abs(row['german_sentence_sentiment']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['max_sentiment_english'] = dataset.apply(lambda row: row[\"english_sentence_lemma_sentiment\"] \n",
    "                                                 if row[\"max_sentiment_english_bool\"]\n",
    "                                                 else row[\"english_sentence_sentiment\"],axis=1)\n",
    "\n",
    "dataset = dataset.drop(columns=[\"max_sentiment_english_bool\"])\n",
    "\n",
    "dataset['max_sentiment_german'] = dataset.apply(lambda row: row[\"german_sentence_lemma_sentiment\"] \n",
    "                                                 if row[\"max_sentiment_german_bool\"]\n",
    "                                                 else row[\"german_sentence_sentiment\"],axis=1)\n",
    "\n",
    "dataset = dataset.drop(columns=[\"max_sentiment_german_bool\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdz(data):\n",
    "    data_stdz = (data - data.mean())/data.std()\n",
    "    return data_stdz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['std_max_english_sentiment']= stdz(dataset['max_sentiment_english'])\n",
    "dataset['std_max_german_sentiment']= stdz(dataset['max_sentiment_german'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pickle(\"../data/dataset_all.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
